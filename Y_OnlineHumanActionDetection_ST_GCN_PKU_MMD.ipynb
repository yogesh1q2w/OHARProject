{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Y_OnlineHumanActionDetection-ST-GCN-PKU-MMD.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrolSsWqmnzu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import _pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUTMmo1wm7Fm",
        "colab_type": "code",
        "outputId": "fb1287b4-750b-4cd5-e0ec-89c36c245985",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsTNs_NxnBfB",
        "colab_type": "code",
        "outputId": "5be8519d-1ff9-4184-ced0-f5831b63b76a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        }
      },
      "source": [
        "!ls /content/drive/My\\ Drive \n",
        "!cp /content/drive/My\\ Drive/Label_PKUMMD.7z /content/Label_PKUMMD.7z\n",
        "!cp /content/drive/My\\ Drive/Skeleton.7z /content/Skeleton.7z\n",
        "!cp /content/drive/My\\ Drive/cross-subject.txt /content/cross-subject.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"Alekhya's Feedback-SOP.gdoc\"\t 'Proofs for Resume'\n",
            "\"Apping Fall'20.gsheet\"\t\t  Skeleton.7z\n",
            " bbox_dvqa_pri_results.json\t  skeleton_data\n",
            " Classroom\t\t\t 'Statement of Purpose.gdoc'\n",
            "'Colab Notebooks'\t\t 'TDL Project'\n",
            " cross-subject.txt\t\t 'TDL Project.gslides'\n",
            " detection_results.pkl\t\t 'Traffic4cast (1).ipynb'\n",
            " Essay.gdoc\t\t\t 'Traffic4cast (3).ipynb'\n",
            " Label_PKUMMD.7z\t\t 'Traffic4cast_(3).ipynb'\n",
            "'ME TRPO parameters.gsheet'\t  Traffic4cast_conv_Berlin.ipynb\n",
            "'Online Human Action Detection'   Traffic4cast_conv_Istanbul.ipynb\n",
            " output\t\t\t\t  Traffic4cast_Istanbul.ipynb\n",
            " PA2\t\t\t\t  Traffic4cast_Moscow.ipynb\n",
            " pickled_data\t\t\t 'Yogesh Tripathi.pdf'\n",
            " Poetry.gdoc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xifjNBnlnH-v",
        "colab_type": "code",
        "outputId": "f9af1af6-e1cb-4510-ee4a-f5d63ddd2556",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        }
      },
      "source": [
        "!apt-get install p7zip-full"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "p7zip-full is already the newest version (16.02+dfsg-6).\n",
            "The following packages were automatically installed and are no longer required:\n",
            "  cuda-cufft-10-1 cuda-cufft-dev-10-1 cuda-curand-10-1 cuda-curand-dev-10-1\n",
            "  cuda-cusolver-10-1 cuda-cusolver-dev-10-1 cuda-cusparse-10-1\n",
            "  cuda-cusparse-dev-10-1 cuda-license-10-2 cuda-npp-10-1 cuda-npp-dev-10-1\n",
            "  cuda-nsight-10-1 cuda-nsight-compute-10-1 cuda-nsight-systems-10-1\n",
            "  cuda-nvgraph-10-1 cuda-nvgraph-dev-10-1 cuda-nvjpeg-10-1\n",
            "  cuda-nvjpeg-dev-10-1 cuda-nvrtc-10-1 cuda-nvrtc-dev-10-1 cuda-nvvp-10-1\n",
            "  libcublas10 libnvidia-common-430 nsight-compute-2019.5.0\n",
            "  nsight-systems-2019.5.2\n",
            "Use 'apt autoremove' to remove them.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 5 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3jPPutCnJwI",
        "colab_type": "code",
        "outputId": "600d6e90-8def-4807-d26d-2f0ed2a84124",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 781
        }
      },
      "source": [
        "%cd /content\n",
        "!7za x Label_PKUMMD.7z\n",
        "!7za x Skeleton.7z"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "\n",
            "7-Zip (a) [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\n",
            "p7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,2 CPUs Intel(R) Xeon(R) CPU @ 2.30GHz (306F0),ASM,AES-NI)\n",
            "\n",
            "Scanning the drive for archives:\n",
            "  0M Scan\b\b\b\b\b\b\b\b\b         \b\b\b\b\b\b\b\b\b1 file, 85935 bytes (84 KiB)\n",
            "\n",
            "Extracting archive: Label_PKUMMD.7z\n",
            "--\n",
            "Path = Label_PKUMMD.7z\n",
            "Type = 7z\n",
            "Physical Size = 85935\n",
            "Headers Size = 6088\n",
            "Method = LZMA2:384k\n",
            "Solid = +\n",
            "Blocks = 1\n",
            "\n",
            "  0%\b\b\b\b    \b\b\b\b  0% 1076 - Train_Label_PKU_final/0364-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEverything is Ok\n",
            "\n",
            "Folders: 1\n",
            "Files: 1076\n",
            "Size:       310737\n",
            "Compressed: 85935\n",
            "\n",
            "7-Zip (a) [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\n",
            "p7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,2 CPUs Intel(R) Xeon(R) CPU @ 2.30GHz (306F0),ASM,AES-NI)\n",
            "\n",
            "Scanning the drive for archives:\n",
            "  0M Scan\b\b\b\b\b\b\b\b\b         \b\b\b\b\b\b\b\b\b1 file, 1087979618 bytes (1038 MiB)\n",
            "\n",
            "Extracting archive: Skeleton.7z\n",
            "--\n",
            "Path = Skeleton.7z\n",
            "Type = 7z\n",
            "Physical Size = 1087979618\n",
            "Headers Size = 10988\n",
            "Method = LZMA2:24\n",
            "Solid = +\n",
            "Blocks = 4\n",
            "\n",
            "  0%\b\b\b\b    \b\b\b\b  0% 3 - PKU_Skeleton_Renew/0002-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  0% 6 - PKU_Skeleton_Renew/0003-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  0% 8 - PKU_Skeleton_Renew/0004-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  0% 11 - PKU_Skeleton_Renew/0005-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  0% 17 - PKU_Skeleton_Renew/0007-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  1% 20 - PKU_Skeleton_Renew/0008-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  1% 23 - PKU_Skeleton_Renew/0009-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  1% 25 - PKU_Skeleton_Renew/0010-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  1% 28 - PKU_Skeleton_Renew/0011-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  2% 29 - PKU_Skeleton_Renew/0011-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  2% 32 - PKU_Skeleton_Renew/0012-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  2% 34 - PKU_Skeleton_Renew/0013-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  2% 36 - PKU_Skeleton_Renew/0013-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  2% 39 - PKU_Skeleton_Renew/0014-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  3% 43 - PKU_Skeleton_Renew/0016-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  3% 47 - PKU_Skeleton_Renew/0017-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  3% 50 - PKU_Skeleton_Renew/0018-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  3% 52 - PKU_Skeleton_Renew/0019-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  3% 54 - PKU_Skeleton_Renew/0019-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  4% 56 - PKU_Skeleton_Renew/0020-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  4% 58 - PKU_Skeleton_Renew/0021-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  4% 60 - PKU_Skeleton_Renew/0021-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  4% 64 - PKU_Skeleton_Renew/0023-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  4% 66 - PKU_Skeleton_Renew/0023-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  5% 69 - PKU_Skeleton_Renew/0024-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  5% 72 - PKU_Skeleton_Renew/0025-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  5% 75 - PKU_Skeleton_Renew/0026-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  5% 77 - PKU_Skeleton_Renew/0027-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  5% 80 - PKU_Skeleton_Renew/0028-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  5% 82 - PKU_Skeleton_Renew/0029-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  6% 84 - PKU_Skeleton_Renew/0029-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  6% 87 - PKU_Skeleton_Renew/0030-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  6% 89 - PKU_Skeleton_Renew/0031-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  6% 92 - PKU_Skeleton_Renew/0032-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  6% 95 - PKU_Skeleton_Renew/0033-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  7% 98 - PKU_Skeleton_Renew/0034-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  7% 101 - PKU_Skeleton_Renew/0035-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  7% 106 - PKU_Skeleton_Renew/0037-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  7% 107 - PKU_Skeleton_Renew/0037-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  7% 110 - PKU_Skeleton_Renew/0038-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  8% 112 - PKU_Skeleton_Renew/0039-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  8% 114 - PKU_Skeleton_Renew/0039-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  8% 116 - PKU_Skeleton_Renew/0040-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  8% 118 - PKU_Skeleton_Renew/0041-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  8% 120 - PKU_Skeleton_Renew/0041-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  9% 123 - PKU_Skeleton_Renew/0042-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  9% 125 - PKU_Skeleton_Renew/0043-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  9% 128 - PKU_Skeleton_Renew/0044-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  9% 131 - PKU_Skeleton_Renew/0045-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  9% 135 - PKU_Skeleton_Renew/0046-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 10% 137 - PKU_Skeleton_Renew/0047-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 10% 140 - PKU_Skeleton_Renew/0048-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 10% 142 - PKU_Skeleton_Renew/0049-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 10% 144 - PKU_Skeleton_Renew/0049-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 10% 147 - PKU_Skeleton_Renew/0050-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 11% 149 - PKU_Skeleton_Renew/0051-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 11% 151 - PKU_Skeleton_Renew/0052-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 11% 153 - PKU_Skeleton_Renew/0052-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 11% 155 - PKU_Skeleton_Renew/0053-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 12% 156 - PKU_Skeleton_Renew/0053-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 12% 159 - PKU_Skeleton_Renew/0054-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 12% 161 - PKU_Skeleton_Renew/0055-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 12% 165 - PKU_Skeleton_Renew/0056-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 12% 167 - PKU_Skeleton_Renew/0057-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 12% 169 - PKU_Skeleton_Renew/0058-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 13% 170 - PKU_Skeleton_Renew/0058-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 13% 173 - PKU_Skeleton_Renew/0059-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 13% 174 - PKU_Skeleton_Renew/0059-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 13% 177 - PKU_Skeleton_Renew/0061-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 14% 178 - PKU_Skeleton_Renew/0061-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 14% 180 - PKU_Skeleton_Renew/0062-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 14% 182 - PKU_Skeleton_Renew/0062-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 14% 184 - PKU_Skeleton_Renew/0063-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 14% 186 - PKU_Skeleton_Renew/0064-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 15% 187 - PKU_Skeleton_Renew/0064-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 15% 192 - PKU_Skeleton_Renew/0066-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 15% 196 - PKU_Skeleton_Renew/0067-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 15% 197 - PKU_Skeleton_Renew/0067-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 15% 200 - PKU_Skeleton_Renew/0068-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 16% 202 - PKU_Skeleton_Renew/0069-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 16% 203 - PKU_Skeleton_Renew/0069-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 16% 206 - PKU_Skeleton_Renew/0070-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 16% 208 - PKU_Skeleton_Renew/0071-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 16% 209 - PKU_Skeleton_Renew/0071-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 17% 211 - PKU_Skeleton_Renew/0072-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 17% 213 - PKU_Skeleton_Renew/0073-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 17% 214 - PKU_Skeleton_Renew/0073-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 17% 216 - PKU_Skeleton_Renew/0074-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 17% 219 - PKU_Skeleton_Renew/0075-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 18% 222 - PKU_Skeleton_Renew/0076-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 18% 225 - PKU_Skeleton_Renew/0077-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 18% 227 - PKU_Skeleton_Renew/0077-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 18% 229 - PKU_Skeleton_Renew/0078-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 18% 231 - PKU_Skeleton_Renew/0079-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 19% 232 - PKU_Skeleton_Renew/0079-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 19% 234 - PKU_Skeleton_Renew/0080-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 19% 236 - PKU_Skeleton_Renew/0080-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 19% 238 - PKU_Skeleton_Renew/0081-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 20% 240 - PKU_Skeleton_Renew/0082-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 20% 242 - PKU_Skeleton_Renew/0082-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 20% 244 - PKU_Skeleton_Renew/0083-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 20% 246 - PKU_Skeleton_Renew/0084-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 20% 248 - PKU_Skeleton_Renew/0084-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 21% 249 - PKU_Skeleton_Renew/0085-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 21% 253 - PKU_Skeleton_Renew/0086-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 21% 256 - PKU_Skeleton_Renew/0087-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 21% 257 - PKU_Skeleton_Renew/0087-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 21% 259 - PKU_Skeleton_Renew/0088-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 22% 262 - PKU_Skeleton_Renew/0089-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 22% 264 - PKU_Skeleton_Renew/0090-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 22% 266 - PKU_Skeleton_Renew/0090-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 22% 268 - PKU_Skeleton_Renew/0091-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 22% 269 - PKU_Skeleton_Renew/0091-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 23% 271 - PKU_Skeleton_Renew/0092-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 23% 273 - PKU_Skeleton_Renew/0093-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 23% 275 - PKU_Skeleton_Renew/0093-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 23% 277 - PKU_Skeleton_Renew/0094-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 24% 279 - PKU_Skeleton_Renew/0095-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 24% 282 - PKU_Skeleton_Renew/0096-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 24% 285 - PKU_Skeleton_Renew/0097-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 24% 287 - PKU_Skeleton_Renew/0097-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 24% 289 - PKU_Skeleton_Renew/0098-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 24% 291 - PKU_Skeleton_Renew/0099-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 25% 293 - PKU_Skeleton_Renew/0099-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 25% 294 - PKU_Skeleton_Renew/0100-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 25% 296 - PKU_Skeleton_Renew/0100-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 25% 297 - PKU_Skeleton_Renew/0101-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 26% 299 - PKU_Skeleton_Renew/0101-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 26% 301 - PKU_Skeleton_Renew/0102-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 26% 303 - PKU_Skeleton_Renew/0103-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 26% 305 - PKU_Skeleton_Renew/0103-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 26% 307 - PKU_Skeleton_Renew/0104-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 27% 310 - PKU_Skeleton_Renew/0105-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 27% 315 - PKU_Skeleton_Renew/0107-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 27% 317 - PKU_Skeleton_Renew/0107-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 27% 318 - PKU_Skeleton_Renew/0108-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 27% 320 - PKU_Skeleton_Renew/0108-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 28% 322 - PKU_Skeleton_Renew/0109-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 28% 324 - PKU_Skeleton_Renew/0110-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 28% 326 - PKU_Skeleton_Renew/0110-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 28% 328 - PKU_Skeleton_Renew/0111-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 28% 329 - PKU_Skeleton_Renew/0111-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 29% 332 - PKU_Skeleton_Renew/0112-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 29% 333 - PKU_Skeleton_Renew/0113-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 29% 335 - PKU_Skeleton_Renew/0113-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 29% 337 - PKU_Skeleton_Renew/0114-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 30% 338 - PKU_Skeleton_Renew/0114-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 30% 342 - PKU_Skeleton_Renew/0116-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 30% 345 - PKU_Skeleton_Renew/0117-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 30% 347 - PKU_Skeleton_Renew/0117-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 30% 348 - PKU_Skeleton_Renew/0118-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 30% 350 - PKU_Skeleton_Renew/0118-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 31% 353 - PKU_Skeleton_Renew/0119-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 31% 354 - PKU_Skeleton_Renew/0120-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 31% 356 - PKU_Skeleton_Renew/0120-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 31% 358 - PKU_Skeleton_Renew/0121-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 32% 360 - PKU_Skeleton_Renew/0122-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 32% 361 - PKU_Skeleton_Renew/0122-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 32% 363 - PKU_Skeleton_Renew/0123-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 32% 364 - PKU_Skeleton_Renew/0123-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 32% 366 - PKU_Skeleton_Renew/0124-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 33% 368 - PKU_Skeleton_Renew/0124-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 33% 370 - PKU_Skeleton_Renew/0125-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 33% 374 - PKU_Skeleton_Renew/0126-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 33% 376 - PKU_Skeleton_Renew/0127-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 33% 378 - PKU_Skeleton_Renew/0128-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 34% 380 - PKU_Skeleton_Renew/0128-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 34% 382 - PKU_Skeleton_Renew/0129-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 34% 383 - PKU_Skeleton_Renew/0129-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 34% 385 - PKU_Skeleton_Renew/0130-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 34% 386 - PKU_Skeleton_Renew/0130-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 35% 388 - PKU_Skeleton_Renew/0131-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 35% 390 - PKU_Skeleton_Renew/0132-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 35% 392 - PKU_Skeleton_Renew/0132-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 35% 393 - PKU_Skeleton_Renew/0133-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 35% 396 - PKU_Skeleton_Renew/0134-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 36% 397 - PKU_Skeleton_Renew/0134-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 36% 399 - PKU_Skeleton_Renew/0135-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 36% 401 - PKU_Skeleton_Renew/0135-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 36% 405 - PKU_Skeleton_Renew/0137-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 36% 407 - PKU_Skeleton_Renew/0137-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 36% 409 - PKU_Skeleton_Renew/0138-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 37% 410 - PKU_Skeleton_Renew/0138-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 37% 413 - PKU_Skeleton_Renew/0139-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 37% 415 - PKU_Skeleton_Renew/0140-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 37% 417 - PKU_Skeleton_Renew/0141-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 38% 418 - PKU_Skeleton_Renew/0141-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 38% 419 - PKU_Skeleton_Renew/0141-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 38% 422 - PKU_Skeleton_Renew/0142-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 38% 423 - PKU_Skeleton_Renew/0143-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 38% 425 - PKU_Skeleton_Renew/0143-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 39% 426 - PKU_Skeleton_Renew/0144-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 39% 428 - PKU_Skeleton_Renew/0144-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 39% 430 - PKU_Skeleton_Renew/0145-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 39% 433 - PKU_Skeleton_Renew/0146-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 39% 436 - PKU_Skeleton_Renew/0147-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 39% 438 - PKU_Skeleton_Renew/0148-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 40% 440 - PKU_Skeleton_Renew/0148-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 40% 442 - PKU_Skeleton_Renew/0149-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 40% 443 - PKU_Skeleton_Renew/0149-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 40% 445 - PKU_Skeleton_Renew/0150-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 40% 448 - PKU_Skeleton_Renew/0151-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 41% 450 - PKU_Skeleton_Renew/0152-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 41% 451 - PKU_Skeleton_Renew/0152-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 41% 453 - PKU_Skeleton_Renew/0153-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 41% 454 - PKU_Skeleton_Renew/0153-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 41% 457 - PKU_Skeleton_Renew/0154-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 42% 459 - PKU_Skeleton_Renew/0155-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 42% 462 - PKU_Skeleton_Renew/0156-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 42% 464 - PKU_Skeleton_Renew/0156-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 42% 466 - PKU_Skeleton_Renew/0157-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 42% 468 - PKU_Skeleton_Renew/0158-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 42% 471 - PKU_Skeleton_Renew/0159-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 43% 473 - PKU_Skeleton_Renew/0159-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 43% 476 - PKU_Skeleton_Renew/0160-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 43% 478 - PKU_Skeleton_Renew/0161-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 43% 480 - PKU_Skeleton_Renew/0162-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 44% 482 - PKU_Skeleton_Renew/0162-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 44% 484 - PKU_Skeleton_Renew/0163-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 44% 485 - PKU_Skeleton_Renew/0163-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 44% 487 - PKU_Skeleton_Renew/0164-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 44% 490 - PKU_Skeleton_Renew/0165-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 44% 493 - PKU_Skeleton_Renew/0166-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 45% 495 - PKU_Skeleton_Renew/0167-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 45% 497 - PKU_Skeleton_Renew/0167-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 45% 499 - PKU_Skeleton_Renew/0168-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 45% 501 - PKU_Skeleton_Renew/0169-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 46% 503 - PKU_Skeleton_Renew/0169-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 46% 505 - PKU_Skeleton_Renew/0170-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 46% 507 - PKU_Skeleton_Renew/0171-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 46% 508 - PKU_Skeleton_Renew/0171-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 46% 510 - PKU_Skeleton_Renew/0172-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 46% 511 - PKU_Skeleton_Renew/0172-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 47% 513 - PKU_Skeleton_Renew/0173-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 47% 514 - PKU_Skeleton_Renew/0173-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 47% 516 - PKU_Skeleton_Renew/0174-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 47% 517 - PKU_Skeleton_Renew/0174-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 47% 520 - PKU_Skeleton_Renew/0175-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 48% 522 - PKU_Skeleton_Renew/0176-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 48% 525 - PKU_Skeleton_Renew/0177-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 48% 527 - PKU_Skeleton_Renew/0177-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 48% 529 - PKU_Skeleton_Renew/0178-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 48% 531 - PKU_Skeleton_Renew/0179-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 49% 532 - PKU_Skeleton_Renew/0179-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 49% 535 - PKU_Skeleton_Renew/0180-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 49% 537 - PKU_Skeleton_Renew/0181-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 49% 538 - PKU_Skeleton_Renew/0181-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 49% 540 - PKU_Skeleton_Renew/0182-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 50% 543 - PKU_Skeleton_Renew/0183-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 50% 544 - PKU_Skeleton_Renew/0183-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 50% 546 - PKU_Skeleton_Renew/0184-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 50% 549 - PKU_Skeleton_Renew/0185-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 50% 553 - PKU_Skeleton_Renew/0186-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 51% 556 - PKU_Skeleton_Renew/0187-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 51% 557 - PKU_Skeleton_Renew/0187-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 51% 559 - PKU_Skeleton_Renew/0188-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 51% 561 - PKU_Skeleton_Renew/0189-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 51% 563 - PKU_Skeleton_Renew/0189-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 52% 565 - PKU_Skeleton_Renew/0190-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 52% 567 - PKU_Skeleton_Renew/0191-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 52% 569 - PKU_Skeleton_Renew/0191-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 52% 571 - PKU_Skeleton_Renew/0192-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 52% 574 - PKU_Skeleton_Renew/0193-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 53% 576 - PKU_Skeleton_Renew/0194-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 53% 578 - PKU_Skeleton_Renew/0194-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 53% 581 - PKU_Skeleton_Renew/0195-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 53% 585 - PKU_Skeleton_Renew/0197-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 53% 588 - PKU_Skeleton_Renew/0198-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 54% 589 - PKU_Skeleton_Renew/0198-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 54% 591 - PKU_Skeleton_Renew/0199-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 54% 594 - PKU_Skeleton_Renew/0200-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 54% 597 - PKU_Skeleton_Renew/0201-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 54% 598 - PKU_Skeleton_Renew/0201-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 55% 601 - PKU_Skeleton_Renew/0202-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 55% 603 - PKU_Skeleton_Renew/0203-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 55% 606 - PKU_Skeleton_Renew/0204-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 55% 608 - PKU_Skeleton_Renew/0204-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 55% 610 - PKU_Skeleton_Renew/0205-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 55% 614 - PKU_Skeleton_Renew/0206-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 56% 616 - PKU_Skeleton_Renew/0207-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 56% 619 - PKU_Skeleton_Renew/0208-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 56% 622 - PKU_Skeleton_Renew/0209-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 56% 625 - PKU_Skeleton_Renew/0210-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 56% 627 - PKU_Skeleton_Renew/0211-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 57% 629 - PKU_Skeleton_Renew/0211-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 57% 630 - PKU_Skeleton_Renew/0212-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 57% 631 - PKU_Skeleton_Renew/0212-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 57% 633 - PKU_Skeleton_Renew/0213-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 57% 635 - PKU_Skeleton_Renew/0213-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 58% 636 - PKU_Skeleton_Renew/0214-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 58% 638 - PKU_Skeleton_Renew/0214-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 58% 641 - PKU_Skeleton_Renew/0215-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 58% 645 - PKU_Skeleton_Renew/0217-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 58% 647 - PKU_Skeleton_Renew/0217-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 59% 649 - PKU_Skeleton_Renew/0218-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 59% 651 - PKU_Skeleton_Renew/0219-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 59% 652 - PKU_Skeleton_Renew/0219-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 59% 654 - PKU_Skeleton_Renew/0220-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 59% 656 - PKU_Skeleton_Renew/0220-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 60% 658 - PKU_Skeleton_Renew/0221-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 60% 660 - PKU_Skeleton_Renew/0222-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 60% 661 - PKU_Skeleton_Renew/0222-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 60% 663 - PKU_Skeleton_Renew/0223-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 60% 665 - PKU_Skeleton_Renew/0223-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 61% 667 - PKU_Skeleton_Renew/0224-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 61% 669 - PKU_Skeleton_Renew/0225-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 61% 673 - PKU_Skeleton_Renew/0226-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 61% 674 - PKU_Skeleton_Renew/0226-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 61% 677 - PKU_Skeleton_Renew/0227-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 62% 678 - PKU_Skeleton_Renew/0228-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 62% 680 - PKU_Skeleton_Renew/0228-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 62% 682 - PKU_Skeleton_Renew/0229-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 62% 683 - PKU_Skeleton_Renew/0229-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 62% 686 - PKU_Skeleton_Renew/0230-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 63% 688 - PKU_Skeleton_Renew/0231-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 63% 690 - PKU_Skeleton_Renew/0232-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 63% 691 - PKU_Skeleton_Renew/0232-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 63% 694 - PKU_Skeleton_Renew/0233-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 63% 696 - PKU_Skeleton_Renew/0234-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 64% 697 - PKU_Skeleton_Renew/0234-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 64% 700 - PKU_Skeleton_Renew/0235-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 64% 703 - PKU_Skeleton_Renew/0236-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 64% 706 - PKU_Skeleton_Renew/0237-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 64% 708 - PKU_Skeleton_Renew/0238-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 64% 709 - PKU_Skeleton_Renew/0238-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 65% 711 - PKU_Skeleton_Renew/0239-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 65% 713 - PKU_Skeleton_Renew/0239-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 65% 715 - PKU_Skeleton_Renew/0240-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 65% 716 - PKU_Skeleton_Renew/0240-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 65% 717 - PKU_Skeleton_Renew/0241-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 65% 718 - PKU_Skeleton_Renew/0241-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 66% 719 - PKU_Skeleton_Renew/0241-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 66% 721 - PKU_Skeleton_Renew/0242-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 66% 722 - PKU_Skeleton_Renew/0242-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 66% 724 - PKU_Skeleton_Renew/0243-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 66% 726 - PKU_Skeleton_Renew/0244-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 67% 728 - PKU_Skeleton_Renew/0244-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 67% 732 - PKU_Skeleton_Renew/0246-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 67% 735 - PKU_Skeleton_Renew/0251-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 67% 737 - PKU_Skeleton_Renew/0251-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 67% 739 - PKU_Skeleton_Renew/0252-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 68% 740 - PKU_Skeleton_Renew/0252-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 68% 742 - PKU_Skeleton_Renew/0253-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 68% 744 - PKU_Skeleton_Renew/0254-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 68% 746 - PKU_Skeleton_Renew/0254-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 68% 747 - PKU_Skeleton_Renew/0255-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 68% 749 - PKU_Skeleton_Renew/0255-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 69% 753 - PKU_Skeleton_Renew/0257-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 69% 754 - PKU_Skeleton_Renew/0257-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 69% 756 - PKU_Skeleton_Renew/0258-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 69% 758 - PKU_Skeleton_Renew/0258-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 69% 760 - PKU_Skeleton_Renew/0259-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 70% 761 - PKU_Skeleton_Renew/0259-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 70% 763 - PKU_Skeleton_Renew/0260-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 70% 765 - PKU_Skeleton_Renew/0261-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 70% 767 - PKU_Skeleton_Renew/0261-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 70% 770 - PKU_Skeleton_Renew/0262-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 70% 772 - PKU_Skeleton_Renew/0263-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 71% 774 - PKU_Skeleton_Renew/0264-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 71% 776 - PKU_Skeleton_Renew/0264-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 71% 777 - PKU_Skeleton_Renew/0265-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 71% 780 - PKU_Skeleton_Renew/0266-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 71% 781 - PKU_Skeleton_Renew/0266-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 72% 783 - PKU_Skeleton_Renew/0267-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 72% 784 - PKU_Skeleton_Renew/0267-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 72% 785 - PKU_Skeleton_Renew/0267-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 72% 787 - PKU_Skeleton_Renew/0268-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 72% 789 - PKU_Skeleton_Renew/0269-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 73% 790 - PKU_Skeleton_Renew/0269-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 73% 791 - PKU_Skeleton_Renew/0269-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 73% 792 - PKU_Skeleton_Renew/0270-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 73% 794 - PKU_Skeleton_Renew/0270-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 73% 797 - PKU_Skeleton_Renew/0271-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 73% 801 - PKU_Skeleton_Renew/0273-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 74% 803 - PKU_Skeleton_Renew/0273-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 74% 804 - PKU_Skeleton_Renew/0274-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 74% 807 - PKU_Skeleton_Renew/0275-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 74% 808 - PKU_Skeleton_Renew/0275-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 74% 810 - PKU_Skeleton_Renew/0276-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 74% 811 - PKU_Skeleton_Renew/0276-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 75% 813 - PKU_Skeleton_Renew/0277-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 75% 814 - PKU_Skeleton_Renew/0277-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 75% 816 - PKU_Skeleton_Renew/0278-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 75% 817 - PKU_Skeleton_Renew/0278-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 75% 819 - PKU_Skeleton_Renew/0279-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 76% 820 - PKU_Skeleton_Renew/0279-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 76% 822 - PKU_Skeleton_Renew/0280-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 76% 824 - PKU_Skeleton_Renew/0280-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 76% 825 - PKU_Skeleton_Renew/0281-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 76% 827 - PKU_Skeleton_Renew/0281-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 77% 828 - PKU_Skeleton_Renew/0282-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 77% 830 - PKU_Skeleton_Renew/0282-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 77% 832 - PKU_Skeleton_Renew/0283-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 77% 834 - PKU_Skeleton_Renew/0284-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 77% 836 - PKU_Skeleton_Renew/0284-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 78% 839 - PKU_Skeleton_Renew/0285-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 78% 842 - PKU_Skeleton_Renew/0286-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 78% 843 - PKU_Skeleton_Renew/0287-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 78% 845 - PKU_Skeleton_Renew/0287-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 78% 847 - PKU_Skeleton_Renew/0288-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 78% 849 - PKU_Skeleton_Renew/0289-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 79% 851 - PKU_Skeleton_Renew/0289-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 79% 853 - PKU_Skeleton_Renew/0290-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 79% 855 - PKU_Skeleton_Renew/0291-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 79% 857 - PKU_Skeleton_Renew/0291-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 80% 859 - PKU_Skeleton_Renew/0292-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 80% 861 - PKU_Skeleton_Renew/0293-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 80% 863 - PKU_Skeleton_Renew/0293-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 80% 865 - PKU_Skeleton_Renew/0294-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 80% 867 - PKU_Skeleton_Renew/0295-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 80% 868 - PKU_Skeleton_Renew/0295-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 81% 870 - PKU_Skeleton_Renew/0296-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 81% 872 - PKU_Skeleton_Renew/0296-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 81% 874 - PKU_Skeleton_Renew/0297-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 81% 875 - PKU_Skeleton_Renew/0297-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 81% 877 - PKU_Skeleton_Renew/0298-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 82% 878 - PKU_Skeleton_Renew/0298-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 82% 881 - PKU_Skeleton_Renew/0299-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 82% 885 - PKU_Skeleton_Renew/0301-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 82% 887 - PKU_Skeleton_Renew/0301-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 82% 889 - PKU_Skeleton_Renew/0302-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 82% 891 - PKU_Skeleton_Renew/0303-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 83% 892 - PKU_Skeleton_Renew/0303-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 83% 894 - PKU_Skeleton_Renew/0304-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 83% 895 - PKU_Skeleton_Renew/0304-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 83% 897 - PKU_Skeleton_Renew/0305-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 83% 899 - PKU_Skeleton_Renew/0305-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 84% 901 - PKU_Skeleton_Renew/0306-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 84% 903 - PKU_Skeleton_Renew/0307-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 84% 905 - PKU_Skeleton_Renew/0307-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 84% 908 - PKU_Skeleton_Renew/0308-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 84% 911 - PKU_Skeleton_Renew/0309-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 84% 915 - PKU_Skeleton_Renew/0311-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 85% 917 - PKU_Skeleton_Renew/0311-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 85% 921 - PKU_Skeleton_Renew/0313-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 85% 923 - PKU_Skeleton_Renew/0313-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 85% 926 - PKU_Skeleton_Renew/0314-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 85% 928 - PKU_Skeleton_Renew/0315-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 86% 929 - PKU_Skeleton_Renew/0315-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 86% 932 - PKU_Skeleton_Renew/0316-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 86% 934 - PKU_Skeleton_Renew/0317-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 86% 936 - PKU_Skeleton_Renew/0318-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 86% 937 - PKU_Skeleton_Renew/0318-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 87% 939 - PKU_Skeleton_Renew/0319-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 87% 942 - PKU_Skeleton_Renew/0320-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 87% 945 - PKU_Skeleton_Renew/0321-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 87% 946 - PKU_Skeleton_Renew/0321-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 87% 949 - PKU_Skeleton_Renew/0322-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 87% 952 - PKU_Skeleton_Renew/0323-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 88% 954 - PKU_Skeleton_Renew/0324-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 88% 956 - PKU_Skeleton_Renew/0324-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 88% 957 - PKU_Skeleton_Renew/0325-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 88% 959 - PKU_Skeleton_Renew/0325-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 89% 961 - PKU_Skeleton_Renew/0326-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 89% 963 - PKU_Skeleton_Renew/0327-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 89% 964 - PKU_Skeleton_Renew/0327-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 89% 966 - PKU_Skeleton_Renew/0328-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 89% 968 - PKU_Skeleton_Renew/0328-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 89% 970 - PKU_Skeleton_Renew/0329-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 90% 974 - PKU_Skeleton_Renew/0330-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 90% 976 - PKU_Skeleton_Renew/0331-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 90% 978 - PKU_Skeleton_Renew/0332-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 90% 981 - PKU_Skeleton_Renew/0333-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 90% 982 - PKU_Skeleton_Renew/0333-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 91% 983 - PKU_Skeleton_Renew/0333-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 91% 985 - PKU_Skeleton_Renew/0334-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 91% 987 - PKU_Skeleton_Renew/0335-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 91% 988 - PKU_Skeleton_Renew/0335-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 91% 990 - PKU_Skeleton_Renew/0336-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 92% 992 - PKU_Skeleton_Renew/0336-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 92% 994 - PKU_Skeleton_Renew/0337-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 92% 997 - PKU_Skeleton_Renew/0338-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 92% 998 - PKU_Skeleton_Renew/0338-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 92% 1001 - PKU_Skeleton_Renew/0339-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 92% 1003 - PKU_Skeleton_Renew/0340-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 93% 1006 - PKU_Skeleton_Renew/0341-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 93% 1007 - PKU_Skeleton_Renew/0341-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 93% 1009 - PKU_Skeleton_Renew/0342-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 93% 1012 - PKU_Skeleton_Renew/0343-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 93% 1014 - PKU_Skeleton_Renew/0344-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 94% 1015 - PKU_Skeleton_Renew/0344-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 94% 1017 - PKU_Skeleton_Renew/0345-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 94% 1019 - PKU_Skeleton_Renew/0345-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 94% 1021 - PKU_Skeleton_Renew/0346-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 94% 1022 - PKU_Skeleton_Renew/0346-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 94% 1024 - PKU_Skeleton_Renew/0347-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 95% 1025 - PKU_Skeleton_Renew/0347-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 95% 1027 - PKU_Skeleton_Renew/0348-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 95% 1028 - PKU_Skeleton_Renew/0348-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 95% 1031 - PKU_Skeleton_Renew/0349-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 95% 1035 - PKU_Skeleton_Renew/0351-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 96% 1037 - PKU_Skeleton_Renew/0351-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 96% 1038 - PKU_Skeleton_Renew/0352-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 96% 1041 - PKU_Skeleton_Renew/0353-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 96% 1043 - PKU_Skeleton_Renew/0353-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 96% 1044 - PKU_Skeleton_Renew/0354-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 97% 1045 - PKU_Skeleton_Renew/0354-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 97% 1047 - PKU_Skeleton_Renew/0355-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 97% 1049 - PKU_Skeleton_Renew/0355-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 97% 1050 - PKU_Skeleton_Renew/0356-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 97% 1052 - PKU_Skeleton_Renew/0356-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 98% 1054 - PKU_Skeleton_Renew/0357-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 98% 1056 - PKU_Skeleton_Renew/0358-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 98% 1058 - PKU_Skeleton_Renew/0358-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 98% 1060 - PKU_Skeleton_Renew/0359-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 98% 1064 - PKU_Skeleton_Renew/0360-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 98% 1066 - PKU_Skeleton_Renew/0361-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 99% 1068 - PKU_Skeleton_Renew/0362-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 99% 1070 - PKU_Skeleton_Renew/0362-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 99% 1072 - PKU_Skeleton_Renew/0363-M.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 99% 1074 - PKU_Skeleton_Renew/0364-L.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 99% 1076 - PKU_Skeleton_Renew/0364-R.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEverything is Ok\n",
            "\n",
            "Folders: 1\n",
            "Files: 1076\n",
            "Size:       7335616051\n",
            "Compressed: 1087979618\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTfxG0A_nLxT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "paths1 = os.listdir('/content/PKU_Skeleton_Renew')\n",
        "paths2 = os.listdir('/content/Train_Label_PKU_final')\n",
        "\n",
        "## Read skeleton renew\n",
        "paths1 = sorted(paths1)\n",
        "paths2 = sorted(paths2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMKS5xWL18SC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "splitFile = open('/content/cross-subject.txt','r')\n",
        "line = splitFile.readlines()\n",
        "train_list = list(line[1].strip().split(', '))\n",
        "test_list = list(line[3].strip().split(', '))\n",
        "train_list[-1] = train_list[-1][:-1]\n",
        "test_list[-1] = test_list[-1][:-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNhChLYD6YbX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMtEYScWnN-w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X1 = []\n",
        "X2 = []\n",
        "F = {}\n",
        "for i,f in enumerate(paths1):\n",
        "  fl = f[:-4]\n",
        "  p1 = os.path.join('/content/PKU_Skeleton_Renew', f)\n",
        "  p2 = os.path.join('/content/Train_Label_PKU_final', f)\n",
        "  fp1 = open(p1, \"r\")\n",
        "  fp2 = open(p2, \"r\")\n",
        "  x1 = [np.reshape(np.array(list(map(float, x.strip().split(' ')))[:75]),(25,3)) for x in fp1.readlines()]\n",
        "  x2 = [list(map(float, x.strip().split(','))) for x in fp2.readlines()]\n",
        "  # print(len(x1))\n",
        "  X1.append(np.array(x1))\n",
        "  X2.append(np.array(x2))\n",
        "  F[fl] = i\n",
        "# print(X1)\n",
        "# print(len(X2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDokFGFLnRb0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "def exp(x, x1, sigma):\n",
        "  return math.exp(-(x1-x)**2 / (sigma * sigma))\n",
        "## create labels\n",
        "Y = []\n",
        "C1 = []\n",
        "C2 = []\n",
        "for i in range(len(X2)):\n",
        "  s = len(X1[i])\n",
        "  seq = X2[i]\n",
        "  t1 = np.zeros((s, 52))\n",
        "  t1[:,0] = 1.\n",
        "  t2 = np.zeros(s)\n",
        "  t3 = np.zeros(s)\n",
        "  sigma = 10.\n",
        "  \n",
        "  for j in range(len(seq)):\n",
        "    # print(seq[j])\n",
        "    mn = 0\n",
        "    mx = len(seq)-1\n",
        "    label, st, en, _ = seq[j]\n",
        "    label = int(label)\n",
        "    st = int(st)\n",
        "    en = int(en)\n",
        "    t1[st-1:en, :] = 0.\n",
        "    t1[st-1:en, label] = 1.\n",
        "    l = st-1\n",
        "    r = en-1\n",
        "    for j in range(32):\n",
        "      if l-j >= mn:\n",
        "        t2[l-j] = exp(l, l-j, sigma)\n",
        "      if l+j <= mx:\n",
        "        t2[l+j] = exp(l, l+j, sigma)\n",
        "\n",
        "    for j in range(32):\n",
        "      if r-j >= mn:\n",
        "        t3[r-j] = exp(r, r-j, sigma)\n",
        "      if r+j <= mx:\n",
        "        t3[r+j] = exp(r, r+j, sigma)\n",
        "  \n",
        "  Y.append(np.array(t1))\n",
        "  C1.append(np.array(t2))\n",
        "  C2.append(np.array(t3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5OxR7g3nhx9",
        "colab_type": "code",
        "outputId": "bec22014-c9ae-4a2e-eded-f6dd94f12cf4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 61
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "tf.compat.v1.enable_eager_execution()\n",
        "tf.compat.v1.enable_v2_tensorshape()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qE-KBFenjIb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class Graph():\n",
        "    \"\"\" The Graph to model the skeletons extracted by the openpose\n",
        "    Args:\n",
        "        strategy (string): must be one of the follow candidates\n",
        "        - uniform: Uniform Labeling\n",
        "        - distance: Distance Partitioning\n",
        "        - spatial: Spatial Configuration\n",
        "        For more information, please refer to the section 'Partition Strategies'\n",
        "            in our paper (https://arxiv.org/abs/1801.07455).\n",
        "        layout (string): must be one of the follow candidates\n",
        "        - openpose: Is consists of 18 joints. For more information, please\n",
        "            refer to https://github.com/CMU-Perceptual-Computing-Lab/openpose#output\n",
        "        - ntu-rgb+d: Is consists of 25 joints. For more information, please\n",
        "            refer to https://github.com/shahroudy/NTURGB-D\n",
        "        max_hop (int): the maximal distance between two connected nodes\n",
        "        dilation (int): controls the spacing between the kernel points\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 layout='openpose',\n",
        "                 strategy='uniform',\n",
        "                 max_hop=1,\n",
        "                 dilation=1):\n",
        "        self.max_hop = max_hop\n",
        "        self.dilation = dilation\n",
        "\n",
        "        self.get_edge(layout)\n",
        "        self.hop_dis = get_hop_distance(\n",
        "            self.num_node, self.edge, max_hop=max_hop)\n",
        "        self.get_adjacency(strategy)\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.A\n",
        "\n",
        "    def get_edge(self, layout):\n",
        "        if layout == 'mobilenet':\n",
        "          self.num_node = 17\n",
        "          self_link = [(i, i) for i in range(self.num_node)]\n",
        "          neighbor_link = [(0, 1), (0, 2), (2, 4), (1, 3), (0, 5), (0, 6),(5,\n",
        "                                                                          6),\n",
        "                           (6, 8), (8, 10), (5, 7), (7, 9), (6, 12), (5, 11),\n",
        "                           (11, 12), (12, 14), (11, 13), (13, 15), (14, 16)]\n",
        "          self.edge = self_link + neighbor_link\n",
        "          self.center = 0\n",
        "        elif layout == 'openpose':\n",
        "            self.num_node = 18\n",
        "            self_link = [(i, i) for i in range(self.num_node)]\n",
        "            neighbor_link = [(4, 3), (3, 2), (7, 6), (6, 5), (13, 12), (12,\n",
        "                                                                        11),\n",
        "                             (10, 9), (9, 8), (11, 5), (8, 2), (5, 1), (2, 1),\n",
        "                             (0, 1), (15, 0), (14, 0), (17, 15), (16, 14)]\n",
        "            self.edge = self_link + neighbor_link\n",
        "            self.center = 1\n",
        "        elif layout == 'ntu-rgb+d':\n",
        "            self.num_node = 25\n",
        "            self_link = [(i, i) for i in range(self.num_node)]\n",
        "            neighbor_1base = [(1, 2), (2, 21), (3, 21), (4, 3), (5, 21),\n",
        "                              (6, 5), (7, 6), (8, 7), (9, 21), (10, 9),\n",
        "                              (11, 10), (12, 11), (13, 1), (14, 13), (15, 14),\n",
        "                              (16, 15), (17, 1), (18, 17), (19, 18), (20, 19),\n",
        "                              (22, 23), (23, 8), (24, 25), (25, 12)]\n",
        "            neighbor_link = [(i - 1, j - 1) for (i, j) in neighbor_1base]\n",
        "            self.edge = self_link + neighbor_link\n",
        "            self.center = 21 - 1\n",
        "        elif layout == 'ntu_edge':\n",
        "            self.num_node = 24\n",
        "            self_link = [(i, i) for i in range(self.num_node)]\n",
        "            neighbor_1base = [(1, 2), (3, 2), (4, 3), (5, 2), (6, 5), (7, 6),\n",
        "                              (8, 7), (9, 2), (10, 9), (11, 10), (12, 11),\n",
        "                              (13, 1), (14, 13), (15, 14), (16, 15), (17, 1),\n",
        "                              (18, 17), (19, 18), (20, 19), (21, 22), (22, 8),\n",
        "                              (23, 24), (24, 12)]\n",
        "            neighbor_link = [(i - 1, j - 1) for (i, j) in neighbor_1base]\n",
        "            self.edge = self_link + neighbor_link\n",
        "            self.center = 2\n",
        "        # elif layout=='customer settings'\n",
        "        #     pass\n",
        "        else:\n",
        "            raise ValueError(\"Do Not Exist This Layout.\")\n",
        "\n",
        "    def get_adjacency(self, strategy):\n",
        "        valid_hop = range(0, self.max_hop + 1, self.dilation)\n",
        "        adjacency = np.zeros((self.num_node, self.num_node))\n",
        "        for hop in valid_hop:\n",
        "            adjacency[self.hop_dis == hop] = 1\n",
        "        normalize_adjacency = normalize_digraph(adjacency)\n",
        "\n",
        "        if strategy == 'uniform':\n",
        "            A = np.zeros((1, self.num_node, self.num_node))\n",
        "            A[0] = normalize_adjacency\n",
        "            self.A = A\n",
        "        elif strategy == 'distance':\n",
        "            A = np.zeros((len(valid_hop), self.num_node, self.num_node))\n",
        "            for i, hop in enumerate(valid_hop):\n",
        "                A[i][self.hop_dis == hop] = normalize_adjacency[self.hop_dis ==\n",
        "                                                                hop]\n",
        "            self.A = A\n",
        "        elif strategy == 'spatial':\n",
        "            A = []\n",
        "            for hop in valid_hop:\n",
        "                a_root = np.zeros((self.num_node, self.num_node))\n",
        "                a_close = np.zeros((self.num_node, self.num_node))\n",
        "                a_further = np.zeros((self.num_node, self.num_node))\n",
        "                for i in range(self.num_node):\n",
        "                    for j in range(self.num_node):\n",
        "                        if self.hop_dis[j, i] == hop:\n",
        "                            if self.hop_dis[j, self.center] == self.hop_dis[\n",
        "                                    i, self.center]:\n",
        "                                a_root[j, i] = normalize_adjacency[j, i]\n",
        "                            elif self.hop_dis[j, self.\n",
        "                                              center] > self.hop_dis[i, self.\n",
        "                                                                     center]:\n",
        "                                a_close[j, i] = normalize_adjacency[j, i]\n",
        "                            else:\n",
        "                                a_further[j, i] = normalize_adjacency[j, i]\n",
        "                if hop == 0:\n",
        "                    A.append(a_root)\n",
        "                else:\n",
        "                    A.append(a_root + a_close)\n",
        "                    A.append(a_further)\n",
        "            A = np.stack(A)\n",
        "            self.A = A\n",
        "        else:\n",
        "            raise ValueError(\"Do Not Exist This Strategy\")\n",
        "\n",
        "\n",
        "def get_hop_distance(num_node, edge, max_hop=1):\n",
        "    A = np.zeros((num_node, num_node))\n",
        "    for i, j in edge:\n",
        "        A[j, i] = 1\n",
        "        A[i, j] = 1\n",
        "\n",
        "    # compute hop steps\n",
        "    hop_dis = np.zeros((num_node, num_node)) + np.inf\n",
        "    transfer_mat = [np.linalg.matrix_power(A, d) for d in range(max_hop + 1)]\n",
        "    arrive_mat = (np.stack(transfer_mat) > 0)\n",
        "    for d in range(max_hop, -1, -1):\n",
        "        hop_dis[arrive_mat[d]] = d\n",
        "    return hop_dis\n",
        "\n",
        "\n",
        "def normalize_digraph(A):\n",
        "    Dl = np.sum(A, 0)\n",
        "    num_node = A.shape[0]\n",
        "    Dn = np.zeros((num_node, num_node))\n",
        "    for i in range(num_node):\n",
        "        if Dl[i] > 0:\n",
        "            Dn[i, i] = Dl[i]**(-1)\n",
        "    AD = np.dot(A, Dn)\n",
        "    return AD\n",
        "\n",
        "\n",
        "def normalize_undigraph(A):\n",
        "    Dl = np.sum(A, 0)\n",
        "    num_node = A.shape[0]\n",
        "    Dn = np.zeros((num_node, num_node))\n",
        "    for i in range(num_node):\n",
        "        if Dl[i] > 0:\n",
        "            Dn[i, i] = Dl[i]**(-0.5)\n",
        "    DAD = np.dot(np.dot(Dn, A), Dn)\n",
        "    return DAD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dde9EOPT1Z-J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ConvTemporalGraphical(tf.keras.layers.Layer):\n",
        "  r\"\"\"The basic modulle for applying a graph convolution.\n",
        "\n",
        "  Args:\n",
        "    in_channels (int): Number of channels in the input sequence of data\n",
        "    out_channels (int): Number of channels produced by the convolution\n",
        "    kernel_size (int): Size of graph convolving kernals\n",
        "    t_kernel_size (int): Size of temporal convolving kernal\n",
        "    t_stride (int, optional): Stride of temporal convolution. Default: 1\n",
        "    t_padding (int, optional): Temporal zero-padding added to both \n",
        "      sides of the input, Default: 0\n",
        "    t_dilation (int, optional): Spacing between temporal kernel elements.\n",
        "      Default: 1\n",
        "    bias (bool, optional): If ``True``, addas a learnable bias to the output.\n",
        "      Default:  ``True``\n",
        "\n",
        "  Shape:\n",
        "    - Input[0]: Input graph sequence in (N, in_channels, T_in, V) format\n",
        "    - Input[1]: Input graph adjacency matrix in (K, V, V) format\n",
        "    - Output[0]: Output graph sequence in (N, out_channels, T_out, V) format\n",
        "    - Output[1]: Graph adjacency matrix for output data in (K, V, V) format\n",
        "\n",
        "    where\n",
        "      N is a batch size,\n",
        "      K is the spatial kernel size, as K == kernel_size[1]\n",
        "      T_in/T_out is a length of input/output sequence\n",
        "      V is the number of graph nodes\n",
        "    \"\"\"    \n",
        "  def __init__(self,\n",
        "               in_channels,\n",
        "               out_channels,\n",
        "               kernel_size,\n",
        "               t_kernel_size=1,\n",
        "               t_stride=1,\n",
        "               t_padding='same',\n",
        "               t_dilation=1,\n",
        "               bias=True):\n",
        "    super(ConvTemporalGraphical, self).__init__()\n",
        "    self.kernel_size = kernel_size\n",
        "    self.conv = tf.keras.layers.Conv2D(\n",
        "        out_channels*kernel_size,\n",
        "        kernel_size=(t_kernel_size, 1),\n",
        "        padding=t_padding,\n",
        "        strides=(t_stride, 1),\n",
        "        dilation_rate=(t_dilation, 1),\n",
        "        use_bias=bias,\n",
        "        data_format='channels_first'\n",
        "    )\n",
        "\n",
        "  def build(self, input_shape):\n",
        "      # # Create a trainable weight variable for this layer.\n",
        "      # self.kernel = self.add_weight(name='kernel', \n",
        "      #                               shape=(input_shape[1], self.output_dim),\n",
        "      #                               initializer='uniform',\n",
        "      #                               trainable=True)\n",
        "      self.built = True\n",
        "      # super(MyLayer, self).build(input_shape)  # Be sure to call this at the end\n",
        "\n",
        "  def call(self, x, A):\n",
        "      x = self.conv(x)\n",
        "      # x = tf.transpose(x, perm=[0, 3, 1, 2])\n",
        "      shape = tf.shape(x)\n",
        "      n = shape[0]\n",
        "      kc = shape[1]\n",
        "      t = shape[2]\n",
        "      v = shape[3]\n",
        "      x = tf.reshape(x, shape=(n, self.kernel_size, kc//self.kernel_size, t, v))\n",
        "      x = tf.einsum('nkctv,kvw->nctw',x,A)\n",
        "      # x = tf.transpose(x, perm=[0, 2, 3, 1])\n",
        "      return x, A\n",
        "\n",
        "  # def compute_output_shape(self, input_shape):\n",
        "  #     return (input_shape[0], self.output_dim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yJ8Jloy1cCN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class st_gcn(tf.keras.Model):\n",
        "    r\"\"\"Applies a spatial temporal graph convolution over an input graph sequence.\n",
        "    Args:\n",
        "        in_channels (int): Number of channels in the input sequence data\n",
        "        out_channels (int): Number of channels produced by the convolution\n",
        "        kernel_size (tuple): Size of the temporal convolving kernel and graph convolving kernel\n",
        "        stride (int, optional): Stride of the temporal convolution. Default: 1\n",
        "        dropout (int, optional): Dropout rate of the final output. Default: 0\n",
        "        residual (bool, optional): If ``True``, applies a residual mechanism. Default: ``True``\n",
        "    Shape:\n",
        "        - Input[0]: Input graph sequence in :math:`(N, in_channels, T_{in}, V)` format\n",
        "        - Input[1]: Input graph adjacency matrix in :math:`(K, V, V)` format\n",
        "        - Output[0]: Outpu graph sequence in :math:`(N, out_channels, T_{out}, V)` format\n",
        "        - Output[1]: Graph adjacency matrix for output data in :math:`(K, V, V)` format\n",
        "        where\n",
        "            :math:`N` is a batch size,\n",
        "            :math:`K` is the spatial kernel size, as :math:`K == kernel_size[1]`,\n",
        "            :math:`T_{in}/T_{out}` is a length of input/output sequence,\n",
        "            :math:`V` is the number of graph nodes.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "               in_channels,\n",
        "               out_channels,\n",
        "               kernel_size,\n",
        "               stride=1,\n",
        "               dropout=0,\n",
        "               residual=True):\n",
        "      super().__init__()\n",
        "      assert len(kernel_size) == 2\n",
        "      assert kernel_size[0] % 2 == 1\n",
        "      padding = ((kernel_size[0] - 1) // 2, 0)\n",
        "      self.gcn = ConvTemporalGraphical(in_channels, out_channels, kernel_size[1])\n",
        "      self.tcn = tf.keras.models.Sequential(\n",
        "          [\n",
        "          tf.keras.layers.BatchNormalization(axis=1),\n",
        "          tf.keras.layers.ReLU(),\n",
        "          tf.keras.layers.Conv2D(\n",
        "              out_channels,\n",
        "              (kernel_size[0], 1),\n",
        "              (stride, 1),\n",
        "              padding='same',\n",
        "              data_format='channels_first'\n",
        "          ),\n",
        "          tf.keras.layers.BatchNormalization(axis=1),\n",
        "          tf.keras.layers.Dropout(dropout)\n",
        "          ]\n",
        "      )\n",
        "      if not residual:\n",
        "        self.residual = lambda x, training: 0\n",
        "\n",
        "      elif (in_channels == out_channels) and (stride == 1):\n",
        "        self.residual = lambda x, y:  x\n",
        "\n",
        "      else:\n",
        "        self.residual = tf.keras.models.Sequential(\n",
        "          [\n",
        "          tf.keras.layers.Conv2D(\n",
        "              out_channels,\n",
        "              1,\n",
        "              (stride, 1),\n",
        "              data_format='channels_first'\n",
        "          ),\n",
        "          tf.keras.layers.BatchNormalization(axis=1),\n",
        "          ]\n",
        "        )\n",
        "      self.relu = tf.keras.layers.ReLU()\n",
        "    def call(self, x, A, training=True):\n",
        "      res = self.residual(x, training)\n",
        "      x, A = self.gcn(x, A)\n",
        "      x = self.tcn(x, training=training) + res\n",
        "\n",
        "      return self.relu(x), A"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PD4jm4x1qPX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model(tf.keras.Model):\n",
        "    r\"\"\"Spatial temporal graph convolutional networks.\n",
        "    Args:\n",
        "        in_channels (int): Number of channels in the input data\n",
        "        num_class (int): Number of classes for the classification task\n",
        "        graph_args (dict): The arguments for building the graph\n",
        "        edge_importance_weighting (bool): If ``True``, adds a learnable\n",
        "            importance weighting to the edges of the graph\n",
        "        **kwargs (optional): Other parameters for graph convolution units\n",
        "    Shape:\n",
        "        - Input: :math:`(N, in_channels, T_{in}, V_{in}, M_{in})`\n",
        "        - Output: :math:`(N, num_class)` where\n",
        "            :math:`N` is a batch size,\n",
        "            :math:`T_{in}` is a length of input sequence,\n",
        "            :math:`V_{in}` is the number of graph nodes,\n",
        "            :math:`M_{in}` is the number of instance in a frame.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, num_class, graph_args,\n",
        "                 edge_importance_weighting, **kwargs):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        # load graph\n",
        "        self.graph = Graph(**graph_args)\n",
        "        A = tf.convert_to_tensor(self.graph.A, dtype=tf.float32)    \n",
        "        # self.register_buffer('A', A)\n",
        "        self.A = A\n",
        "        # build networks\n",
        "        spatial_kernel_size = self.graph.A.shape[0]\n",
        "        temporal_kernel_size = 9\n",
        "        kernel_size = (temporal_kernel_size, spatial_kernel_size)\n",
        "        self.data_bn = tf.keras.layers.BatchNormalization()\n",
        "        kwargs0 = {k: v for k, v in kwargs.items() if k != 'dropout'}\n",
        "        self.st_gcn_networks = (\n",
        "            st_gcn(in_channels, 64, kernel_size, 1, residual=False, **kwargs0),\n",
        "            st_gcn(64, 64, kernel_size, 1, **kwargs),\n",
        "            st_gcn(64, 64, kernel_size, 1, **kwargs),\n",
        "            st_gcn(64, 64, kernel_size, 1, **kwargs),\n",
        "            st_gcn(64, 128, kernel_size, 2, **kwargs),\n",
        "            st_gcn(128, 128, kernel_size, 1, **kwargs),\n",
        "            st_gcn(128, 128, kernel_size, 1, **kwargs),\n",
        "            st_gcn(128, 256, kernel_size, 2, **kwargs),\n",
        "            st_gcn(256, 256, kernel_size, 1, **kwargs),\n",
        "            st_gcn(256, 256, kernel_size, 1, **kwargs),\n",
        "        )\n",
        "        self.st_gcn0 = self.st_gcn_networks[0]\n",
        "        self.st_gcn1 = self.st_gcn_networks[1]\n",
        "        self.st_gcn2 = self.st_gcn_networks[2]\n",
        "        self.st_gcn3 = self.st_gcn_networks[3]\n",
        "        self.st_gcn4 = self.st_gcn_networks[4]\n",
        "        self.st_gcn5 = self.st_gcn_networks[5]\n",
        "        self.st_gcn6 = self.st_gcn_networks[6]\n",
        "        self.st_gcn7 = self.st_gcn_networks[7]\n",
        "        self.st_gcn8 = self.st_gcn_networks[8]\n",
        "        self.st_gcn9 = self.st_gcn_networks[9]\n",
        "        # initialize parameters for edge importance weighting\n",
        "        # if edge_importance_weighting:\n",
        "        #     self.edge_importance = nn.ParameterList([\n",
        "        #         nn.Parameter(torch.ones(self.A.size()))\n",
        "        #         for i in self.st_gcn_networks\n",
        "        #     ])\n",
        "        # else:\n",
        "        self.edge_importance = [1] * len(self.st_gcn_networks)\n",
        "\n",
        "        # fcn for prediction\n",
        "        self.fcnprefinal = tf.keras.layers.Conv2D(256, kernel_size=1, data_format='channels_first')\n",
        "        self.fcn = tf.keras.layers.Conv2D(num_class, kernel_size=1, data_format='channels_first')\n",
        "\n",
        "    def call(self, x, training=True):\n",
        "\n",
        "        # data normalization\n",
        "        x = tf.cast(x, tf.float32)\n",
        "        shape = tf.shape(x)\n",
        "        N = shape[0]\n",
        "        C = shape[1]\n",
        "        T = shape[2]\n",
        "        V = shape[3]\n",
        "        M = shape[4]\n",
        "        x = tf.transpose(x, (0, 4, 3, 1, 2))\n",
        "        x = tf.reshape(x, (N * M, V * C, T))\n",
        "        x = self.data_bn(x, training=training)\n",
        "        x = tf.reshape(x, (N, M, V, C, T))\n",
        "        x = tf.transpose(x, perm=(0, 1, 3, 4, 2))\n",
        "        x = tf.reshape(x, (N * M, C, T, V))\n",
        "        # forwad\n",
        "        # print(\"A shape\", self.A.shape)\n",
        "        for gcn, importance in zip(self.st_gcn_networks, self.edge_importance):\n",
        "            x, _ = gcn(x, self.A * importance, training=training)\n",
        "            # print(\"gcned\", x.shape)\n",
        "\n",
        "        # global pooling\n",
        "        d1 = tf.shape(x)[2]\n",
        "        d2 = tf.shape(x)[3]\n",
        "        x = tf.keras.layers.AveragePooling2D((d1, d2), data_format='channels_first')(x)\n",
        "        ## Uncomment this for frames = 25 for conversion to tflite\n",
        "        # x = tf.keras.layers.AveragePooling2D((7, 17), data_format='channels_first')(x)\n",
        "        x = tf.math.reduce_mean(tf.reshape(x, (N, M, -1, 1, 1)), axis=1)\n",
        "        # prediction\n",
        "\n",
        "        pref_op = self.fcnprefinal(x)\n",
        "        pref_op = tf.reshape(pref_op, (tf.shape(x)[0],-1))\n",
        "\n",
        "        x = self.fcn(x)\n",
        "        \n",
        "        x = tf.reshape(x, (tf.shape(x)[0], -1))\n",
        "        \n",
        "        x = tf.keras.layers.Softmax()(x)\n",
        "        return x, pref_op\n",
        "       \n",
        "    def extract_feature(self, x):\n",
        "\n",
        "        # data normalization\n",
        "        shape = tf.shape(x)\n",
        "        N = shape[0]\n",
        "        C = shape[1]\n",
        "        T = shape[2]\n",
        "        V = shape[3]\n",
        "        M = shape[4]\n",
        "        x = tf.transpose(x, (0, 4, 3, 1, 2))\n",
        "        x = tf.reshape(x, (N * M, V * C, T))\n",
        "        # batch normalization\n",
        "        # x = self.data_bn(x)\n",
        "        x = tf.reshape(x, (N, M, V, C, T))\n",
        "        x = tf.transpose(x, perm=(0, 1, 3, 4, 2))\n",
        "        x = tf.reshape(x, (N * M, C, T, V))\n",
        "\n",
        "        # forwad\n",
        "        for gcn, importance in zip(self.st_gcn_networks, self.edge_importance):\n",
        "            x, _ = gcn(x, self.A * importance)\n",
        "\n",
        "        shape = tf.shape(x)\n",
        "        c = shape[1]\n",
        "        t = shape[2]\n",
        "        v = shape[3]\n",
        "        feature = tf.transpose(tf.reshape(x, (N, M, c, t, v)), (0, 2, 3, 4, 1))\n",
        "\n",
        "        # prediction\n",
        "        x = self.fcn(x)\n",
        "        output = tf.transpose(tf.reshape(x, (N, M, -1, t, v)),(0, 2, 3, 4, 1))\n",
        "\n",
        "        return output, feature\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIDqbR-q1sIW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss(m, x, y, loss_object):\n",
        "  y_ ,_= m(x)\n",
        "  return loss_object(y, y_)\n",
        "\n",
        "def grad(m, inputs, targets, loss_object):\n",
        "  with tf.GradientTape(persistent=True) as tape:\n",
        "    loss_value = loss(m, inputs, targets, loss_object)\n",
        "  return loss_value, tape.gradient(loss_value, m.trainable_variables)\n",
        "\n",
        "model = Model(3, 52, {'layout': 'ntu-rgb+d', 'strategy': 'spatial'}, True)\n",
        "num_batches = 5000\n",
        "batch_size = 128\n",
        "num_frames = 20\n",
        "num_steps = 1\n",
        "train_ex = [F[i] for i in train_list]\n",
        "test_ex = [F[i] for i in test_list]\n",
        "\n",
        "X1 = np.array(X1)\n",
        "Y = np.array(Y)\n",
        "rawX = X1\n",
        "rawY = Y\n",
        "\n",
        "for lr in [5e-5]:\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=lr) \n",
        "  for step in range(num_steps):\n",
        "    for bnum in range(num_batches):\n",
        "      batchX = []\n",
        "      batchY = []\n",
        "      ct1 = 0\n",
        "      ct = batch_size // 10\n",
        "      while ct1 < batch_size:\n",
        "        t = np.random.choice(train_ex)\n",
        "        n = rawX[t].shape[0]\n",
        "        i = np.random.randint(n-num_frames)\n",
        "        # print(t,i+num_frames-1)\n",
        "        if rawY[t][i+num_frames-1][0] == 1.0:\n",
        "          if ct > 0:\n",
        "            ct -= 1\n",
        "            ct1 += 1\n",
        "          else: continue\n",
        "        else:\n",
        "          ct1 += 1\n",
        "        batchX.append(rawX[t][i:i+num_frames])\n",
        "        batchY.append(rawY[t][i:i+num_frames])\n",
        "      batchX = np.array(batchX)\n",
        "      batchY = np.array(batchY)\n",
        "      batchY = batchY[:, -1, :]\n",
        "      batchX = np.transpose(batchX, (0, 3, 1, 2))\n",
        "      batchX = np.expand_dims(batchX, axis=4)\n",
        "      # print(\"Batch : \", bnum)\n",
        "      # print(\"Batch X shape : \", batchX.shape, \", Batch Y shape : \", batchY.shape)\n",
        "      cce = tf.keras.losses.CategoricalCrossentropy()\n",
        "      loss_value, grads = grad(model, batchX, batchY, cce)\n",
        "      optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "      print(\"Step# \",step,\" , Batch# \", bnum, \" , Loss: \", loss_value.numpy()) \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAnvsCbYyivJ",
        "colab_type": "code",
        "outputId": "970f7907-8021-401f-f51a-3c2cb69c9183",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# testing \n",
        "tot = 0\n",
        "tp,fp,fn = 0,0,0\n",
        "for t in test_ex:\n",
        "  N = rawX[t].shape[0]\n",
        "  print(\"Total frames in {}: {}\".format(t, N))\n",
        "  batchX = []\n",
        "  batchY = []\n",
        "  for i in range(num_frames, N+1,3):\n",
        "    batchX.append(rawX[t][i-num_frames:i])\n",
        "    batchY.append(rawY[t][i-num_frames:i])\n",
        "  batchX = np.array(batchX)\n",
        "  batchY = np.array(batchY)\n",
        "  batchY = batchY[:, -1, :]\n",
        "  batchX = np.transpose(batchX, (0, 3, 1, 2))\n",
        "  batchX = np.expand_dims(batchX, axis=4)\n",
        "  print(\"Batch X shape : \", batchX.shape, \", Batch Y shape : \", batchY.shape)\n",
        "  # cce = tf.keras.losses.CategoricalCrossentropy()\n",
        "  preds,_ = model(batchX, training=False)\n",
        "  xx,yy,zz = f1(preds.numpy(),batchY)\n",
        "  tp+=xx\n",
        "  fp+=yy\n",
        "  fn+=zz\n",
        "  # print(np.shape(preds))\n",
        "  a = np.argmax(preds, axis=1)\n",
        "  b = np.argmax(batchY, axis=1)\n",
        "  # print(\"ground truth\", b)\n",
        "  # print(\"predictions\", a)\n",
        "\n",
        "  c = (a==b)\n",
        "  # print(np.sum(c.astype(int))/c.shape[0])\n",
        "  acc = np.sum(c.astype(int))/c.shape[0]\n",
        "  tot += acc\n",
        "  print(\"Accuracy of this example = \",acc*100)\n",
        "\n",
        "prec = tp/(tp+fp)\n",
        "rec = tp/(tp+fn)\n",
        "f1sc = (2*prec*rec)/(prec+rec)\n",
        "print(\"Test accuracy : \", (tot/len(test_ex)) * 100.)\n",
        "print(\"F1-score = \", f1sc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total frames in 854: 6455\n",
            "Batch X shape :  (2146, 3, 20, 25, 1) , Batch Y shape :  (2146, 52)\n",
            "Accuracy of this example =  51.16495806150979\n",
            "Total frames in 855: 6467\n",
            "Batch X shape :  (2150, 3, 20, 25, 1) , Batch Y shape :  (2150, 52)\n",
            "Accuracy of this example =  48.7906976744186\n",
            "Total frames in 856: 5381\n",
            "Batch X shape :  (1788, 3, 20, 25, 1) , Batch Y shape :  (1788, 52)\n",
            "Accuracy of this example =  50.33557046979866\n",
            "Total frames in 857: 6202\n",
            "Batch X shape :  (2061, 3, 20, 25, 1) , Batch Y shape :  (2061, 52)\n",
            "Accuracy of this example =  42.74623968947113\n",
            "Total frames in 858: 6228\n",
            "Batch X shape :  (2070, 3, 20, 25, 1) , Batch Y shape :  (2070, 52)\n",
            "Accuracy of this example =  37.53623188405797\n",
            "Total frames in 859: 5062\n",
            "Batch X shape :  (1681, 3, 20, 25, 1) , Batch Y shape :  (1681, 52)\n",
            "Accuracy of this example =  34.800713860797146\n",
            "Total frames in 860: 5690\n",
            "Batch X shape :  (1891, 3, 20, 25, 1) , Batch Y shape :  (1891, 52)\n",
            "Accuracy of this example =  52.03595980962453\n",
            "Total frames in 861: 5671\n",
            "Batch X shape :  (1884, 3, 20, 25, 1) , Batch Y shape :  (1884, 52)\n",
            "Accuracy of this example =  54.989384288747345\n",
            "Total frames in 862: 4645\n",
            "Batch X shape :  (1542, 3, 20, 25, 1) , Batch Y shape :  (1542, 52)\n",
            "Accuracy of this example =  48.50843060959792\n",
            "Total frames in 863: 4654\n",
            "Batch X shape :  (1545, 3, 20, 25, 1) , Batch Y shape :  (1545, 52)\n",
            "Accuracy of this example =  54.239482200647245\n",
            "Total frames in 864: 4642\n",
            "Batch X shape :  (1541, 3, 20, 25, 1) , Batch Y shape :  (1541, 52)\n",
            "Accuracy of this example =  57.88449059052563\n",
            "Total frames in 865: 3879\n",
            "Batch X shape :  (1287, 3, 20, 25, 1) , Batch Y shape :  (1287, 52)\n",
            "Accuracy of this example =  51.82595182595182\n",
            "Total frames in 866: 6365\n",
            "Batch X shape :  (2116, 3, 20, 25, 1) , Batch Y shape :  (2116, 52)\n",
            "Accuracy of this example =  51.79584120982986\n",
            "Total frames in 867: 6319\n",
            "Batch X shape :  (2100, 3, 20, 25, 1) , Batch Y shape :  (2100, 52)\n",
            "Accuracy of this example =  58.666666666666664\n",
            "Total frames in 868: 5205\n",
            "Batch X shape :  (1729, 3, 20, 25, 1) , Batch Y shape :  (1729, 52)\n",
            "Accuracy of this example =  55.8126084441874\n",
            "Total frames in 869: 6528\n",
            "Batch X shape :  (2170, 3, 20, 25, 1) , Batch Y shape :  (2170, 52)\n",
            "Accuracy of this example =  62.58064516129033\n",
            "Total frames in 870: 6519\n",
            "Batch X shape :  (2167, 3, 20, 25, 1) , Batch Y shape :  (2167, 52)\n",
            "Accuracy of this example =  58.69866174434703\n",
            "Total frames in 871: 5430\n",
            "Batch X shape :  (1804, 3, 20, 25, 1) , Batch Y shape :  (1804, 52)\n",
            "Accuracy of this example =  57.92682926829268\n",
            "Total frames in 872: 6756\n",
            "Batch X shape :  (2246, 3, 20, 25, 1) , Batch Y shape :  (2246, 52)\n",
            "Accuracy of this example =  65.18254674977739\n",
            "Total frames in 873: 6766\n",
            "Batch X shape :  (2249, 3, 20, 25, 1) , Batch Y shape :  (2249, 52)\n",
            "Accuracy of this example =  71.23165851489551\n",
            "Total frames in 874: 5595\n",
            "Batch X shape :  (1859, 3, 20, 25, 1) , Batch Y shape :  (1859, 52)\n",
            "Accuracy of this example =  65.4653039268424\n",
            "Total frames in 875: 5855\n",
            "Batch X shape :  (1946, 3, 20, 25, 1) , Batch Y shape :  (1946, 52)\n",
            "Accuracy of this example =  56.42343268242549\n",
            "Total frames in 876: 5912\n",
            "Batch X shape :  (1965, 3, 20, 25, 1) , Batch Y shape :  (1965, 52)\n",
            "Accuracy of this example =  64.68193384223918\n",
            "Total frames in 877: 5006\n",
            "Batch X shape :  (1663, 3, 20, 25, 1) , Batch Y shape :  (1663, 52)\n",
            "Accuracy of this example =  63.680096211665656\n",
            "Total frames in 878: 2366\n",
            "Batch X shape :  (783, 3, 20, 25, 1) , Batch Y shape :  (783, 52)\n",
            "Accuracy of this example =  61.1749680715198\n",
            "Total frames in 879: 2378\n",
            "Batch X shape :  (787, 3, 20, 25, 1) , Batch Y shape :  (787, 52)\n",
            "Accuracy of this example =  58.83100381194409\n",
            "Total frames in 880: 2060\n",
            "Batch X shape :  (681, 3, 20, 25, 1) , Batch Y shape :  (681, 52)\n",
            "Accuracy of this example =  71.95301027900148\n",
            "Total frames in 881: 2079\n",
            "Batch X shape :  (687, 3, 20, 25, 1) , Batch Y shape :  (687, 52)\n",
            "Accuracy of this example =  65.06550218340611\n",
            "Total frames in 882: 2085\n",
            "Batch X shape :  (689, 3, 20, 25, 1) , Batch Y shape :  (689, 52)\n",
            "Accuracy of this example =  49.63715529753265\n",
            "Total frames in 883: 1793\n",
            "Batch X shape :  (592, 3, 20, 25, 1) , Batch Y shape :  (592, 52)\n",
            "Accuracy of this example =  73.47972972972973\n",
            "Total frames in 884: 6889\n",
            "Batch X shape :  (2290, 3, 20, 25, 1) , Batch Y shape :  (2290, 52)\n",
            "Accuracy of this example =  57.554585152838435\n",
            "Total frames in 885: 6833\n",
            "Batch X shape :  (2272, 3, 20, 25, 1) , Batch Y shape :  (2272, 52)\n",
            "Accuracy of this example =  61.3556338028169\n",
            "Total frames in 886: 5691\n",
            "Batch X shape :  (1891, 3, 20, 25, 1) , Batch Y shape :  (1891, 52)\n",
            "Accuracy of this example =  59.12215758857747\n",
            "Total frames in 887: 5729\n",
            "Batch X shape :  (1904, 3, 20, 25, 1) , Batch Y shape :  (1904, 52)\n",
            "Accuracy of this example =  67.85714285714286\n",
            "Total frames in 888: 5834\n",
            "Batch X shape :  (1939, 3, 20, 25, 1) , Batch Y shape :  (1939, 52)\n",
            "Accuracy of this example =  72.51160391954615\n",
            "Total frames in 889: 4766\n",
            "Batch X shape :  (1583, 3, 20, 25, 1) , Batch Y shape :  (1583, 52)\n",
            "Accuracy of this example =  67.46683512318383\n",
            "Total frames in 890: 6378\n",
            "Batch X shape :  (2120, 3, 20, 25, 1) , Batch Y shape :  (2120, 52)\n",
            "Accuracy of this example =  64.00943396226415\n",
            "Total frames in 891: 6449\n",
            "Batch X shape :  (2144, 3, 20, 25, 1) , Batch Y shape :  (2144, 52)\n",
            "Accuracy of this example =  63.059701492537314\n",
            "Total frames in 892: 5405\n",
            "Batch X shape :  (1796, 3, 20, 25, 1) , Batch Y shape :  (1796, 52)\n",
            "Accuracy of this example =  61.97104677060133\n",
            "Total frames in 893: 5738\n",
            "Batch X shape :  (1907, 3, 20, 25, 1) , Batch Y shape :  (1907, 52)\n",
            "Accuracy of this example =  69.84792868379654\n",
            "Total frames in 894: 5777\n",
            "Batch X shape :  (1920, 3, 20, 25, 1) , Batch Y shape :  (1920, 52)\n",
            "Accuracy of this example =  72.5\n",
            "Total frames in 895: 4831\n",
            "Batch X shape :  (1604, 3, 20, 25, 1) , Batch Y shape :  (1604, 52)\n",
            "Accuracy of this example =  70.51122194513717\n",
            "Total frames in 896: 5857\n",
            "Batch X shape :  (1946, 3, 20, 25, 1) , Batch Y shape :  (1946, 52)\n",
            "Accuracy of this example =  54.419321685508734\n",
            "Total frames in 897: 5843\n",
            "Batch X shape :  (1942, 3, 20, 25, 1) , Batch Y shape :  (1942, 52)\n",
            "Accuracy of this example =  56.59114315139032\n",
            "Total frames in 898: 4903\n",
            "Batch X shape :  (1628, 3, 20, 25, 1) , Batch Y shape :  (1628, 52)\n",
            "Accuracy of this example =  61.48648648648649\n",
            "Total frames in 899: 4234\n",
            "Batch X shape :  (1405, 3, 20, 25, 1) , Batch Y shape :  (1405, 52)\n",
            "Accuracy of this example =  57.15302491103203\n",
            "Total frames in 900: 4388\n",
            "Batch X shape :  (1457, 3, 20, 25, 1) , Batch Y shape :  (1457, 52)\n",
            "Accuracy of this example =  57.927247769389155\n",
            "Total frames in 901: 3563\n",
            "Batch X shape :  (1182, 3, 20, 25, 1) , Batch Y shape :  (1182, 52)\n",
            "Accuracy of this example =  56.93739424703892\n",
            "Total frames in 902: 6732\n",
            "Batch X shape :  (2238, 3, 20, 25, 1) , Batch Y shape :  (2238, 52)\n",
            "Accuracy of this example =  64.6112600536193\n",
            "Total frames in 903: 6668\n",
            "Batch X shape :  (2217, 3, 20, 25, 1) , Batch Y shape :  (2217, 52)\n",
            "Accuracy of this example =  59.99097880018043\n",
            "Total frames in 904: 5547\n",
            "Batch X shape :  (1843, 3, 20, 25, 1) , Batch Y shape :  (1843, 52)\n",
            "Accuracy of this example =  59.63103635377103\n",
            "Total frames in 905: 3699\n",
            "Batch X shape :  (1227, 3, 20, 25, 1) , Batch Y shape :  (1227, 52)\n",
            "Accuracy of this example =  60.71719641401793\n",
            "Total frames in 906: 3784\n",
            "Batch X shape :  (1255, 3, 20, 25, 1) , Batch Y shape :  (1255, 52)\n",
            "Accuracy of this example =  57.84860557768924\n",
            "Total frames in 907: 3250\n",
            "Batch X shape :  (1077, 3, 20, 25, 1) , Batch Y shape :  (1077, 52)\n",
            "Accuracy of this example =  53.946146703806875\n",
            "Total frames in 908: 1571\n",
            "Batch X shape :  (518, 3, 20, 25, 1) , Batch Y shape :  (518, 52)\n",
            "Accuracy of this example =  59.07335907335908\n",
            "Total frames in 909: 1580\n",
            "Batch X shape :  (521, 3, 20, 25, 1) , Batch Y shape :  (521, 52)\n",
            "Accuracy of this example =  57.58157389635316\n",
            "Total frames in 910: 1282\n",
            "Batch X shape :  (421, 3, 20, 25, 1) , Batch Y shape :  (421, 52)\n",
            "Accuracy of this example =  45.605700712589076\n",
            "Total frames in 911: 1364\n",
            "Batch X shape :  (449, 3, 20, 25, 1) , Batch Y shape :  (449, 52)\n",
            "Accuracy of this example =  21.603563474387528\n",
            "Total frames in 912: 1388\n",
            "Batch X shape :  (457, 3, 20, 25, 1) , Batch Y shape :  (457, 52)\n",
            "Accuracy of this example =  57.330415754923415\n",
            "Total frames in 913: 1158\n",
            "Batch X shape :  (380, 3, 20, 25, 1) , Batch Y shape :  (380, 52)\n",
            "Accuracy of this example =  47.89473684210526\n",
            "Total frames in 914: 4865\n",
            "Batch X shape :  (1616, 3, 20, 25, 1) , Batch Y shape :  (1616, 52)\n",
            "Accuracy of this example =  60.829207920792086\n",
            "Total frames in 915: 4859\n",
            "Batch X shape :  (1614, 3, 20, 25, 1) , Batch Y shape :  (1614, 52)\n",
            "Accuracy of this example =  57.55885997521685\n",
            "Total frames in 916: 4092\n",
            "Batch X shape :  (1358, 3, 20, 25, 1) , Batch Y shape :  (1358, 52)\n",
            "Accuracy of this example =  55.44918998527246\n",
            "Total frames in 917: 3182\n",
            "Batch X shape :  (1055, 3, 20, 25, 1) , Batch Y shape :  (1055, 52)\n",
            "Accuracy of this example =  59.62085308056872\n",
            "Total frames in 918: 3187\n",
            "Batch X shape :  (1056, 3, 20, 25, 1) , Batch Y shape :  (1056, 52)\n",
            "Accuracy of this example =  58.90151515151515\n",
            "Total frames in 919: 2737\n",
            "Batch X shape :  (906, 3, 20, 25, 1) , Batch Y shape :  (906, 52)\n",
            "Accuracy of this example =  62.69315673289183\n",
            "Total frames in 920: 4754\n",
            "Batch X shape :  (1579, 3, 20, 25, 1) , Batch Y shape :  (1579, 52)\n",
            "Accuracy of this example =  58.328055731475615\n",
            "Total frames in 921: 4760\n",
            "Batch X shape :  (1581, 3, 20, 25, 1) , Batch Y shape :  (1581, 52)\n",
            "Accuracy of this example =  53.510436432637576\n",
            "Total frames in 922: 4068\n",
            "Batch X shape :  (1350, 3, 20, 25, 1) , Batch Y shape :  (1350, 52)\n",
            "Accuracy of this example =  54.888888888888886\n",
            "Total frames in 923: 4201\n",
            "Batch X shape :  (1394, 3, 20, 25, 1) , Batch Y shape :  (1394, 52)\n",
            "Accuracy of this example =  62.48206599713056\n",
            "Total frames in 924: 4227\n",
            "Batch X shape :  (1403, 3, 20, 25, 1) , Batch Y shape :  (1403, 52)\n",
            "Accuracy of this example =  62.295081967213115\n",
            "Total frames in 925: 3752\n",
            "Batch X shape :  (1245, 3, 20, 25, 1) , Batch Y shape :  (1245, 52)\n",
            "Accuracy of this example =  62.00803212851406\n",
            "Total frames in 926: 7002\n",
            "Batch X shape :  (2328, 3, 20, 25, 1) , Batch Y shape :  (2328, 52)\n",
            "Accuracy of this example =  62.24226804123711\n",
            "Total frames in 927: 6987\n",
            "Batch X shape :  (2323, 3, 20, 25, 1) , Batch Y shape :  (2323, 52)\n",
            "Accuracy of this example =  62.37623762376238\n",
            "Total frames in 928: 6180\n",
            "Batch X shape :  (2054, 3, 20, 25, 1) , Batch Y shape :  (2054, 52)\n",
            "Accuracy of this example =  59.834469328140216\n",
            "Total frames in 929: 4417\n",
            "Batch X shape :  (1466, 3, 20, 25, 1) , Batch Y shape :  (1466, 52)\n",
            "Accuracy of this example =  56.070941336971345\n",
            "Total frames in 930: 4448\n",
            "Batch X shape :  (1477, 3, 20, 25, 1) , Batch Y shape :  (1477, 52)\n",
            "Accuracy of this example =  52.538930264048744\n",
            "Total frames in 931: 3907\n",
            "Batch X shape :  (1296, 3, 20, 25, 1) , Batch Y shape :  (1296, 52)\n",
            "Accuracy of this example =  51.85185185185185\n",
            "Total frames in 932: 6340\n",
            "Batch X shape :  (2107, 3, 20, 25, 1) , Batch Y shape :  (2107, 52)\n",
            "Accuracy of this example =  57.949691504508785\n",
            "Total frames in 933: 6397\n",
            "Batch X shape :  (2126, 3, 20, 25, 1) , Batch Y shape :  (2126, 52)\n",
            "Accuracy of this example =  55.59736594543744\n",
            "Total frames in 934: 5633\n",
            "Batch X shape :  (1872, 3, 20, 25, 1) , Batch Y shape :  (1872, 52)\n",
            "Accuracy of this example =  52.56410256410257\n",
            "Total frames in 935: 5914\n",
            "Batch X shape :  (1965, 3, 20, 25, 1) , Batch Y shape :  (1965, 52)\n",
            "Accuracy of this example =  58.42239185750636\n",
            "Total frames in 936: 5911\n",
            "Batch X shape :  (1964, 3, 20, 25, 1) , Batch Y shape :  (1964, 52)\n",
            "Accuracy of this example =  59.72505091649695\n",
            "Total frames in 937: 5222\n",
            "Batch X shape :  (1735, 3, 20, 25, 1) , Batch Y shape :  (1735, 52)\n",
            "Accuracy of this example =  62.47838616714697\n",
            "Total frames in 938: 1766\n",
            "Batch X shape :  (583, 3, 20, 25, 1) , Batch Y shape :  (583, 52)\n",
            "Accuracy of this example =  42.19554030874786\n",
            "Total frames in 939: 1783\n",
            "Batch X shape :  (588, 3, 20, 25, 1) , Batch Y shape :  (588, 52)\n",
            "Accuracy of this example =  39.795918367346935\n",
            "Total frames in 940: 1543\n",
            "Batch X shape :  (508, 3, 20, 25, 1) , Batch Y shape :  (508, 52)\n",
            "Accuracy of this example =  59.055118110236215\n",
            "Total frames in 941: 1726\n",
            "Batch X shape :  (569, 3, 20, 25, 1) , Batch Y shape :  (569, 52)\n",
            "Accuracy of this example =  64.14762741652021\n",
            "Total frames in 942: 1744\n",
            "Batch X shape :  (575, 3, 20, 25, 1) , Batch Y shape :  (575, 52)\n",
            "Accuracy of this example =  61.73913043478261\n",
            "Total frames in 943: 1503\n",
            "Batch X shape :  (495, 3, 20, 25, 1) , Batch Y shape :  (495, 52)\n",
            "Accuracy of this example =  28.484848484848484\n",
            "Total frames in 944: 7456\n",
            "Batch X shape :  (2479, 3, 20, 25, 1) , Batch Y shape :  (2479, 52)\n",
            "Accuracy of this example =  59.620814844695445\n",
            "Total frames in 945: 7417\n",
            "Batch X shape :  (2466, 3, 20, 25, 1) , Batch Y shape :  (2466, 52)\n",
            "Accuracy of this example =  63.21978913219789\n",
            "Total frames in 946: 6674\n",
            "Batch X shape :  (2219, 3, 20, 25, 1) , Batch Y shape :  (2219, 52)\n",
            "Accuracy of this example =  62.415502478593964\n",
            "Total frames in 947: 4718\n",
            "Batch X shape :  (1567, 3, 20, 25, 1) , Batch Y shape :  (1567, 52)\n",
            "Accuracy of this example =  58.13656668793874\n",
            "Total frames in 948: 4625\n",
            "Batch X shape :  (1536, 3, 20, 25, 1) , Batch Y shape :  (1536, 52)\n",
            "Accuracy of this example =  63.4765625\n",
            "Total frames in 949: 4294\n",
            "Batch X shape :  (1425, 3, 20, 25, 1) , Batch Y shape :  (1425, 52)\n",
            "Accuracy of this example =  62.526315789473685\n",
            "Total frames in 950: 5829\n",
            "Batch X shape :  (1937, 3, 20, 25, 1) , Batch Y shape :  (1937, 52)\n",
            "Accuracy of this example =  52.65875064532782\n",
            "Total frames in 951: 5887\n",
            "Batch X shape :  (1956, 3, 20, 25, 1) , Batch Y shape :  (1956, 52)\n",
            "Accuracy of this example =  58.84458077709611\n",
            "Total frames in 952: 5316\n",
            "Batch X shape :  (1766, 3, 20, 25, 1) , Batch Y shape :  (1766, 52)\n",
            "Accuracy of this example =  54.24688561721405\n",
            "Total frames in 953: 4546\n",
            "Batch X shape :  (1509, 3, 20, 25, 1) , Batch Y shape :  (1509, 52)\n",
            "Accuracy of this example =  62.292909211398275\n",
            "Total frames in 954: 4562\n",
            "Batch X shape :  (1515, 3, 20, 25, 1) , Batch Y shape :  (1515, 52)\n",
            "Accuracy of this example =  65.14851485148515\n",
            "Total frames in 955: 4122\n",
            "Batch X shape :  (1368, 3, 20, 25, 1) , Batch Y shape :  (1368, 52)\n",
            "Accuracy of this example =  62.64619883040936\n",
            "Total frames in 956: 6975\n",
            "Batch X shape :  (2319, 3, 20, 25, 1) , Batch Y shape :  (2319, 52)\n",
            "Accuracy of this example =  33.85079775765416\n",
            "Total frames in 957: 7024\n",
            "Batch X shape :  (2335, 3, 20, 25, 1) , Batch Y shape :  (2335, 52)\n",
            "Accuracy of this example =  27.280513918629552\n",
            "Total frames in 958: 6270\n",
            "Batch X shape :  (2084, 3, 20, 25, 1) , Batch Y shape :  (2084, 52)\n",
            "Accuracy of this example =  36.90019193857965\n",
            "Total frames in 959: 5624\n",
            "Batch X shape :  (1869, 3, 20, 25, 1) , Batch Y shape :  (1869, 52)\n",
            "Accuracy of this example =  44.515783841626536\n",
            "Total frames in 960: 5614\n",
            "Batch X shape :  (1865, 3, 20, 25, 1) , Batch Y shape :  (1865, 52)\n",
            "Accuracy of this example =  44.02144772117963\n",
            "Total frames in 961: 4914\n",
            "Batch X shape :  (1632, 3, 20, 25, 1) , Batch Y shape :  (1632, 52)\n",
            "Accuracy of this example =  55.024509803921575\n",
            "Total frames in 962: 6475\n",
            "Batch X shape :  (2152, 3, 20, 25, 1) , Batch Y shape :  (2152, 52)\n",
            "Accuracy of this example =  56.31970260223048\n",
            "Total frames in 963: 6560\n",
            "Batch X shape :  (2181, 3, 20, 25, 1) , Batch Y shape :  (2181, 52)\n",
            "Accuracy of this example =  51.35259055479138\n",
            "Total frames in 964: 5830\n",
            "Batch X shape :  (1937, 3, 20, 25, 1) , Batch Y shape :  (1937, 52)\n",
            "Accuracy of this example =  75.01290655653072\n",
            "Total frames in 965: 5980\n",
            "Batch X shape :  (1987, 3, 20, 25, 1) , Batch Y shape :  (1987, 52)\n",
            "Accuracy of this example =  53.044791142425765\n",
            "Total frames in 966: 6115\n",
            "Batch X shape :  (2032, 3, 20, 25, 1) , Batch Y shape :  (2032, 52)\n",
            "Accuracy of this example =  60.48228346456693\n",
            "Total frames in 967: 5367\n",
            "Batch X shape :  (1783, 3, 20, 25, 1) , Batch Y shape :  (1783, 52)\n",
            "Accuracy of this example =  67.69489624228828\n",
            "Total frames in 968: 1962\n",
            "Batch X shape :  (648, 3, 20, 25, 1) , Batch Y shape :  (648, 52)\n",
            "Accuracy of this example =  15.277777777777779\n",
            "Total frames in 969: 1972\n",
            "Batch X shape :  (651, 3, 20, 25, 1) , Batch Y shape :  (651, 52)\n",
            "Accuracy of this example =  37.94162826420891\n",
            "Total frames in 970: 1682\n",
            "Batch X shape :  (555, 3, 20, 25, 1) , Batch Y shape :  (555, 52)\n",
            "Accuracy of this example =  62.88288288288288\n",
            "Total frames in 971: 1917\n",
            "Batch X shape :  (633, 3, 20, 25, 1) , Batch Y shape :  (633, 52)\n",
            "Accuracy of this example =  15.00789889415482\n",
            "Total frames in 972: 1902\n",
            "Batch X shape :  (628, 3, 20, 25, 1) , Batch Y shape :  (628, 52)\n",
            "Accuracy of this example =  75.63694267515923\n",
            "Total frames in 973: 1658\n",
            "Batch X shape :  (547, 3, 20, 25, 1) , Batch Y shape :  (547, 52)\n",
            "Accuracy of this example =  14.80804387568556\n",
            "Total frames in 974: 6261\n",
            "Batch X shape :  (2081, 3, 20, 25, 1) , Batch Y shape :  (2081, 52)\n",
            "Accuracy of this example =  69.00528592023066\n",
            "Total frames in 975: 6267\n",
            "Batch X shape :  (2083, 3, 20, 25, 1) , Batch Y shape :  (2083, 52)\n",
            "Accuracy of this example =  69.99519923187711\n",
            "Total frames in 976: 5615\n",
            "Batch X shape :  (1866, 3, 20, 25, 1) , Batch Y shape :  (1866, 52)\n",
            "Accuracy of this example =  67.41693461950696\n",
            "Total frames in 977: 5806\n",
            "Batch X shape :  (1929, 3, 20, 25, 1) , Batch Y shape :  (1929, 52)\n",
            "Accuracy of this example =  64.02280974598239\n",
            "Total frames in 978: 5872\n",
            "Batch X shape :  (1951, 3, 20, 25, 1) , Batch Y shape :  (1951, 52)\n",
            "Accuracy of this example =  58.53408508457202\n",
            "Total frames in 979: 5188\n",
            "Batch X shape :  (1723, 3, 20, 25, 1) , Batch Y shape :  (1723, 52)\n",
            "Accuracy of this example =  64.82878699941962\n",
            "Total frames in 980: 6670\n",
            "Batch X shape :  (2217, 3, 20, 25, 1) , Batch Y shape :  (2217, 52)\n",
            "Accuracy of this example =  56.47271087054578\n",
            "Total frames in 981: 6561\n",
            "Batch X shape :  (2181, 3, 20, 25, 1) , Batch Y shape :  (2181, 52)\n",
            "Accuracy of this example =  53.32416322787712\n",
            "Total frames in 982: 5989\n",
            "Batch X shape :  (1990, 3, 20, 25, 1) , Batch Y shape :  (1990, 52)\n",
            "Accuracy of this example =  60.35175879396984\n",
            "Total frames in 983: 6231\n",
            "Batch X shape :  (2071, 3, 20, 25, 1) , Batch Y shape :  (2071, 52)\n",
            "Accuracy of this example =  69.86962819893772\n",
            "Total frames in 984: 6278\n",
            "Batch X shape :  (2087, 3, 20, 25, 1) , Batch Y shape :  (2087, 52)\n",
            "Accuracy of this example =  54.671777671298514\n",
            "Total frames in 985: 5463\n",
            "Batch X shape :  (1815, 3, 20, 25, 1) , Batch Y shape :  (1815, 52)\n",
            "Accuracy of this example =  67.4931129476584\n",
            "Test accuracy :  56.66039876571698\n",
            "F1-score =  0.46625070901871807\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2tMdNCkNf5F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pre_final_out = 256\n",
        "hidden_dim_class = 512\n",
        "n_classes = 52\n",
        "\n",
        "def build_regression_pipe(input_dim, hidden_dim, n_class):\n",
        "  seq_input = tf.keras.layers.Input(shape=(input_dim), name='reg_input')\n",
        "  sm = tf.keras.layers.Input(shape=(n_class), name='soft_selector')\n",
        "\n",
        "  l1 = tf.keras.layers.Dense(n_class*hidden_dim, activation='relu')\n",
        "  l2 = tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "  r1 = tf.keras.layers.Reshape((n_class, hidden_dim))\n",
        "  r2 = tf.keras.layers.Reshape((n_class, 1))\n",
        "  x = r1(l1(seq_input))\n",
        "  x = tf.keras.layers.Multiply()([x, r2(sm)])\n",
        "  x = tf.keras.layers.Flatten()(x)\n",
        "  out = tf.keras.layers.Reshape(())(l2(x))   \n",
        "  return tf.keras.Model(inputs=[seq_input, sm], outputs=[out])\n",
        "\n",
        "start_reg_pipe = build_regression_pipe(pre_final_out, hidden_dim_class, n_classes)\n",
        "end_reg_pipe = build_regression_pipe(pre_final_out, hidden_dim_class, n_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QAyndiVLwhH",
        "colab_type": "code",
        "outputId": "131391f2-d031-4140-dc7f-ce6f0c74689e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "lamd = 0.5\n",
        "\n",
        "def grad(m, m1, m2, inputs, targets, y_st, y_en):\n",
        "  cce = tf.keras.losses.CategoricalCrossentropy()\n",
        "  mse = tf.keras.losses.MSE\n",
        "  \n",
        "  with tf.GradientTape(persistent=True) as tape:\n",
        "    y_, prefinal = m(inputs)\n",
        "    # print(prefinal.shape)\n",
        "    startpreds = m1([prefinal,y_])\n",
        "    endpreds = m2([prefinal,y_])\n",
        "    loss_class = cce(targets,y_)\n",
        "    loss_reg = (mse(y_st,startpreds) + mse(y_en,endpreds))\n",
        "    loss_reg = tf.cast(loss_reg,'float64')\n",
        "    loss_class = tf.cast(loss_class,'float64')\n",
        "    loss = lamd*loss_class + loss_reg\n",
        "    # print(loss,loss_class, loss_reg)\n",
        "  return loss, loss_class, loss_reg, tape.gradient(loss, m.trainable_variables), tape.gradient(loss, m1.trainable_variables), tape.gradient(loss, m2.trainable_variables)\n",
        "\n",
        "model2 = Model(3, 52, {'layout': 'ntu-rgb+d', 'strategy': 'spatial'}, True)\n",
        "num_batches = 5000\n",
        "batch_size = 128\n",
        "num_frames = 20\n",
        "num_step = 1\n",
        "train_ex = [F[i] for i in train_list]\n",
        "test_ex = [F[i] for i in test_list]\n",
        "\n",
        "X1 = np.array(X1)\n",
        "Y = np.array(Y)\n",
        "rawX = X1\n",
        "rawY = Y\n",
        "rawC1 = np.array(C1)\n",
        "rawC2 = np.array(C2)\n",
        "\n",
        "for lr in [5e-5]:\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=lr) \n",
        "  for _ in range(num_step):\n",
        "    for bnum in range(num_batches):\n",
        "      batchX = []\n",
        "      batchY = []\n",
        "      batchC1 = []\n",
        "      batchC2 = []\n",
        "      ct1 = 0\n",
        "      ct = batch_size // 10\n",
        "      while ct1 < batch_size:\n",
        "        t = np.random.choice(train_ex)\n",
        "        n = rawX[t].shape[0]\n",
        "        i = np.random.randint(n-num_frames)\n",
        "        if rawY[t][i+num_frames-1][0] == 1.0:\n",
        "          if ct > 0:\n",
        "            ct -= 1\n",
        "            ct1 += 1\n",
        "          else: continue\n",
        "        else:\n",
        "          ct1 += 1\n",
        "        batchX.append(rawX[t][i:i+num_frames])\n",
        "        batchY.append(rawY[t][i:i+num_frames])\n",
        "        batchC1.append(rawC1[t][i:i+num_frames])\n",
        "        batchC2.append(rawC2[t][i:i+num_frames])\n",
        "      batchX = np.array(batchX)\n",
        "      batchY = np.array(batchY)\n",
        "      batchC1 = np.array(batchC1)\n",
        "      batchC2 = np.array(batchC2)\n",
        "      batchY = batchY[:, -1, :]\n",
        "      batchC1 = batchC1[:,-1]\n",
        "      batchC2 = batchC2[:,-1]\n",
        "      batchX = np.transpose(batchX, (0, 3, 1, 2))\n",
        "      batchX = np.expand_dims(batchX, axis=4)\n",
        "\n",
        "      loss_value, loss_cls, loss_reg, grad_cl, grad_st, grad_en = grad(model2, start_reg_pipe, end_reg_pipe, batchX, batchY, batchC1, batchC2)\n",
        "      optimizer.apply_gradients(zip(grad_cl, model2.trainable_variables))\n",
        "      optimizer.apply_gradients(zip(grad_st, start_reg_pipe.trainable_variables))\n",
        "      optimizer.apply_gradients(zip(grad_en, end_reg_pipe.trainable_variables))\n",
        "      print(\"Loss: \", loss_value.numpy(), \"\\nClassification loss:\", loss_cls.numpy(),\"\\nRegression loss:\", loss_reg.numpy())\n",
        "      print('----------------') "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss:  2.6963998079299927 \n",
            "Classification loss: 4.593501091003418 \n",
            "Regression loss: 0.3996492624282837\n",
            "----------------\n",
            "Loss:  2.6324686408042908 \n",
            "Classification loss: 4.469198226928711 \n",
            "Regression loss: 0.3978695273399353\n",
            "----------------\n",
            "Loss:  2.589692324399948 \n",
            "Classification loss: 4.4140424728393555 \n",
            "Regression loss: 0.3826710879802704\n",
            "----------------\n",
            "Loss:  2.5002295672893524 \n",
            "Classification loss: 4.242809295654297 \n",
            "Regression loss: 0.378824919462204\n",
            "----------------\n",
            "Loss:  2.4810992181301117 \n",
            "Classification loss: 4.254096031188965 \n",
            "Regression loss: 0.3540512025356293\n",
            "----------------\n",
            "Loss:  2.521955579519272 \n",
            "Classification loss: 4.351070404052734 \n",
            "Regression loss: 0.34642037749290466\n",
            "----------------\n",
            "Loss:  2.440389037132263 \n",
            "Classification loss: 4.223236083984375 \n",
            "Regression loss: 0.3287709951400757\n",
            "----------------\n",
            "Loss:  2.385732591152191 \n",
            "Classification loss: 4.135295391082764 \n",
            "Regression loss: 0.3180848956108093\n",
            "----------------\n",
            "Loss:  2.365443766117096 \n",
            "Classification loss: 4.100681304931641 \n",
            "Regression loss: 0.31510311365127563\n",
            "----------------\n",
            "Loss:  2.382183939218521 \n",
            "Classification loss: 4.186259746551514 \n",
            "Regression loss: 0.2890540659427643\n",
            "----------------\n",
            "Loss:  2.2563588619232178 \n",
            "Classification loss: 3.958272933959961 \n",
            "Regression loss: 0.2772223949432373\n",
            "----------------\n",
            "Loss:  2.2335336804389954 \n",
            "Classification loss: 3.930063247680664 \n",
            "Regression loss: 0.26850205659866333\n",
            "----------------\n",
            "Loss:  2.2890490293502808 \n",
            "Classification loss: 4.07907247543335 \n",
            "Regression loss: 0.24951279163360596\n",
            "----------------\n",
            "Loss:  2.3667275309562683 \n",
            "Classification loss: 4.224123001098633 \n",
            "Regression loss: 0.2546660304069519\n",
            "----------------\n",
            "Loss:  2.2838049679994583 \n",
            "Classification loss: 4.1060709953308105 \n",
            "Regression loss: 0.23076947033405304\n",
            "----------------\n",
            "Loss:  2.1907305121421814 \n",
            "Classification loss: 3.9362645149230957 \n",
            "Regression loss: 0.22259825468063354\n",
            "----------------\n",
            "Loss:  2.1262611150741577 \n",
            "Classification loss: 3.8442094326019287 \n",
            "Regression loss: 0.20415639877319336\n",
            "----------------\n",
            "Loss:  2.0652374774217606 \n",
            "Classification loss: 3.755340576171875 \n",
            "Regression loss: 0.18756718933582306\n",
            "----------------\n",
            "Loss:  2.090389594435692 \n",
            "Classification loss: 3.8455779552459717 \n",
            "Regression loss: 0.167600616812706\n",
            "----------------\n",
            "Loss:  2.079127699136734 \n",
            "Classification loss: 3.8433384895324707 \n",
            "Regression loss: 0.15745845437049866\n",
            "----------------\n",
            "Loss:  1.992609664797783 \n",
            "Classification loss: 3.6849193572998047 \n",
            "Regression loss: 0.15014998614788055\n",
            "----------------\n",
            "Loss:  1.993492990732193 \n",
            "Classification loss: 3.7125473022460938 \n",
            "Regression loss: 0.13721933960914612\n",
            "----------------\n",
            "Loss:  1.9457705616950989 \n",
            "Classification loss: 3.640784978866577 \n",
            "Regression loss: 0.1253780722618103\n",
            "----------------\n",
            "Loss:  2.0316230058670044 \n",
            "Classification loss: 3.8517348766326904 \n",
            "Regression loss: 0.10575556755065918\n",
            "----------------\n",
            "Loss:  1.8563563227653503 \n",
            "Classification loss: 3.5149893760681152 \n",
            "Regression loss: 0.09886163473129272\n",
            "----------------\n",
            "Loss:  1.9590764939785004 \n",
            "Classification loss: 3.714545726776123 \n",
            "Regression loss: 0.10180363059043884\n",
            "----------------\n",
            "Loss:  2.037354201078415 \n",
            "Classification loss: 3.8220648765563965 \n",
            "Regression loss: 0.12632176280021667\n",
            "----------------\n",
            "Loss:  2.037302501499653 \n",
            "Classification loss: 3.870683193206787 \n",
            "Regression loss: 0.10196090489625931\n",
            "----------------\n",
            "Loss:  1.858665868639946 \n",
            "Classification loss: 3.5663983821868896 \n",
            "Regression loss: 0.07546667754650116\n",
            "----------------\n",
            "Loss:  1.9162581488490105 \n",
            "Classification loss: 3.652811050415039 \n",
            "Regression loss: 0.08985262364149094\n",
            "----------------\n",
            "Loss:  1.8537010252475739 \n",
            "Classification loss: 3.5467824935913086 \n",
            "Regression loss: 0.08030977845191956\n",
            "----------------\n",
            "Loss:  1.8983496725559235 \n",
            "Classification loss: 3.641742706298828 \n",
            "Regression loss: 0.0774783194065094\n",
            "----------------\n",
            "Loss:  1.8720225915312767 \n",
            "Classification loss: 3.624101400375366 \n",
            "Regression loss: 0.0599718913435936\n",
            "----------------\n",
            "Loss:  1.8853861764073372 \n",
            "Classification loss: 3.5967729091644287 \n",
            "Regression loss: 0.08699972182512283\n",
            "----------------\n",
            "Loss:  1.7633922845125198 \n",
            "Classification loss: 3.4132680892944336 \n",
            "Regression loss: 0.05675823986530304\n",
            "----------------\n",
            "Loss:  1.809127300977707 \n",
            "Classification loss: 3.4918673038482666 \n",
            "Regression loss: 0.06319364905357361\n",
            "----------------\n",
            "Loss:  1.8273492828011513 \n",
            "Classification loss: 3.5036416053771973 \n",
            "Regression loss: 0.07552848011255264\n",
            "----------------\n",
            "Loss:  1.83245300501585 \n",
            "Classification loss: 3.520102024078369 \n",
            "Regression loss: 0.0724019929766655\n",
            "----------------\n",
            "Loss:  1.8311418071389198 \n",
            "Classification loss: 3.452991247177124 \n",
            "Regression loss: 0.10464618355035782\n",
            "----------------\n",
            "Loss:  1.7366361245512962 \n",
            "Classification loss: 3.3328871726989746 \n",
            "Regression loss: 0.07019253820180893\n",
            "----------------\n",
            "Loss:  1.7652226760983467 \n",
            "Classification loss: 3.435086250305176 \n",
            "Regression loss: 0.04767955094575882\n",
            "----------------\n",
            "Loss:  1.716932587325573 \n",
            "Classification loss: 3.3510332107543945 \n",
            "Regression loss: 0.0414159819483757\n",
            "----------------\n",
            "Loss:  1.8071222603321075 \n",
            "Classification loss: 3.4307820796966553 \n",
            "Regression loss: 0.09173122048377991\n",
            "----------------\n",
            "Loss:  1.7649147249758244 \n",
            "Classification loss: 3.461719512939453 \n",
            "Regression loss: 0.034054968506097794\n",
            "----------------\n",
            "Loss:  1.7589882612228394 \n",
            "Classification loss: 3.364363431930542 \n",
            "Regression loss: 0.07680654525756836\n",
            "----------------\n",
            "Loss:  1.7047318555414677 \n",
            "Classification loss: 3.3169937133789062 \n",
            "Regression loss: 0.04623499885201454\n",
            "----------------\n",
            "Loss:  1.7492275536060333 \n",
            "Classification loss: 3.334583282470703 \n",
            "Regression loss: 0.08193591237068176\n",
            "----------------\n",
            "Loss:  1.758612684905529 \n",
            "Classification loss: 3.415318250656128 \n",
            "Regression loss: 0.05095355957746506\n",
            "----------------\n",
            "Loss:  1.7178056985139847 \n",
            "Classification loss: 3.3194150924682617 \n",
            "Regression loss: 0.05809815227985382\n",
            "----------------\n",
            "Loss:  1.6248978432267904 \n",
            "Classification loss: 3.1930975914001465 \n",
            "Regression loss: 0.028349047526717186\n",
            "----------------\n",
            "Loss:  1.7053327858448029 \n",
            "Classification loss: 3.279252052307129 \n",
            "Regression loss: 0.0657067596912384\n",
            "----------------\n",
            "Loss:  1.6863418594002724 \n",
            "Classification loss: 3.251274824142456 \n",
            "Regression loss: 0.06070444732904434\n",
            "----------------\n",
            "Loss:  1.7557939440011978 \n",
            "Classification loss: 3.3399910926818848 \n",
            "Regression loss: 0.08579839766025543\n",
            "----------------\n",
            "Loss:  1.7134518772363663 \n",
            "Classification loss: 3.3165791034698486 \n",
            "Regression loss: 0.055162325501441956\n",
            "----------------\n",
            "Loss:  1.6729213148355484 \n",
            "Classification loss: 3.2427825927734375 \n",
            "Regression loss: 0.05153001844882965\n",
            "----------------\n",
            "Loss:  1.6753980703651905 \n",
            "Classification loss: 3.2610111236572266 \n",
            "Regression loss: 0.044892508536577225\n",
            "----------------\n",
            "Loss:  1.7257103472948074 \n",
            "Classification loss: 3.2642741203308105 \n",
            "Regression loss: 0.09357328712940216\n",
            "----------------\n",
            "Loss:  1.6449878886342049 \n",
            "Classification loss: 3.175936460494995 \n",
            "Regression loss: 0.057019658386707306\n",
            "----------------\n",
            "Loss:  1.644843652844429 \n",
            "Classification loss: 3.2297394275665283 \n",
            "Regression loss: 0.029973939061164856\n",
            "----------------\n",
            "Loss:  1.7006628476083279 \n",
            "Classification loss: 3.2947511672973633 \n",
            "Regression loss: 0.053287263959646225\n",
            "----------------\n",
            "Loss:  1.7393639013171196 \n",
            "Classification loss: 3.304661273956299 \n",
            "Regression loss: 0.08703326433897018\n",
            "----------------\n",
            "Loss:  1.7161476463079453 \n",
            "Classification loss: 3.308316230773926 \n",
            "Regression loss: 0.06198953092098236\n",
            "----------------\n",
            "Loss:  1.7330703437328339 \n",
            "Classification loss: 3.2870676517486572 \n",
            "Regression loss: 0.08953651785850525\n",
            "----------------\n",
            "Loss:  1.639375165104866 \n",
            "Classification loss: 3.1784088611602783 \n",
            "Regression loss: 0.05017073452472687\n",
            "----------------\n",
            "Loss:  1.6964291408658028 \n",
            "Classification loss: 3.263195514678955 \n",
            "Regression loss: 0.06483138352632523\n",
            "----------------\n",
            "Loss:  1.7289090566337109 \n",
            "Classification loss: 3.3812084197998047 \n",
            "Regression loss: 0.03830484673380852\n",
            "----------------\n",
            "Loss:  1.6407385766506195 \n",
            "Classification loss: 3.133110761642456 \n",
            "Regression loss: 0.07418319582939148\n",
            "----------------\n",
            "Loss:  1.720773871988058 \n",
            "Classification loss: 3.345928430557251 \n",
            "Regression loss: 0.0478096567094326\n",
            "----------------\n",
            "Loss:  1.6762874126434326 \n",
            "Classification loss: 3.166388750076294 \n",
            "Regression loss: 0.09309303760528564\n",
            "----------------\n",
            "Loss:  1.6316463276743889 \n",
            "Classification loss: 3.125469207763672 \n",
            "Regression loss: 0.06891172379255295\n",
            "----------------\n",
            "Loss:  1.6705130189657211 \n",
            "Classification loss: 3.2318027019500732 \n",
            "Regression loss: 0.05461166799068451\n",
            "----------------\n",
            "Loss:  1.6824906691908836 \n",
            "Classification loss: 3.234683036804199 \n",
            "Regression loss: 0.06514915078878403\n",
            "----------------\n",
            "Loss:  1.6035981997847557 \n",
            "Classification loss: 3.1081645488739014 \n",
            "Regression loss: 0.04951592534780502\n",
            "----------------\n",
            "Loss:  1.6797619499266148 \n",
            "Classification loss: 3.2366864681243896 \n",
            "Regression loss: 0.06141871586441994\n",
            "----------------\n",
            "Loss:  1.5668637454509735 \n",
            "Classification loss: 3.0512027740478516 \n",
            "Regression loss: 0.04126235842704773\n",
            "----------------\n",
            "Loss:  1.6665477901697159 \n",
            "Classification loss: 3.194638729095459 \n",
            "Regression loss: 0.06922842562198639\n",
            "----------------\n",
            "Loss:  1.6282504498958588 \n",
            "Classification loss: 3.1283488273620605 \n",
            "Regression loss: 0.06407603621482849\n",
            "----------------\n",
            "Loss:  1.5980765670537949 \n",
            "Classification loss: 3.0589566230773926 \n",
            "Regression loss: 0.06859825551509857\n",
            "----------------\n",
            "Loss:  1.7379491329193115 \n",
            "Classification loss: 3.371461868286133 \n",
            "Regression loss: 0.05221819877624512\n",
            "----------------\n",
            "Loss:  1.6413403898477554 \n",
            "Classification loss: 3.1340982913970947 \n",
            "Regression loss: 0.07429124414920807\n",
            "----------------\n",
            "Loss:  1.7162276208400726 \n",
            "Classification loss: 3.3205885887145996 \n",
            "Regression loss: 0.05593332648277283\n",
            "----------------\n",
            "Loss:  1.7075047120451927 \n",
            "Classification loss: 3.2522120475769043 \n",
            "Regression loss: 0.08139868825674057\n",
            "----------------\n",
            "Loss:  1.5854826867580414 \n",
            "Classification loss: 3.0467066764831543 \n",
            "Regression loss: 0.06212934851646423\n",
            "----------------\n",
            "Loss:  1.6665797643363476 \n",
            "Classification loss: 3.229768753051758 \n",
            "Regression loss: 0.051695387810468674\n",
            "----------------\n",
            "Loss:  1.6211004555225372 \n",
            "Classification loss: 3.095649480819702 \n",
            "Regression loss: 0.07327571511268616\n",
            "----------------\n",
            "Loss:  1.620718028396368 \n",
            "Classification loss: 3.1327993869781494 \n",
            "Regression loss: 0.05431833490729332\n",
            "----------------\n",
            "Loss:  1.6188341081142426 \n",
            "Classification loss: 3.0695433616638184 \n",
            "Regression loss: 0.08406242728233337\n",
            "----------------\n",
            "Loss:  1.5526321306824684 \n",
            "Classification loss: 3.016016960144043 \n",
            "Regression loss: 0.04462365061044693\n",
            "----------------\n",
            "Loss:  1.68362557888031 \n",
            "Classification loss: 3.2660439014434814 \n",
            "Regression loss: 0.050603628158569336\n",
            "----------------\n",
            "Loss:  1.5683184303343296 \n",
            "Classification loss: 3.0630507469177246 \n",
            "Regression loss: 0.0367930568754673\n",
            "----------------\n",
            "Loss:  1.6456361375749111 \n",
            "Classification loss: 3.1893234252929688 \n",
            "Regression loss: 0.05097442492842674\n",
            "----------------\n",
            "Loss:  1.6102940775454044 \n",
            "Classification loss: 3.1110966205596924 \n",
            "Regression loss: 0.05474576726555824\n",
            "----------------\n",
            "Loss:  1.6517249681055546 \n",
            "Classification loss: 3.1835055351257324 \n",
            "Regression loss: 0.05997220054268837\n",
            "----------------\n",
            "Loss:  1.6793336272239685 \n",
            "Classification loss: 3.1960909366607666 \n",
            "Regression loss: 0.0812881588935852\n",
            "----------------\n",
            "Loss:  1.4866617657244205 \n",
            "Classification loss: 2.8952322006225586 \n",
            "Regression loss: 0.03904566541314125\n",
            "----------------\n",
            "Loss:  1.6049148216843605 \n",
            "Classification loss: 3.1047704219818115 \n",
            "Regression loss: 0.05252961069345474\n",
            "----------------\n",
            "Loss:  1.6345263309776783 \n",
            "Classification loss: 3.1616532802581787 \n",
            "Regression loss: 0.053699690848588943\n",
            "----------------\n",
            "Loss:  1.6289594769477844 \n",
            "Classification loss: 3.157752513885498 \n",
            "Regression loss: 0.0500832200050354\n",
            "----------------\n",
            "Loss:  1.5959618613123894 \n",
            "Classification loss: 3.0271687507629395 \n",
            "Regression loss: 0.08237748593091965\n",
            "----------------\n",
            "Loss:  1.5786423236131668 \n",
            "Classification loss: 3.0824546813964844 \n",
            "Regression loss: 0.03741498291492462\n",
            "----------------\n",
            "Loss:  1.6014658100903034 \n",
            "Classification loss: 3.1189799308776855 \n",
            "Regression loss: 0.04197584465146065\n",
            "----------------\n",
            "Loss:  1.6847395449876785 \n",
            "Classification loss: 3.2107741832733154 \n",
            "Regression loss: 0.07935245335102081\n",
            "----------------\n",
            "Loss:  1.5795511305332184 \n",
            "Classification loss: 3.0134973526000977 \n",
            "Regression loss: 0.07280245423316956\n",
            "----------------\n",
            "Loss:  1.6487791985273361 \n",
            "Classification loss: 3.1715316772460938 \n",
            "Regression loss: 0.06301335990428925\n",
            "----------------\n",
            "Loss:  1.585921484977007 \n",
            "Classification loss: 3.0586016178131104 \n",
            "Regression loss: 0.056620676070451736\n",
            "----------------\n",
            "Loss:  1.6196549087762833 \n",
            "Classification loss: 3.11122465133667 \n",
            "Regression loss: 0.0640425831079483\n",
            "----------------\n",
            "Loss:  1.6382492408156395 \n",
            "Classification loss: 3.117380142211914 \n",
            "Regression loss: 0.07955916970968246\n",
            "----------------\n",
            "Loss:  1.5772258788347244 \n",
            "Classification loss: 3.061122417449951 \n",
            "Regression loss: 0.04666467010974884\n",
            "----------------\n",
            "Loss:  1.5241668485105038 \n",
            "Classification loss: 2.9661006927490234 \n",
            "Regression loss: 0.04111650213599205\n",
            "----------------\n",
            "Loss:  1.5904086269438267 \n",
            "Classification loss: 3.0904436111450195 \n",
            "Regression loss: 0.04518682137131691\n",
            "----------------\n",
            "Loss:  1.5432902425527573 \n",
            "Classification loss: 2.9739553928375244 \n",
            "Regression loss: 0.056312546133995056\n",
            "----------------\n",
            "Loss:  1.5974221155047417 \n",
            "Classification loss: 3.0403666496276855 \n",
            "Regression loss: 0.0772387906908989\n",
            "----------------\n",
            "Loss:  1.6518989279866219 \n",
            "Classification loss: 3.1463656425476074 \n",
            "Regression loss: 0.07871610671281815\n",
            "----------------\n",
            "Loss:  1.5899693816900253 \n",
            "Classification loss: 3.0503625869750977 \n",
            "Regression loss: 0.0647880882024765\n",
            "----------------\n",
            "Loss:  1.4876798056066036 \n",
            "Classification loss: 2.896359443664551 \n",
            "Regression loss: 0.03950008377432823\n",
            "----------------\n",
            "Loss:  1.5684679448604584 \n",
            "Classification loss: 3.0560240745544434 \n",
            "Regression loss: 0.040455907583236694\n",
            "----------------\n",
            "Loss:  1.5233750455081463 \n",
            "Classification loss: 2.9457247257232666 \n",
            "Regression loss: 0.050512682646512985\n",
            "----------------\n",
            "Loss:  1.4963580146431923 \n",
            "Classification loss: 2.857626438140869 \n",
            "Regression loss: 0.06754479557275772\n",
            "----------------\n",
            "Loss:  1.5720326900482178 \n",
            "Classification loss: 3.0349512100219727 \n",
            "Regression loss: 0.054557085037231445\n",
            "----------------\n",
            "Loss:  1.5001151338219643 \n",
            "Classification loss: 2.874127149581909 \n",
            "Regression loss: 0.06305155903100967\n",
            "----------------\n",
            "Loss:  1.6247903779149055 \n",
            "Classification loss: 3.1189699172973633 \n",
            "Regression loss: 0.06530541926622391\n",
            "----------------\n",
            "Loss:  1.5662215948104858 \n",
            "Classification loss: 3.0081751346588135 \n",
            "Regression loss: 0.0621340274810791\n",
            "----------------\n",
            "Loss:  1.6528040319681168 \n",
            "Classification loss: 3.126274585723877 \n",
            "Regression loss: 0.08966673910617828\n",
            "----------------\n",
            "Loss:  1.5961463302373886 \n",
            "Classification loss: 3.084758758544922 \n",
            "Regression loss: 0.05376695096492767\n",
            "----------------\n",
            "Loss:  1.5714924074709415 \n",
            "Classification loss: 3.0198729038238525 \n",
            "Regression loss: 0.061555955559015274\n",
            "----------------\n",
            "Loss:  1.5417756363749504 \n",
            "Classification loss: 2.957279682159424 \n",
            "Regression loss: 0.0631357952952385\n",
            "----------------\n",
            "Loss:  1.5826089531183243 \n",
            "Classification loss: 3.043513059616089 \n",
            "Regression loss: 0.060852423310279846\n",
            "----------------\n",
            "Loss:  1.585295781493187 \n",
            "Classification loss: 3.0557188987731934 \n",
            "Regression loss: 0.05743633210659027\n",
            "----------------\n",
            "Loss:  1.6144735924899578 \n",
            "Classification loss: 3.1315083503723145 \n",
            "Regression loss: 0.04871941730380058\n",
            "----------------\n",
            "Loss:  1.467227902263403 \n",
            "Classification loss: 2.8475053310394287 \n",
            "Regression loss: 0.04347523674368858\n",
            "----------------\n",
            "Loss:  1.5624564625322819 \n",
            "Classification loss: 3.0234148502349854 \n",
            "Regression loss: 0.0507490374147892\n",
            "----------------\n",
            "Loss:  1.5853774696588516 \n",
            "Classification loss: 3.0064544677734375 \n",
            "Regression loss: 0.08215023577213287\n",
            "----------------\n",
            "Loss:  1.5913659185171127 \n",
            "Classification loss: 3.095825433731079 \n",
            "Regression loss: 0.04345320165157318\n",
            "----------------\n",
            "Loss:  1.4989399649202824 \n",
            "Classification loss: 2.910553216934204 \n",
            "Regression loss: 0.04366335645318031\n",
            "----------------\n",
            "Loss:  1.589557208120823 \n",
            "Classification loss: 3.0714967250823975 \n",
            "Regression loss: 0.053808845579624176\n",
            "----------------\n",
            "Loss:  1.5475367605686188 \n",
            "Classification loss: 3.0127780437469482 \n",
            "Regression loss: 0.04114773869514465\n",
            "----------------\n",
            "Loss:  1.452447721734643 \n",
            "Classification loss: 2.8733763694763184 \n",
            "Regression loss: 0.015759536996483803\n",
            "----------------\n",
            "Loss:  1.5577340126037598 \n",
            "Classification loss: 2.942922592163086 \n",
            "Regression loss: 0.0862727165222168\n",
            "----------------\n",
            "Loss:  1.528498813509941 \n",
            "Classification loss: 2.9432592391967773 \n",
            "Regression loss: 0.05686919391155243\n",
            "----------------\n",
            "Loss:  1.4863884076476097 \n",
            "Classification loss: 2.865091323852539 \n",
            "Regression loss: 0.05384274572134018\n",
            "----------------\n",
            "Loss:  1.547667384147644 \n",
            "Classification loss: 2.9739887714385986 \n",
            "Regression loss: 0.06067299842834473\n",
            "----------------\n",
            "Loss:  1.6305921599268913 \n",
            "Classification loss: 3.0912587642669678 \n",
            "Regression loss: 0.08496277779340744\n",
            "----------------\n",
            "Loss:  1.4900200366973877 \n",
            "Classification loss: 2.885511636734009 \n",
            "Regression loss: 0.0472642183303833\n",
            "----------------\n",
            "Loss:  1.6332873404026031 \n",
            "Classification loss: 3.1104795932769775 \n",
            "Regression loss: 0.07804754376411438\n",
            "----------------\n",
            "Loss:  1.5408323407173157 \n",
            "Classification loss: 2.9683690071105957 \n",
            "Regression loss: 0.05664783716201782\n",
            "----------------\n",
            "Loss:  1.5156504586338997 \n",
            "Classification loss: 2.949398994445801 \n",
            "Regression loss: 0.0409509614109993\n",
            "----------------\n",
            "Loss:  1.6168036460876465 \n",
            "Classification loss: 3.043572425842285 \n",
            "Regression loss: 0.0950174331665039\n",
            "----------------\n",
            "Loss:  1.4178993627429008 \n",
            "Classification loss: 2.7400636672973633 \n",
            "Regression loss: 0.04786752909421921\n",
            "----------------\n",
            "Loss:  1.5862310081720352 \n",
            "Classification loss: 2.9985222816467285 \n",
            "Regression loss: 0.08696986734867096\n",
            "----------------\n",
            "Loss:  1.4790438823401928 \n",
            "Classification loss: 2.854243516921997 \n",
            "Regression loss: 0.05192212387919426\n",
            "----------------\n",
            "Loss:  1.447518266737461 \n",
            "Classification loss: 2.798694133758545 \n",
            "Regression loss: 0.04817119985818863\n",
            "----------------\n",
            "Loss:  1.5042149722576141 \n",
            "Classification loss: 2.877264976501465 \n",
            "Regression loss: 0.06558248400688171\n",
            "----------------\n",
            "Loss:  1.5038999393582344 \n",
            "Classification loss: 2.856369733810425 \n",
            "Regression loss: 0.075715072453022\n",
            "----------------\n",
            "Loss:  1.5810188576579094 \n",
            "Classification loss: 3.047642469406128 \n",
            "Regression loss: 0.05719762295484543\n",
            "----------------\n",
            "Loss:  1.4724827595055103 \n",
            "Classification loss: 2.8654940128326416 \n",
            "Regression loss: 0.03973575308918953\n",
            "----------------\n",
            "Loss:  1.463291049003601 \n",
            "Classification loss: 2.8019003868103027 \n",
            "Regression loss: 0.06234085559844971\n",
            "----------------\n",
            "Loss:  1.5082864165306091 \n",
            "Classification loss: 2.889533042907715 \n",
            "Regression loss: 0.06351989507675171\n",
            "----------------\n",
            "Loss:  1.5155751332640648 \n",
            "Classification loss: 2.8791868686676025 \n",
            "Regression loss: 0.07598169893026352\n",
            "----------------\n",
            "Loss:  1.4419662617146969 \n",
            "Classification loss: 2.8151800632476807 \n",
            "Regression loss: 0.03437623009085655\n",
            "----------------\n",
            "Loss:  1.517933402210474 \n",
            "Classification loss: 2.97748064994812 \n",
            "Regression loss: 0.029193077236413956\n",
            "----------------\n",
            "Loss:  1.5737353190779686 \n",
            "Classification loss: 2.9818949699401855 \n",
            "Regression loss: 0.08278783410787582\n",
            "----------------\n",
            "Loss:  1.5338637977838516 \n",
            "Classification loss: 2.9640445709228516 \n",
            "Regression loss: 0.05184151232242584\n",
            "----------------\n",
            "Loss:  1.4127086959779263 \n",
            "Classification loss: 2.7459676265716553 \n",
            "Regression loss: 0.03972488269209862\n",
            "----------------\n",
            "Loss:  1.378996379673481 \n",
            "Classification loss: 2.657050609588623 \n",
            "Regression loss: 0.050471074879169464\n",
            "----------------\n",
            "Loss:  1.5009953454136848 \n",
            "Classification loss: 2.8578596115112305 \n",
            "Regression loss: 0.07206553965806961\n",
            "----------------\n",
            "Loss:  1.6107394993305206 \n",
            "Classification loss: 3.1163694858551025 \n",
            "Regression loss: 0.05255475640296936\n",
            "----------------\n",
            "Loss:  1.4709447175264359 \n",
            "Classification loss: 2.8142409324645996 \n",
            "Regression loss: 0.06382425129413605\n",
            "----------------\n",
            "Loss:  1.5942048840224743 \n",
            "Classification loss: 3.0680465698242188 \n",
            "Regression loss: 0.060181599110364914\n",
            "----------------\n",
            "Loss:  1.5843887999653816 \n",
            "Classification loss: 3.0312271118164062 \n",
            "Regression loss: 0.0687752440571785\n",
            "----------------\n",
            "Loss:  1.533602811396122 \n",
            "Classification loss: 2.9530539512634277 \n",
            "Regression loss: 0.05707583576440811\n",
            "----------------\n",
            "Loss:  1.4515861868858337 \n",
            "Classification loss: 2.7780630588531494 \n",
            "Regression loss: 0.06255465745925903\n",
            "----------------\n",
            "Loss:  1.5575694181025028 \n",
            "Classification loss: 3.0082647800445557 \n",
            "Regression loss: 0.05343702808022499\n",
            "----------------\n",
            "Loss:  1.5631851702928543 \n",
            "Classification loss: 2.995669364929199 \n",
            "Regression loss: 0.0653504878282547\n",
            "----------------\n",
            "Loss:  1.5223390683531761 \n",
            "Classification loss: 2.9614622592926025 \n",
            "Regression loss: 0.04160793870687485\n",
            "----------------\n",
            "Loss:  1.6316248029470444 \n",
            "Classification loss: 3.1275196075439453 \n",
            "Regression loss: 0.06786499917507172\n",
            "----------------\n",
            "Loss:  1.477653294801712 \n",
            "Classification loss: 2.86498761177063 \n",
            "Regression loss: 0.045159488916397095\n",
            "----------------\n",
            "Loss:  1.5352996662259102 \n",
            "Classification loss: 2.9174442291259766 \n",
            "Regression loss: 0.0765775516629219\n",
            "----------------\n",
            "Loss:  1.5151362121105194 \n",
            "Classification loss: 2.8601245880126953 \n",
            "Regression loss: 0.08507391810417175\n",
            "----------------\n",
            "Loss:  1.6454281210899353 \n",
            "Classification loss: 3.158353328704834 \n",
            "Regression loss: 0.06625145673751831\n",
            "----------------\n",
            "Loss:  1.4315821193158627 \n",
            "Classification loss: 2.7745532989501953 \n",
            "Regression loss: 0.044305469840765\n",
            "----------------\n",
            "Loss:  1.658547431230545 \n",
            "Classification loss: 3.1564226150512695 \n",
            "Regression loss: 0.08033612370491028\n",
            "----------------\n",
            "Loss:  1.5206020176410675 \n",
            "Classification loss: 2.9010143280029297 \n",
            "Regression loss: 0.07009485363960266\n",
            "----------------\n",
            "Loss:  1.5049564205110073 \n",
            "Classification loss: 2.908046007156372 \n",
            "Regression loss: 0.050933416932821274\n",
            "----------------\n",
            "Loss:  1.4874071925878525 \n",
            "Classification loss: 2.820812463760376 \n",
            "Regression loss: 0.07700096070766449\n",
            "----------------\n",
            "Loss:  1.5051366873085499 \n",
            "Classification loss: 2.910918712615967 \n",
            "Regression loss: 0.04967733100056648\n",
            "----------------\n",
            "Loss:  1.522352121770382 \n",
            "Classification loss: 2.986288070678711 \n",
            "Regression loss: 0.02920808643102646\n",
            "----------------\n",
            "Loss:  1.473877664655447 \n",
            "Classification loss: 2.8240387439727783 \n",
            "Regression loss: 0.061858292669057846\n",
            "----------------\n",
            "Loss:  1.4908517152071 \n",
            "Classification loss: 2.8296008110046387 \n",
            "Regression loss: 0.07605130970478058\n",
            "----------------\n",
            "Loss:  1.3933884911239147 \n",
            "Classification loss: 2.710111618041992 \n",
            "Regression loss: 0.038332682102918625\n",
            "----------------\n",
            "Loss:  1.5355824306607246 \n",
            "Classification loss: 2.977860450744629 \n",
            "Regression loss: 0.04665220528841019\n",
            "----------------\n",
            "Loss:  1.459479220211506 \n",
            "Classification loss: 2.8265857696533203 \n",
            "Regression loss: 0.046186335384845734\n",
            "----------------\n",
            "Loss:  1.5430556535720825 \n",
            "Classification loss: 2.947852611541748 \n",
            "Regression loss: 0.0691293478012085\n",
            "----------------\n",
            "Loss:  1.4842118956148624 \n",
            "Classification loss: 2.888615608215332 \n",
            "Regression loss: 0.039904091507196426\n",
            "----------------\n",
            "Loss:  1.4967932254076004 \n",
            "Classification loss: 2.9251885414123535 \n",
            "Regression loss: 0.034198954701423645\n",
            "----------------\n",
            "Loss:  1.4215204305946827 \n",
            "Classification loss: 2.7353286743164062 \n",
            "Regression loss: 0.05385609343647957\n",
            "----------------\n",
            "Loss:  1.4841529205441475 \n",
            "Classification loss: 2.8851191997528076 \n",
            "Regression loss: 0.04159332066774368\n",
            "----------------\n",
            "Loss:  1.3942029401659966 \n",
            "Classification loss: 2.648895025253296 \n",
            "Regression loss: 0.0697554275393486\n",
            "----------------\n",
            "Loss:  1.4756005443632603 \n",
            "Classification loss: 2.843323230743408 \n",
            "Regression loss: 0.05393892899155617\n",
            "----------------\n",
            "Loss:  1.4510419443249702 \n",
            "Classification loss: 2.7802300453186035 \n",
            "Regression loss: 0.06092692166566849\n",
            "----------------\n",
            "Loss:  1.500537782907486 \n",
            "Classification loss: 2.9100780487060547 \n",
            "Regression loss: 0.04549875855445862\n",
            "----------------\n",
            "Loss:  1.4413466341793537 \n",
            "Classification loss: 2.8076579570770264 \n",
            "Regression loss: 0.03751765564084053\n",
            "----------------\n",
            "Loss:  1.572769746184349 \n",
            "Classification loss: 3.0064806938171387 \n",
            "Regression loss: 0.06952939927577972\n",
            "----------------\n",
            "Loss:  1.3831809014081955 \n",
            "Classification loss: 2.6711180210113525 \n",
            "Regression loss: 0.047621890902519226\n",
            "----------------\n",
            "Loss:  1.3796287700533867 \n",
            "Classification loss: 2.648709535598755 \n",
            "Regression loss: 0.05527400225400925\n",
            "----------------\n",
            "Loss:  1.5353075712919235 \n",
            "Classification loss: 2.9640376567840576 \n",
            "Regression loss: 0.053288742899894714\n",
            "----------------\n",
            "Loss:  1.4787029847502708 \n",
            "Classification loss: 2.8568286895751953 \n",
            "Regression loss: 0.05028863996267319\n",
            "----------------\n",
            "Loss:  1.4427552297711372 \n",
            "Classification loss: 2.714938163757324 \n",
            "Regression loss: 0.08528614789247513\n",
            "----------------\n",
            "Loss:  1.5013945922255516 \n",
            "Classification loss: 2.9125871658325195 \n",
            "Regression loss: 0.04510100930929184\n",
            "----------------\n",
            "Loss:  1.5436404421925545 \n",
            "Classification loss: 2.936537504196167 \n",
            "Regression loss: 0.07537169009447098\n",
            "----------------\n",
            "Loss:  1.4232840463519096 \n",
            "Classification loss: 2.699280023574829 \n",
            "Regression loss: 0.07364403456449509\n",
            "----------------\n",
            "Loss:  1.4472315087914467 \n",
            "Classification loss: 2.7153868675231934 \n",
            "Regression loss: 0.08953807502985\n",
            "----------------\n",
            "Loss:  1.5075479373335838 \n",
            "Classification loss: 2.8423142433166504 \n",
            "Regression loss: 0.08639081567525864\n",
            "----------------\n",
            "Loss:  1.5600952729582787 \n",
            "Classification loss: 2.9967079162597656 \n",
            "Regression loss: 0.061741314828395844\n",
            "----------------\n",
            "Loss:  1.3659774884581566 \n",
            "Classification loss: 2.633993148803711 \n",
            "Regression loss: 0.04898091405630112\n",
            "----------------\n",
            "Loss:  1.4533183388411999 \n",
            "Classification loss: 2.8078343868255615 \n",
            "Regression loss: 0.04940114542841911\n",
            "----------------\n",
            "Loss:  1.448643073439598 \n",
            "Classification loss: 2.737514019012451 \n",
            "Regression loss: 0.0798860639333725\n",
            "----------------\n",
            "Loss:  1.4158404655754566 \n",
            "Classification loss: 2.713083028793335 \n",
            "Regression loss: 0.05929895117878914\n",
            "----------------\n",
            "Loss:  1.484366662800312 \n",
            "Classification loss: 2.8401293754577637 \n",
            "Regression loss: 0.0643019750714302\n",
            "----------------\n",
            "Loss:  1.4962666034698486 \n",
            "Classification loss: 2.9072952270507812 \n",
            "Regression loss: 0.04261898994445801\n",
            "----------------\n",
            "Loss:  1.4299769923090935 \n",
            "Classification loss: 2.6852359771728516 \n",
            "Regression loss: 0.0873590037226677\n",
            "----------------\n",
            "Loss:  1.5282507240772247 \n",
            "Classification loss: 2.879807710647583 \n",
            "Regression loss: 0.08834686875343323\n",
            "----------------\n",
            "Loss:  1.413944661617279 \n",
            "Classification loss: 2.698577404022217 \n",
            "Regression loss: 0.06465595960617065\n",
            "----------------\n",
            "Loss:  1.4981834068894386 \n",
            "Classification loss: 2.789994716644287 \n",
            "Regression loss: 0.10318604856729507\n",
            "----------------\n",
            "Loss:  1.501129373908043 \n",
            "Classification loss: 2.8630197048187256 \n",
            "Regression loss: 0.06961952149868011\n",
            "----------------\n",
            "Loss:  1.4321287348866463 \n",
            "Classification loss: 2.7460005283355713 \n",
            "Regression loss: 0.059128470718860626\n",
            "----------------\n",
            "Loss:  1.5299978628754616 \n",
            "Classification loss: 2.9416427612304688 \n",
            "Regression loss: 0.0591764822602272\n",
            "----------------\n",
            "Loss:  1.4402438625693321 \n",
            "Classification loss: 2.790487051010132 \n",
            "Regression loss: 0.045000337064266205\n",
            "----------------\n",
            "Loss:  1.4477246217429638 \n",
            "Classification loss: 2.7972733974456787 \n",
            "Regression loss: 0.049087923020124435\n",
            "----------------\n",
            "Loss:  1.4758357107639313 \n",
            "Classification loss: 2.783402919769287 \n",
            "Regression loss: 0.08413425087928772\n",
            "----------------\n",
            "Loss:  1.5785861611366272 \n",
            "Classification loss: 3.0670533180236816 \n",
            "Regression loss: 0.04505950212478638\n",
            "----------------\n",
            "Loss:  1.5077729038894176 \n",
            "Classification loss: 2.91342830657959 \n",
            "Regression loss: 0.051058750599622726\n",
            "----------------\n",
            "Loss:  1.4033502861857414 \n",
            "Classification loss: 2.716701030731201 \n",
            "Regression loss: 0.04499977082014084\n",
            "----------------\n",
            "Loss:  1.425910446792841 \n",
            "Classification loss: 2.740093469619751 \n",
            "Regression loss: 0.05586371198296547\n",
            "----------------\n",
            "Loss:  1.4105957075953484 \n",
            "Classification loss: 2.7074379920959473 \n",
            "Regression loss: 0.056876711547374725\n",
            "----------------\n",
            "Loss:  1.362062893807888 \n",
            "Classification loss: 2.6131396293640137 \n",
            "Regression loss: 0.055493079125881195\n",
            "----------------\n",
            "Loss:  1.5008451789617538 \n",
            "Classification loss: 2.8533754348754883 \n",
            "Regression loss: 0.0741574615240097\n",
            "----------------\n",
            "Loss:  1.441405437886715 \n",
            "Classification loss: 2.7477869987487793 \n",
            "Regression loss: 0.06751193851232529\n",
            "----------------\n",
            "Loss:  1.4592302739620209 \n",
            "Classification loss: 2.788724899291992 \n",
            "Regression loss: 0.06486782431602478\n",
            "----------------\n",
            "Loss:  1.4653985276818275 \n",
            "Classification loss: 2.819068193435669 \n",
            "Regression loss: 0.05586443096399307\n",
            "----------------\n",
            "Loss:  1.4270171746611595 \n",
            "Classification loss: 2.689788341522217 \n",
            "Regression loss: 0.08212300390005112\n",
            "----------------\n",
            "Loss:  1.4659573286771774 \n",
            "Classification loss: 2.7648959159851074 \n",
            "Regression loss: 0.08350937068462372\n",
            "----------------\n",
            "Loss:  1.484600804746151 \n",
            "Classification loss: 2.8276255130767822 \n",
            "Regression loss: 0.07078804820775986\n",
            "----------------\n",
            "Loss:  1.4994278103113174 \n",
            "Classification loss: 2.845877170562744 \n",
            "Regression loss: 0.07648922502994537\n",
            "----------------\n",
            "Loss:  1.4120145849883556 \n",
            "Classification loss: 2.7517051696777344 \n",
            "Regression loss: 0.03616200014948845\n",
            "----------------\n",
            "Loss:  1.4738809689879417 \n",
            "Classification loss: 2.8351311683654785 \n",
            "Regression loss: 0.056315384805202484\n",
            "----------------\n",
            "Loss:  1.3929448425769806 \n",
            "Classification loss: 2.6618478298187256 \n",
            "Regression loss: 0.0620209276676178\n",
            "----------------\n",
            "Loss:  1.4807833954691887 \n",
            "Classification loss: 2.8137292861938477 \n",
            "Regression loss: 0.07391875237226486\n",
            "----------------\n",
            "Loss:  1.4128091111779213 \n",
            "Classification loss: 2.6692278385162354 \n",
            "Regression loss: 0.07819519191980362\n",
            "----------------\n",
            "Loss:  1.4541966244578362 \n",
            "Classification loss: 2.7662038803100586 \n",
            "Regression loss: 0.07109468430280685\n",
            "----------------\n",
            "Loss:  1.4353669062256813 \n",
            "Classification loss: 2.7530882358551025 \n",
            "Regression loss: 0.058822788298130035\n",
            "----------------\n",
            "Loss:  1.3971801586449146 \n",
            "Classification loss: 2.6801857948303223 \n",
            "Regression loss: 0.057087261229753494\n",
            "----------------\n",
            "Loss:  1.5175881534814835 \n",
            "Classification loss: 2.8985323905944824 \n",
            "Regression loss: 0.06832195818424225\n",
            "----------------\n",
            "Loss:  1.4098616801202297 \n",
            "Classification loss: 2.714644432067871 \n",
            "Regression loss: 0.052539464086294174\n",
            "----------------\n",
            "Loss:  1.3907204940915108 \n",
            "Classification loss: 2.725797176361084 \n",
            "Regression loss: 0.02782190591096878\n",
            "----------------\n",
            "Loss:  1.4242265820503235 \n",
            "Classification loss: 2.7144362926483154 \n",
            "Regression loss: 0.06700843572616577\n",
            "----------------\n",
            "Loss:  1.523973397910595 \n",
            "Classification loss: 2.9474406242370605 \n",
            "Regression loss: 0.05025308579206467\n",
            "----------------\n",
            "Loss:  1.3480004891753197 \n",
            "Classification loss: 2.6275720596313477 \n",
            "Regression loss: 0.034214459359645844\n",
            "----------------\n",
            "Loss:  1.4047488644719124 \n",
            "Classification loss: 2.6661319732666016 \n",
            "Regression loss: 0.0716828778386116\n",
            "----------------\n",
            "Loss:  1.435303432866931 \n",
            "Classification loss: 2.8136115074157715 \n",
            "Regression loss: 0.02849767915904522\n",
            "----------------\n",
            "Loss:  1.4474393427371979 \n",
            "Classification loss: 2.7353708744049072 \n",
            "Regression loss: 0.07975390553474426\n",
            "----------------\n",
            "Loss:  1.4174202382564545 \n",
            "Classification loss: 2.6977720260620117 \n",
            "Regression loss: 0.06853422522544861\n",
            "----------------\n",
            "Loss:  1.337751843035221 \n",
            "Classification loss: 2.5491011142730713 \n",
            "Regression loss: 0.06320128589868546\n",
            "----------------\n",
            "Loss:  1.4091548323631287 \n",
            "Classification loss: 2.6869325637817383 \n",
            "Regression loss: 0.06568855047225952\n",
            "----------------\n",
            "Loss:  1.4035111032426357 \n",
            "Classification loss: 2.710341453552246 \n",
            "Regression loss: 0.04834037646651268\n",
            "----------------\n",
            "Loss:  1.4361068606376648 \n",
            "Classification loss: 2.689979076385498 \n",
            "Regression loss: 0.09111732244491577\n",
            "----------------\n",
            "Loss:  1.4381831549108028 \n",
            "Classification loss: 2.803680419921875 \n",
            "Regression loss: 0.03634294494986534\n",
            "----------------\n",
            "Loss:  1.4353462755680084 \n",
            "Classification loss: 2.736396312713623 \n",
            "Regression loss: 0.0671481192111969\n",
            "----------------\n",
            "Loss:  1.4145039543509483 \n",
            "Classification loss: 2.7377681732177734 \n",
            "Regression loss: 0.045619867742061615\n",
            "----------------\n",
            "Loss:  1.502893142402172 \n",
            "Classification loss: 2.8238868713378906 \n",
            "Regression loss: 0.09094970673322678\n",
            "----------------\n",
            "Loss:  1.5239778235554695 \n",
            "Classification loss: 2.910325288772583 \n",
            "Regression loss: 0.06881517916917801\n",
            "----------------\n",
            "Loss:  1.4332269616425037 \n",
            "Classification loss: 2.761448860168457 \n",
            "Regression loss: 0.05250253155827522\n",
            "----------------\n",
            "Loss:  1.4996133670210838 \n",
            "Classification loss: 2.8086447715759277 \n",
            "Regression loss: 0.09529098123311996\n",
            "----------------\n",
            "Loss:  1.4161325693130493 \n",
            "Classification loss: 2.7068047523498535 \n",
            "Regression loss: 0.06273019313812256\n",
            "----------------\n",
            "Loss:  1.3523907251656055 \n",
            "Classification loss: 2.5801401138305664 \n",
            "Regression loss: 0.06232066825032234\n",
            "----------------\n",
            "Loss:  1.3955121710896492 \n",
            "Classification loss: 2.67159366607666 \n",
            "Regression loss: 0.05971533805131912\n",
            "----------------\n",
            "Loss:  1.4363885149359703 \n",
            "Classification loss: 2.696398973464966 \n",
            "Regression loss: 0.0881890282034874\n",
            "----------------\n",
            "Loss:  1.352280117571354 \n",
            "Classification loss: 2.58186674118042 \n",
            "Regression loss: 0.06134674698114395\n",
            "----------------\n",
            "Loss:  1.4384318441152573 \n",
            "Classification loss: 2.7466020584106445 \n",
            "Regression loss: 0.065130814909935\n",
            "----------------\n",
            "Loss:  1.4538016617298126 \n",
            "Classification loss: 2.790043830871582 \n",
            "Regression loss: 0.058779746294021606\n",
            "----------------\n",
            "Loss:  1.402087401598692 \n",
            "Classification loss: 2.687283992767334 \n",
            "Regression loss: 0.05844540521502495\n",
            "----------------\n",
            "Loss:  1.4017763212323189 \n",
            "Classification loss: 2.6847758293151855 \n",
            "Regression loss: 0.059388406574726105\n",
            "----------------\n",
            "Loss:  1.337933398783207 \n",
            "Classification loss: 2.5372300148010254 \n",
            "Regression loss: 0.06931839138269424\n",
            "----------------\n",
            "Loss:  1.5336147248744965 \n",
            "Classification loss: 2.9403481483459473 \n",
            "Regression loss: 0.06344065070152283\n",
            "----------------\n",
            "Loss:  1.3460884615778923 \n",
            "Classification loss: 2.6028637886047363 \n",
            "Regression loss: 0.04465656727552414\n",
            "----------------\n",
            "Loss:  1.4010023176670074 \n",
            "Classification loss: 2.632612705230713 \n",
            "Regression loss: 0.084695965051651\n",
            "----------------\n",
            "Loss:  1.3923188149929047 \n",
            "Classification loss: 2.7015929222106934 \n",
            "Regression loss: 0.04152235388755798\n",
            "----------------\n",
            "Loss:  1.3815902397036552 \n",
            "Classification loss: 2.6391355991363525 \n",
            "Regression loss: 0.06202244013547897\n",
            "----------------\n",
            "Loss:  1.2855526395142078 \n",
            "Classification loss: 2.501218557357788 \n",
            "Regression loss: 0.0349433608353138\n",
            "----------------\n",
            "Loss:  1.2922237515449524 \n",
            "Classification loss: 2.4694957733154297 \n",
            "Regression loss: 0.05747586488723755\n",
            "----------------\n",
            "Loss:  1.445951920002699 \n",
            "Classification loss: 2.793123245239258 \n",
            "Regression loss: 0.04939029738306999\n",
            "----------------\n",
            "Loss:  1.4294459074735641 \n",
            "Classification loss: 2.6996965408325195 \n",
            "Regression loss: 0.07959763705730438\n",
            "----------------\n",
            "Loss:  1.4737182632088661 \n",
            "Classification loss: 2.795464038848877 \n",
            "Regression loss: 0.07598624378442764\n",
            "----------------\n",
            "Loss:  1.4091902747750282 \n",
            "Classification loss: 2.6339313983917236 \n",
            "Regression loss: 0.09222457557916641\n",
            "----------------\n",
            "Loss:  1.3181325122714043 \n",
            "Classification loss: 2.544887065887451 \n",
            "Regression loss: 0.04568897932767868\n",
            "----------------\n",
            "Loss:  1.373658187687397 \n",
            "Classification loss: 2.582242965698242 \n",
            "Regression loss: 0.08253670483827591\n",
            "----------------\n",
            "Loss:  1.308140017092228 \n",
            "Classification loss: 2.507115602493286 \n",
            "Regression loss: 0.05458221584558487\n",
            "----------------\n",
            "Loss:  1.4747239276766777 \n",
            "Classification loss: 2.807776927947998 \n",
            "Regression loss: 0.07083546370267868\n",
            "----------------\n",
            "Loss:  1.4653508365154266 \n",
            "Classification loss: 2.833528518676758 \n",
            "Regression loss: 0.04858657717704773\n",
            "----------------\n",
            "Loss:  1.4097786284983158 \n",
            "Classification loss: 2.7089033126831055 \n",
            "Regression loss: 0.05532697215676308\n",
            "----------------\n",
            "Loss:  1.4472259730100632 \n",
            "Classification loss: 2.7059741020202637 \n",
            "Regression loss: 0.09423892199993134\n",
            "----------------\n",
            "Loss:  1.4198539704084396 \n",
            "Classification loss: 2.7322232723236084 \n",
            "Regression loss: 0.05374233424663544\n",
            "----------------\n",
            "Loss:  1.3855773955583572 \n",
            "Classification loss: 2.611100673675537 \n",
            "Regression loss: 0.08002705872058868\n",
            "----------------\n",
            "Loss:  1.482324257493019 \n",
            "Classification loss: 2.8090648651123047 \n",
            "Regression loss: 0.07779182493686676\n",
            "----------------\n",
            "Loss:  1.3492266573011875 \n",
            "Classification loss: 2.5956227779388428 \n",
            "Regression loss: 0.05141526833176613\n",
            "----------------\n",
            "Loss:  1.3772498592734337 \n",
            "Classification loss: 2.700322151184082 \n",
            "Regression loss: 0.02708878368139267\n",
            "----------------\n",
            "Loss:  1.3398672938346863 \n",
            "Classification loss: 2.5637993812561035 \n",
            "Regression loss: 0.05796760320663452\n",
            "----------------\n",
            "Loss:  1.4047335274517536 \n",
            "Classification loss: 2.7075705528259277 \n",
            "Regression loss: 0.05094825103878975\n",
            "----------------\n",
            "Loss:  1.343013048171997 \n",
            "Classification loss: 2.5946924686431885 \n",
            "Regression loss: 0.04566681385040283\n",
            "----------------\n",
            "Loss:  1.535266451537609 \n",
            "Classification loss: 2.888490676879883 \n",
            "Regression loss: 0.0910211130976677\n",
            "----------------\n",
            "Loss:  1.4729817286133766 \n",
            "Classification loss: 2.7957944869995117 \n",
            "Regression loss: 0.07508448511362076\n",
            "----------------\n",
            "Loss:  1.4322388172149658 \n",
            "Classification loss: 2.7604610919952393 \n",
            "Regression loss: 0.05200827121734619\n",
            "----------------\n",
            "Loss:  1.4442918449640274 \n",
            "Classification loss: 2.757369041442871 \n",
            "Regression loss: 0.06560732424259186\n",
            "----------------\n",
            "Loss:  1.3700401782989502 \n",
            "Classification loss: 2.5676302909851074 \n",
            "Regression loss: 0.08622503280639648\n",
            "----------------\n",
            "Loss:  1.33406001329422 \n",
            "Classification loss: 2.534985303878784 \n",
            "Regression loss: 0.06656736135482788\n",
            "----------------\n",
            "Loss:  1.3576721772551537 \n",
            "Classification loss: 2.5756988525390625 \n",
            "Regression loss: 0.0698227509856224\n",
            "----------------\n",
            "Loss:  1.3399397283792496 \n",
            "Classification loss: 2.5284457206726074 \n",
            "Regression loss: 0.07571686804294586\n",
            "----------------\n",
            "Loss:  1.3480505794286728 \n",
            "Classification loss: 2.5405757427215576 \n",
            "Regression loss: 0.07776270806789398\n",
            "----------------\n",
            "Loss:  1.2727357782423496 \n",
            "Classification loss: 2.4318227767944336 \n",
            "Regression loss: 0.05682438984513283\n",
            "----------------\n",
            "Loss:  1.3971173241734505 \n",
            "Classification loss: 2.6355037689208984 \n",
            "Regression loss: 0.07936543971300125\n",
            "----------------\n",
            "Loss:  1.3107965551316738 \n",
            "Classification loss: 2.543029308319092 \n",
            "Regression loss: 0.039281900972127914\n",
            "----------------\n",
            "Loss:  1.3978918716311455 \n",
            "Classification loss: 2.673360824584961 \n",
            "Regression loss: 0.06121145933866501\n",
            "----------------\n",
            "Loss:  1.3718839436769485 \n",
            "Classification loss: 2.608509063720703 \n",
            "Regression loss: 0.06762941181659698\n",
            "----------------\n",
            "Loss:  1.3267468847334385 \n",
            "Classification loss: 2.5516133308410645 \n",
            "Regression loss: 0.050940219312906265\n",
            "----------------\n",
            "Loss:  1.3375878110527992 \n",
            "Classification loss: 2.547670602798462 \n",
            "Regression loss: 0.06375250965356827\n",
            "----------------\n",
            "Loss:  1.375151440501213 \n",
            "Classification loss: 2.6048855781555176 \n",
            "Regression loss: 0.07270865142345428\n",
            "----------------\n",
            "Loss:  1.2732942029833794 \n",
            "Classification loss: 2.4329867362976074 \n",
            "Regression loss: 0.05680083483457565\n",
            "----------------\n",
            "Loss:  1.353539153933525 \n",
            "Classification loss: 2.562877655029297 \n",
            "Regression loss: 0.07210032641887665\n",
            "----------------\n",
            "Loss:  1.314330868422985 \n",
            "Classification loss: 2.5492475032806396 \n",
            "Regression loss: 0.03970711678266525\n",
            "----------------\n",
            "Loss:  1.355139896273613 \n",
            "Classification loss: 2.5326201915740967 \n",
            "Regression loss: 0.08882980048656464\n",
            "----------------\n",
            "Loss:  1.3147062510252 \n",
            "Classification loss: 2.471473217010498 \n",
            "Regression loss: 0.07896964251995087\n",
            "----------------\n",
            "Loss:  1.394650436937809 \n",
            "Classification loss: 2.6672253608703613 \n",
            "Regression loss: 0.061037756502628326\n",
            "----------------\n",
            "Loss:  1.3117594793438911 \n",
            "Classification loss: 2.4699366092681885 \n",
            "Regression loss: 0.0767911747097969\n",
            "----------------\n",
            "Loss:  1.4449979439377785 \n",
            "Classification loss: 2.7405753135681152 \n",
            "Regression loss: 0.07471028715372086\n",
            "----------------\n",
            "Loss:  1.3132736757397652 \n",
            "Classification loss: 2.554622173309326 \n",
            "Regression loss: 0.03596258908510208\n",
            "----------------\n",
            "Loss:  1.4199456572532654 \n",
            "Classification loss: 2.7483935356140137 \n",
            "Regression loss: 0.045748889446258545\n",
            "----------------\n",
            "Loss:  1.3022495806217194 \n",
            "Classification loss: 2.4951703548431396 \n",
            "Regression loss: 0.054664403200149536\n",
            "----------------\n",
            "Loss:  1.2750798389315605 \n",
            "Classification loss: 2.418164014816284 \n",
            "Regression loss: 0.06599783152341843\n",
            "----------------\n",
            "Loss:  1.3514279946684837 \n",
            "Classification loss: 2.614600896835327 \n",
            "Regression loss: 0.04412754625082016\n",
            "----------------\n",
            "Loss:  1.3742705881595612 \n",
            "Classification loss: 2.5765883922576904 \n",
            "Regression loss: 0.08597639203071594\n",
            "----------------\n",
            "Loss:  1.3987034410238266 \n",
            "Classification loss: 2.697652816772461 \n",
            "Regression loss: 0.04987703263759613\n",
            "----------------\n",
            "Loss:  1.33949463814497 \n",
            "Classification loss: 2.5518836975097656 \n",
            "Regression loss: 0.06355278939008713\n",
            "----------------\n",
            "Loss:  1.3701059073209763 \n",
            "Classification loss: 2.564241886138916 \n",
            "Regression loss: 0.08798496425151825\n",
            "----------------\n",
            "Loss:  1.3185103796422482 \n",
            "Classification loss: 2.548257827758789 \n",
            "Regression loss: 0.04438146576285362\n",
            "----------------\n",
            "Loss:  1.3300645649433136 \n",
            "Classification loss: 2.552903652191162 \n",
            "Regression loss: 0.053612738847732544\n",
            "----------------\n",
            "Loss:  1.3176602236926556 \n",
            "Classification loss: 2.5313425064086914 \n",
            "Regression loss: 0.05198897048830986\n",
            "----------------\n",
            "Loss:  1.340883694589138 \n",
            "Classification loss: 2.5488505363464355 \n",
            "Regression loss: 0.06645842641592026\n",
            "----------------\n",
            "Loss:  1.3325554504990578 \n",
            "Classification loss: 2.5350966453552246 \n",
            "Regression loss: 0.06500712782144547\n",
            "----------------\n",
            "Loss:  1.351293422281742 \n",
            "Classification loss: 2.594634532928467 \n",
            "Regression loss: 0.0539761558175087\n",
            "----------------\n",
            "Loss:  1.2480359822511673 \n",
            "Classification loss: 2.387436866760254 \n",
            "Regression loss: 0.054317548871040344\n",
            "----------------\n",
            "Loss:  1.3016079477965832 \n",
            "Classification loss: 2.5007591247558594 \n",
            "Regression loss: 0.05122838541865349\n",
            "----------------\n",
            "Loss:  1.3021816462278366 \n",
            "Classification loss: 2.5401415824890137 \n",
            "Regression loss: 0.03211085498332977\n",
            "----------------\n",
            "Loss:  1.357357196509838 \n",
            "Classification loss: 2.5571064949035645 \n",
            "Regression loss: 0.07880394905805588\n",
            "----------------\n",
            "Loss:  1.3405467048287392 \n",
            "Classification loss: 2.612436532974243 \n",
            "Regression loss: 0.034328438341617584\n",
            "----------------\n",
            "Loss:  1.2761334925889969 \n",
            "Classification loss: 2.4200377464294434 \n",
            "Regression loss: 0.06611461937427521\n",
            "----------------\n",
            "Loss:  1.3614098131656647 \n",
            "Classification loss: 2.596944808959961 \n",
            "Regression loss: 0.0629374086856842\n",
            "----------------\n",
            "Loss:  1.3009883165359497 \n",
            "Classification loss: 2.515695095062256 \n",
            "Regression loss: 0.04314076900482178\n",
            "----------------\n",
            "Loss:  1.2399769127368927 \n",
            "Classification loss: 2.352938175201416 \n",
            "Regression loss: 0.06350782513618469\n",
            "----------------\n",
            "Loss:  1.4107625633478165 \n",
            "Classification loss: 2.6744585037231445 \n",
            "Regression loss: 0.0735333114862442\n",
            "----------------\n",
            "Loss:  1.29070545732975 \n",
            "Classification loss: 2.477273464202881 \n",
            "Regression loss: 0.05206872522830963\n",
            "----------------\n",
            "Loss:  1.2363455910235643 \n",
            "Classification loss: 2.4112675189971924 \n",
            "Regression loss: 0.030711831524968147\n",
            "----------------\n",
            "Loss:  1.3067582473158836 \n",
            "Classification loss: 2.480379343032837 \n",
            "Regression loss: 0.06656857579946518\n",
            "----------------\n",
            "Loss:  1.2830137833952904 \n",
            "Classification loss: 2.4618113040924072 \n",
            "Regression loss: 0.05210813134908676\n",
            "----------------\n",
            "Loss:  1.353000059723854 \n",
            "Classification loss: 2.6194350719451904 \n",
            "Regression loss: 0.04328252375125885\n",
            "----------------\n",
            "Loss:  1.418398231267929 \n",
            "Classification loss: 2.6597418785095215 \n",
            "Regression loss: 0.08852729201316833\n",
            "----------------\n",
            "Loss:  1.365666426718235 \n",
            "Classification loss: 2.619593381881714 \n",
            "Regression loss: 0.05586973577737808\n",
            "----------------\n",
            "Loss:  1.287749171257019 \n",
            "Classification loss: 2.416512966156006 \n",
            "Regression loss: 0.07949268817901611\n",
            "----------------\n",
            "Loss:  1.3855384662747383 \n",
            "Classification loss: 2.6907851696014404 \n",
            "Regression loss: 0.0401458814740181\n",
            "----------------\n",
            "Loss:  1.2605291828513145 \n",
            "Classification loss: 2.417738676071167 \n",
            "Regression loss: 0.05165984481573105\n",
            "----------------\n",
            "Loss:  1.2542551197111607 \n",
            "Classification loss: 2.4353768825531006 \n",
            "Regression loss: 0.03656667843461037\n",
            "----------------\n",
            "Loss:  1.2896907925605774 \n",
            "Classification loss: 2.5016632080078125 \n",
            "Regression loss: 0.03885918855667114\n",
            "----------------\n",
            "Loss:  1.3465176559984684 \n",
            "Classification loss: 2.5955355167388916 \n",
            "Regression loss: 0.0487498976290226\n",
            "----------------\n",
            "Loss:  1.3582029417157173 \n",
            "Classification loss: 2.5433268547058105 \n",
            "Regression loss: 0.08653951436281204\n",
            "----------------\n",
            "Loss:  1.281519040465355 \n",
            "Classification loss: 2.41581130027771 \n",
            "Regression loss: 0.07361339032649994\n",
            "----------------\n",
            "Loss:  1.4018596485257149 \n",
            "Classification loss: 2.6589269638061523 \n",
            "Regression loss: 0.0723961666226387\n",
            "----------------\n",
            "Loss:  1.3369428552687168 \n",
            "Classification loss: 2.60733699798584 \n",
            "Regression loss: 0.03327435627579689\n",
            "----------------\n",
            "Loss:  1.2907256856560707 \n",
            "Classification loss: 2.4959254264831543 \n",
            "Regression loss: 0.04276297241449356\n",
            "----------------\n",
            "Loss:  1.310125395655632 \n",
            "Classification loss: 2.482917070388794 \n",
            "Regression loss: 0.06866686046123505\n",
            "----------------\n",
            "Loss:  1.4057966358959675 \n",
            "Classification loss: 2.6913299560546875 \n",
            "Regression loss: 0.060131657868623734\n",
            "----------------\n",
            "Loss:  1.3993171155452728 \n",
            "Classification loss: 2.6577768325805664 \n",
            "Regression loss: 0.07042869925498962\n",
            "----------------\n",
            "Loss:  1.309055708348751 \n",
            "Classification loss: 2.503899574279785 \n",
            "Regression loss: 0.05710592120885849\n",
            "----------------\n",
            "Loss:  1.2319205291569233 \n",
            "Classification loss: 2.340684413909912 \n",
            "Regression loss: 0.06157832220196724\n",
            "----------------\n",
            "Loss:  1.2341588996350765 \n",
            "Classification loss: 2.388282537460327 \n",
            "Regression loss: 0.04001763090491295\n",
            "----------------\n",
            "Loss:  1.4823333472013474 \n",
            "Classification loss: 2.7221062183380127 \n",
            "Regression loss: 0.121280238032341\n",
            "----------------\n",
            "Loss:  1.2933330237865448 \n",
            "Classification loss: 2.4449567794799805 \n",
            "Regression loss: 0.07085463404655457\n",
            "----------------\n",
            "Loss:  1.4163555577397346 \n",
            "Classification loss: 2.702829360961914 \n",
            "Regression loss: 0.06494087725877762\n",
            "----------------\n",
            "Loss:  1.3553609885275364 \n",
            "Classification loss: 2.6043684482574463 \n",
            "Regression loss: 0.05317676439881325\n",
            "----------------\n",
            "Loss:  1.2750350013375282 \n",
            "Classification loss: 2.451051712036133 \n",
            "Regression loss: 0.04950914531946182\n",
            "----------------\n",
            "Loss:  1.290757529437542 \n",
            "Classification loss: 2.4924492835998535 \n",
            "Regression loss: 0.044532887637615204\n",
            "----------------\n",
            "Loss:  1.315521478652954 \n",
            "Classification loss: 2.5349104404449463 \n",
            "Regression loss: 0.04806625843048096\n",
            "----------------\n",
            "Loss:  1.399415597319603 \n",
            "Classification loss: 2.631364583969116 \n",
            "Regression loss: 0.08373330533504486\n",
            "----------------\n",
            "Loss:  1.3463784009218216 \n",
            "Classification loss: 2.588792562484741 \n",
            "Regression loss: 0.05198211967945099\n",
            "----------------\n",
            "Loss:  1.3222915679216385 \n",
            "Classification loss: 2.494609832763672 \n",
            "Regression loss: 0.07498665153980255\n",
            "----------------\n",
            "Loss:  1.2383278086781502 \n",
            "Classification loss: 2.3840625286102295 \n",
            "Regression loss: 0.04629654437303543\n",
            "----------------\n",
            "Loss:  1.3624789491295815 \n",
            "Classification loss: 2.6316466331481934 \n",
            "Regression loss: 0.04665563255548477\n",
            "----------------\n",
            "Loss:  1.3173985034227371 \n",
            "Classification loss: 2.49454927444458 \n",
            "Regression loss: 0.07012386620044708\n",
            "----------------\n",
            "Loss:  1.2153797000646591 \n",
            "Classification loss: 2.351933002471924 \n",
            "Regression loss: 0.039413198828697205\n",
            "----------------\n",
            "Loss:  1.3579039052128792 \n",
            "Classification loss: 2.6023802757263184 \n",
            "Regression loss: 0.05671376734972\n",
            "----------------\n",
            "Loss:  1.2691450640559196 \n",
            "Classification loss: 2.392254590988159 \n",
            "Regression loss: 0.07301776856184006\n",
            "----------------\n",
            "Loss:  1.2569670304656029 \n",
            "Classification loss: 2.409223794937134 \n",
            "Regression loss: 0.05235513299703598\n",
            "----------------\n",
            "Loss:  1.270511418581009 \n",
            "Classification loss: 2.407951831817627 \n",
            "Regression loss: 0.06653550267219543\n",
            "----------------\n",
            "Loss:  1.303149938583374 \n",
            "Classification loss: 2.4687576293945312 \n",
            "Regression loss: 0.0687711238861084\n",
            "----------------\n",
            "Loss:  1.3464371375739574 \n",
            "Classification loss: 2.571955680847168 \n",
            "Regression loss: 0.06045929715037346\n",
            "----------------\n",
            "Loss:  1.3350962176918983 \n",
            "Classification loss: 2.5895345211029053 \n",
            "Regression loss: 0.04032895714044571\n",
            "----------------\n",
            "Loss:  1.250043485313654 \n",
            "Classification loss: 2.4185099601745605 \n",
            "Regression loss: 0.04078850522637367\n",
            "----------------\n",
            "Loss:  1.214264139533043 \n",
            "Classification loss: 2.259441375732422 \n",
            "Regression loss: 0.08454345166683197\n",
            "----------------\n",
            "Loss:  1.31593481823802 \n",
            "Classification loss: 2.5316929817199707 \n",
            "Regression loss: 0.05008832737803459\n",
            "----------------\n",
            "Loss:  1.3893007338047028 \n",
            "Classification loss: 2.6494665145874023 \n",
            "Regression loss: 0.06456747651100159\n",
            "----------------\n",
            "Loss:  1.2983530014753342 \n",
            "Classification loss: 2.457714557647705 \n",
            "Regression loss: 0.06949572265148163\n",
            "----------------\n",
            "Loss:  1.4369994401931763 \n",
            "Classification loss: 2.7185606956481934 \n",
            "Regression loss: 0.07771909236907959\n",
            "----------------\n",
            "Loss:  1.3031080141663551 \n",
            "Classification loss: 2.463796615600586 \n",
            "Regression loss: 0.07120970636606216\n",
            "----------------\n",
            "Loss:  1.3520387634634972 \n",
            "Classification loss: 2.61807918548584 \n",
            "Regression loss: 0.04299917072057724\n",
            "----------------\n",
            "Loss:  1.3758845999836922 \n",
            "Classification loss: 2.6065306663513184 \n",
            "Regression loss: 0.07261926680803299\n",
            "----------------\n",
            "Loss:  1.2229446321725845 \n",
            "Classification loss: 2.286266565322876 \n",
            "Regression loss: 0.07981134951114655\n",
            "----------------\n",
            "Loss:  1.3496123887598515 \n",
            "Classification loss: 2.60377836227417 \n",
            "Regression loss: 0.047723207622766495\n",
            "----------------\n",
            "Loss:  1.3895152807235718 \n",
            "Classification loss: 2.663008451461792 \n",
            "Regression loss: 0.05801105499267578\n",
            "----------------\n",
            "Loss:  1.2771012112498283 \n",
            "Classification loss: 2.423462152481079 \n",
            "Regression loss: 0.06537013500928879\n",
            "----------------\n",
            "Loss:  1.3674412965774536 \n",
            "Classification loss: 2.580220937728882 \n",
            "Regression loss: 0.0773308277130127\n",
            "----------------\n",
            "Loss:  1.412099964916706 \n",
            "Classification loss: 2.6980676651000977 \n",
            "Regression loss: 0.06306613236665726\n",
            "----------------\n",
            "Loss:  1.2567440271377563 \n",
            "Classification loss: 2.349931478500366 \n",
            "Regression loss: 0.08177828788757324\n",
            "----------------\n",
            "Loss:  1.2247665226459503 \n",
            "Classification loss: 2.3346219062805176 \n",
            "Regression loss: 0.05745556950569153\n",
            "----------------\n",
            "Loss:  1.2706502228975296 \n",
            "Classification loss: 2.45418643951416 \n",
            "Regression loss: 0.043557003140449524\n",
            "----------------\n",
            "Loss:  1.295035794377327 \n",
            "Classification loss: 2.442570209503174 \n",
            "Regression loss: 0.07375068962574005\n",
            "----------------\n",
            "Loss:  1.2728239819407463 \n",
            "Classification loss: 2.4173824787139893 \n",
            "Regression loss: 0.06413274258375168\n",
            "----------------\n",
            "Loss:  1.2975324243307114 \n",
            "Classification loss: 2.4706153869628906 \n",
            "Regression loss: 0.06222473084926605\n",
            "----------------\n",
            "Loss:  1.3363480418920517 \n",
            "Classification loss: 2.577289581298828 \n",
            "Regression loss: 0.047703251242637634\n",
            "----------------\n",
            "Loss:  1.3935275375843048 \n",
            "Classification loss: 2.671876907348633 \n",
            "Regression loss: 0.0575890839099884\n",
            "----------------\n",
            "Loss:  1.1358879543840885 \n",
            "Classification loss: 2.187406301498413 \n",
            "Regression loss: 0.04218480363488197\n",
            "----------------\n",
            "Loss:  1.291839137673378 \n",
            "Classification loss: 2.378326892852783 \n",
            "Regression loss: 0.10267569124698639\n",
            "----------------\n",
            "Loss:  1.2964819893240929 \n",
            "Classification loss: 2.480719804763794 \n",
            "Regression loss: 0.05612208694219589\n",
            "----------------\n",
            "Loss:  1.4885220155119896 \n",
            "Classification loss: 2.849228620529175 \n",
            "Regression loss: 0.06390770524740219\n",
            "----------------\n",
            "Loss:  1.3111656457185745 \n",
            "Classification loss: 2.469536304473877 \n",
            "Regression loss: 0.07639749348163605\n",
            "----------------\n",
            "Loss:  1.374052569270134 \n",
            "Classification loss: 2.6554250717163086 \n",
            "Regression loss: 0.046340033411979675\n",
            "----------------\n",
            "Loss:  1.344742327928543 \n",
            "Classification loss: 2.6063530445098877 \n",
            "Regression loss: 0.04156580567359924\n",
            "----------------\n",
            "Loss:  1.3288213834166527 \n",
            "Classification loss: 2.5548901557922363 \n",
            "Regression loss: 0.051376305520534515\n",
            "----------------\n",
            "Loss:  1.3046103194355965 \n",
            "Classification loss: 2.496450424194336 \n",
            "Regression loss: 0.0563851073384285\n",
            "----------------\n",
            "Loss:  1.239975031465292 \n",
            "Classification loss: 2.40324068069458 \n",
            "Regression loss: 0.03835469111800194\n",
            "----------------\n",
            "Loss:  1.329208180308342 \n",
            "Classification loss: 2.5940017700195312 \n",
            "Regression loss: 0.032207295298576355\n",
            "----------------\n",
            "Loss:  1.2355064526200294 \n",
            "Classification loss: 2.3454549312591553 \n",
            "Regression loss: 0.06277898699045181\n",
            "----------------\n",
            "Loss:  1.3801954686641693 \n",
            "Classification loss: 2.552633285522461 \n",
            "Regression loss: 0.10387882590293884\n",
            "----------------\n",
            "Loss:  1.3498767353594303 \n",
            "Classification loss: 2.6429789066314697 \n",
            "Regression loss: 0.02838728204369545\n",
            "----------------\n",
            "Loss:  1.2779652178287506 \n",
            "Classification loss: 2.4283666610717773 \n",
            "Regression loss: 0.06378188729286194\n",
            "----------------\n",
            "Loss:  1.4307253286242485 \n",
            "Classification loss: 2.7344775199890137 \n",
            "Regression loss: 0.06348656862974167\n",
            "----------------\n",
            "Loss:  1.3322791419923306 \n",
            "Classification loss: 2.539947509765625 \n",
            "Regression loss: 0.06230538710951805\n",
            "----------------\n",
            "Loss:  1.3518757820129395 \n",
            "Classification loss: 2.573329448699951 \n",
            "Regression loss: 0.06521105766296387\n",
            "----------------\n",
            "Loss:  1.2582540288567543 \n",
            "Classification loss: 2.37772274017334 \n",
            "Regression loss: 0.06939265877008438\n",
            "----------------\n",
            "Loss:  1.2876532152295113 \n",
            "Classification loss: 2.3947787284851074 \n",
            "Regression loss: 0.09026385098695755\n",
            "----------------\n",
            "Loss:  1.311447937041521 \n",
            "Classification loss: 2.498838186264038 \n",
            "Regression loss: 0.06202884390950203\n",
            "----------------\n",
            "Loss:  1.2970925569534302 \n",
            "Classification loss: 2.43157696723938 \n",
            "Regression loss: 0.08130407333374023\n",
            "----------------\n",
            "Loss:  1.310961589217186 \n",
            "Classification loss: 2.479125738143921 \n",
            "Regression loss: 0.07139872014522552\n",
            "----------------\n",
            "Loss:  1.2699523121118546 \n",
            "Classification loss: 2.3781609535217285 \n",
            "Regression loss: 0.0808718353509903\n",
            "----------------\n",
            "Loss:  1.2663668245077133 \n",
            "Classification loss: 2.426248073577881 \n",
            "Regression loss: 0.05324278771877289\n",
            "----------------\n",
            "Loss:  1.343845471739769 \n",
            "Classification loss: 2.562020778656006 \n",
            "Regression loss: 0.06283508241176605\n",
            "----------------\n",
            "Loss:  1.202180027961731 \n",
            "Classification loss: 2.258551597595215 \n",
            "Regression loss: 0.07290422916412354\n",
            "----------------\n",
            "Loss:  1.2991915978491306 \n",
            "Classification loss: 2.491422414779663 \n",
            "Regression loss: 0.05348039045929909\n",
            "----------------\n",
            "Loss:  1.1868511959910393 \n",
            "Classification loss: 2.270308017730713 \n",
            "Regression loss: 0.05169718712568283\n",
            "----------------\n",
            "Loss:  1.2529328726232052 \n",
            "Classification loss: 2.428117036819458 \n",
            "Regression loss: 0.03887435421347618\n",
            "----------------\n",
            "Loss:  1.2440983466804028 \n",
            "Classification loss: 2.3818037509918213 \n",
            "Regression loss: 0.05319647118449211\n",
            "----------------\n",
            "Loss:  1.2789491266012192 \n",
            "Classification loss: 2.4597747325897217 \n",
            "Regression loss: 0.04906176030635834\n",
            "----------------\n",
            "Loss:  1.305364042520523 \n",
            "Classification loss: 2.5351157188415527 \n",
            "Regression loss: 0.037806183099746704\n",
            "----------------\n",
            "Loss:  1.2918341867625713 \n",
            "Classification loss: 2.517291784286499 \n",
            "Regression loss: 0.03318829461932182\n",
            "----------------\n",
            "Loss:  1.2825552374124527 \n",
            "Classification loss: 2.4746625423431396 \n",
            "Regression loss: 0.045223966240882874\n",
            "----------------\n",
            "Loss:  1.2201189622282982 \n",
            "Classification loss: 2.3378098011016846 \n",
            "Regression loss: 0.0512140616774559\n",
            "----------------\n",
            "Loss:  1.3416738957166672 \n",
            "Classification loss: 2.5470917224884033 \n",
            "Regression loss: 0.06812803447246552\n",
            "----------------\n",
            "Loss:  1.2801072522997856 \n",
            "Classification loss: 2.430731773376465 \n",
            "Regression loss: 0.06474136561155319\n",
            "----------------\n",
            "Loss:  1.3307246714830399 \n",
            "Classification loss: 2.5344314575195312 \n",
            "Regression loss: 0.06350894272327423\n",
            "----------------\n",
            "Loss:  1.2105155400931835 \n",
            "Classification loss: 2.316636085510254 \n",
            "Regression loss: 0.052197497338056564\n",
            "----------------\n",
            "Loss:  1.23933919146657 \n",
            "Classification loss: 2.3826193809509277 \n",
            "Regression loss: 0.04802950099110603\n",
            "----------------\n",
            "Loss:  1.359812930226326 \n",
            "Classification loss: 2.5750036239624023 \n",
            "Regression loss: 0.07231111824512482\n",
            "----------------\n",
            "Loss:  1.2600272819399834 \n",
            "Classification loss: 2.3806381225585938 \n",
            "Regression loss: 0.06970822066068649\n",
            "----------------\n",
            "Loss:  1.1987309530377388 \n",
            "Classification loss: 2.2648262977600098 \n",
            "Regression loss: 0.06631780415773392\n",
            "----------------\n",
            "Loss:  1.3466281481087208 \n",
            "Classification loss: 2.6153409481048584 \n",
            "Regression loss: 0.03895767405629158\n",
            "----------------\n",
            "Loss:  1.278708778321743 \n",
            "Classification loss: 2.4225194454193115 \n",
            "Regression loss: 0.06744905561208725\n",
            "----------------\n",
            "Loss:  1.2427298203110695 \n",
            "Classification loss: 2.3629355430603027 \n",
            "Regression loss: 0.06126204878091812\n",
            "----------------\n",
            "Loss:  1.213479671627283 \n",
            "Classification loss: 2.3442957401275635 \n",
            "Regression loss: 0.04133180156350136\n",
            "----------------\n",
            "Loss:  1.2651076950132847 \n",
            "Classification loss: 2.433779716491699 \n",
            "Regression loss: 0.048217836767435074\n",
            "----------------\n",
            "Loss:  1.1520092152059078 \n",
            "Classification loss: 2.1885299682617188 \n",
            "Regression loss: 0.05774423107504845\n",
            "----------------\n",
            "Loss:  1.2976506501436234 \n",
            "Classification loss: 2.4639296531677246 \n",
            "Regression loss: 0.06568582355976105\n",
            "----------------\n",
            "Loss:  1.305427897721529 \n",
            "Classification loss: 2.506875991821289 \n",
            "Regression loss: 0.051989901810884476\n",
            "----------------\n",
            "Loss:  1.3145826160907745 \n",
            "Classification loss: 2.4760866165161133 \n",
            "Regression loss: 0.0765393078327179\n",
            "----------------\n",
            "Loss:  1.2281663082540035 \n",
            "Classification loss: 2.3401403427124023 \n",
            "Regression loss: 0.05809613689780235\n",
            "----------------\n",
            "Loss:  1.2614755779504776 \n",
            "Classification loss: 2.3625481128692627 \n",
            "Regression loss: 0.08020152151584625\n",
            "----------------\n",
            "Loss:  1.3145620748400688 \n",
            "Classification loss: 2.514087200164795 \n",
            "Regression loss: 0.057518474757671356\n",
            "----------------\n",
            "Loss:  1.3288389593362808 \n",
            "Classification loss: 2.5244126319885254 \n",
            "Regression loss: 0.06663264334201813\n",
            "----------------\n",
            "Loss:  1.161786027252674 \n",
            "Classification loss: 2.2031478881835938 \n",
            "Regression loss: 0.06021208316087723\n",
            "----------------\n",
            "Loss:  1.2339702285826206 \n",
            "Classification loss: 2.3540871143341064 \n",
            "Regression loss: 0.0569266714155674\n",
            "----------------\n",
            "Loss:  1.1752593144774437 \n",
            "Classification loss: 2.187865734100342 \n",
            "Regression loss: 0.0813264474272728\n",
            "----------------\n",
            "Loss:  1.2219963371753693 \n",
            "Classification loss: 2.321018695831299 \n",
            "Regression loss: 0.06148698925971985\n",
            "----------------\n",
            "Loss:  1.2442434839904308 \n",
            "Classification loss: 2.3772079944610596 \n",
            "Regression loss: 0.05563948675990105\n",
            "----------------\n",
            "Loss:  1.255680974572897 \n",
            "Classification loss: 2.4262936115264893 \n",
            "Regression loss: 0.04253416880965233\n",
            "----------------\n",
            "Loss:  1.2465076446533203 \n",
            "Classification loss: 2.36291766166687 \n",
            "Regression loss: 0.06504881381988525\n",
            "----------------\n",
            "Loss:  1.2106328532099724 \n",
            "Classification loss: 2.3199357986450195 \n",
            "Regression loss: 0.050664953887462616\n",
            "----------------\n",
            "Loss:  1.2338432371616364 \n",
            "Classification loss: 2.3871877193450928 \n",
            "Regression loss: 0.040249377489089966\n",
            "----------------\n",
            "Loss:  1.169133685529232 \n",
            "Classification loss: 2.2537753582000732 \n",
            "Regression loss: 0.042246006429195404\n",
            "----------------\n",
            "Loss:  1.2996795549988747 \n",
            "Classification loss: 2.4909260272979736 \n",
            "Regression loss: 0.05421654134988785\n",
            "----------------\n",
            "Loss:  1.1874328218400478 \n",
            "Classification loss: 2.266777276992798 \n",
            "Regression loss: 0.05404418334364891\n",
            "----------------\n",
            "Loss:  1.348666723817587 \n",
            "Classification loss: 2.5750772953033447 \n",
            "Regression loss: 0.061128076165914536\n",
            "----------------\n",
            "Loss:  1.2636656537652016 \n",
            "Classification loss: 2.3606224060058594 \n",
            "Regression loss: 0.08335445076227188\n",
            "----------------\n",
            "Loss:  1.2214367762207985 \n",
            "Classification loss: 2.31704044342041 \n",
            "Regression loss: 0.06291655451059341\n",
            "----------------\n",
            "Loss:  1.2894019335508347 \n",
            "Classification loss: 2.466160774230957 \n",
            "Regression loss: 0.05632154643535614\n",
            "----------------\n",
            "Loss:  1.2584452964365482 \n",
            "Classification loss: 2.4092493057250977 \n",
            "Regression loss: 0.053820643573999405\n",
            "----------------\n",
            "Loss:  1.3273189812898636 \n",
            "Classification loss: 2.5270514488220215 \n",
            "Regression loss: 0.06379325687885284\n",
            "----------------\n",
            "Loss:  1.3018093556165695 \n",
            "Classification loss: 2.4783756732940674 \n",
            "Regression loss: 0.06262151896953583\n",
            "----------------\n",
            "Loss:  1.3106883838772774 \n",
            "Classification loss: 2.4962291717529297 \n",
            "Regression loss: 0.06257379800081253\n",
            "----------------\n",
            "Loss:  1.3101712986826897 \n",
            "Classification loss: 2.4663186073303223 \n",
            "Regression loss: 0.07701199501752853\n",
            "----------------\n",
            "Loss:  1.312471803277731 \n",
            "Classification loss: 2.5206587314605713 \n",
            "Regression loss: 0.0521424375474453\n",
            "----------------\n",
            "Loss:  1.2460503727197647 \n",
            "Classification loss: 2.3698387145996094 \n",
            "Regression loss: 0.06113101541996002\n",
            "----------------\n",
            "Loss:  1.1891539618372917 \n",
            "Classification loss: 2.273554801940918 \n",
            "Regression loss: 0.05237656086683273\n",
            "----------------\n",
            "Loss:  1.322615947574377 \n",
            "Classification loss: 2.5612497329711914 \n",
            "Regression loss: 0.04199108108878136\n",
            "----------------\n",
            "Loss:  1.2758609056472778 \n",
            "Classification loss: 2.4697465896606445 \n",
            "Regression loss: 0.040987610816955566\n",
            "----------------\n",
            "Loss:  1.2446991242468357 \n",
            "Classification loss: 2.4076027870178223 \n",
            "Regression loss: 0.040897730737924576\n",
            "----------------\n",
            "Loss:  1.1545161604881287 \n",
            "Classification loss: 2.1997389793395996 \n",
            "Regression loss: 0.05464667081832886\n",
            "----------------\n",
            "Loss:  1.2336474433541298 \n",
            "Classification loss: 2.3742151260375977 \n",
            "Regression loss: 0.04653988033533096\n",
            "----------------\n",
            "Loss:  1.2878761142492294 \n",
            "Classification loss: 2.4788918495178223 \n",
            "Regression loss: 0.0484301894903183\n",
            "----------------\n",
            "Loss:  1.2800782471895218 \n",
            "Classification loss: 2.4911015033721924 \n",
            "Regression loss: 0.0345274955034256\n",
            "----------------\n",
            "Loss:  1.2778185158967972 \n",
            "Classification loss: 2.465205669403076 \n",
            "Regression loss: 0.045215681195259094\n",
            "----------------\n",
            "Loss:  1.247382652014494 \n",
            "Classification loss: 2.3921053409576416 \n",
            "Regression loss: 0.05132998153567314\n",
            "----------------\n",
            "Loss:  1.265853151679039 \n",
            "Classification loss: 2.423703908920288 \n",
            "Regression loss: 0.05400119721889496\n",
            "----------------\n",
            "Loss:  1.169960904866457 \n",
            "Classification loss: 2.2897660732269287 \n",
            "Regression loss: 0.02507786825299263\n",
            "----------------\n",
            "Loss:  1.292193554341793 \n",
            "Classification loss: 2.4643497467041016 \n",
            "Regression loss: 0.06001868098974228\n",
            "----------------\n",
            "Loss:  1.2240100726485252 \n",
            "Classification loss: 2.344508171081543 \n",
            "Regression loss: 0.051755987107753754\n",
            "----------------\n",
            "Loss:  1.2962641417980194 \n",
            "Classification loss: 2.515733242034912 \n",
            "Regression loss: 0.038397520780563354\n",
            "----------------\n",
            "Loss:  1.1744874343276024 \n",
            "Classification loss: 2.2203774452209473 \n",
            "Regression loss: 0.06429871171712875\n",
            "----------------\n",
            "Loss:  1.1999839954078197 \n",
            "Classification loss: 2.3046722412109375 \n",
            "Regression loss: 0.047647874802351\n",
            "----------------\n",
            "Loss:  1.231538888067007 \n",
            "Classification loss: 2.3741455078125 \n",
            "Regression loss: 0.044466134160757065\n",
            "----------------\n",
            "Loss:  1.1144358403980732 \n",
            "Classification loss: 2.1664929389953613 \n",
            "Regression loss: 0.031189370900392532\n",
            "----------------\n",
            "Loss:  1.2170558869838715 \n",
            "Classification loss: 2.305332660675049 \n",
            "Regression loss: 0.06438955664634705\n",
            "----------------\n",
            "Loss:  1.253473311662674 \n",
            "Classification loss: 2.4235639572143555 \n",
            "Regression loss: 0.041691333055496216\n",
            "----------------\n",
            "Loss:  1.2264335788786411 \n",
            "Classification loss: 2.3687281608581543 \n",
            "Regression loss: 0.04206949844956398\n",
            "----------------\n",
            "Loss:  1.286640852689743 \n",
            "Classification loss: 2.4760937690734863 \n",
            "Regression loss: 0.04859396815299988\n",
            "----------------\n",
            "Loss:  1.2641567066311836 \n",
            "Classification loss: 2.4345996379852295 \n",
            "Regression loss: 0.04685688763856888\n",
            "----------------\n",
            "Loss:  1.25180097296834 \n",
            "Classification loss: 2.393867015838623 \n",
            "Regression loss: 0.0548674650490284\n",
            "----------------\n",
            "Loss:  1.182916909456253 \n",
            "Classification loss: 2.257169485092163 \n",
            "Regression loss: 0.05433216691017151\n",
            "----------------\n",
            "Loss:  1.2722675800323486 \n",
            "Classification loss: 2.3879494667053223 \n",
            "Regression loss: 0.0782928466796875\n",
            "----------------\n",
            "Loss:  1.3100411221385002 \n",
            "Classification loss: 2.5309395790100098 \n",
            "Regression loss: 0.04457133263349533\n",
            "----------------\n",
            "Loss:  1.2380643114447594 \n",
            "Classification loss: 2.342534065246582 \n",
            "Regression loss: 0.06679727882146835\n",
            "----------------\n",
            "Loss:  1.1391425356268883 \n",
            "Classification loss: 2.1695213317871094 \n",
            "Regression loss: 0.05438186973333359\n",
            "----------------\n",
            "Loss:  1.2489361763000488 \n",
            "Classification loss: 2.408033847808838 \n",
            "Regression loss: 0.04491925239562988\n",
            "----------------\n",
            "Loss:  1.2771761864423752 \n",
            "Classification loss: 2.39788818359375 \n",
            "Regression loss: 0.07823209464550018\n",
            "----------------\n",
            "Loss:  1.2226161733269691 \n",
            "Classification loss: 2.364767074584961 \n",
            "Regression loss: 0.04023263603448868\n",
            "----------------\n",
            "Loss:  1.2117644846439362 \n",
            "Classification loss: 2.3057804107666016 \n",
            "Regression loss: 0.058874279260635376\n",
            "----------------\n",
            "Loss:  1.0647374615073204 \n",
            "Classification loss: 1.9997907876968384 \n",
            "Regression loss: 0.06484206765890121\n",
            "----------------\n",
            "Loss:  1.2668957710266113 \n",
            "Classification loss: 2.4206573963165283 \n",
            "Regression loss: 0.05656707286834717\n",
            "----------------\n",
            "Loss:  1.2314230911433697 \n",
            "Classification loss: 2.360445022583008 \n",
            "Regression loss: 0.05120057985186577\n",
            "----------------\n",
            "Loss:  1.2876199632883072 \n",
            "Classification loss: 2.457868814468384 \n",
            "Regression loss: 0.058685556054115295\n",
            "----------------\n",
            "Loss:  1.2101606503129005 \n",
            "Classification loss: 2.250917911529541 \n",
            "Regression loss: 0.08470169454813004\n",
            "----------------\n",
            "Loss:  1.2849672231823206 \n",
            "Classification loss: 2.5081005096435547 \n",
            "Regression loss: 0.03091696836054325\n",
            "----------------\n",
            "Loss:  1.2927028834819794 \n",
            "Classification loss: 2.4284327030181885 \n",
            "Regression loss: 0.07848653197288513\n",
            "----------------\n",
            "Loss:  1.271572731435299 \n",
            "Classification loss: 2.406473159790039 \n",
            "Regression loss: 0.06833615154027939\n",
            "----------------\n",
            "Loss:  1.143913872539997 \n",
            "Classification loss: 2.1564230918884277 \n",
            "Regression loss: 0.06570232659578323\n",
            "----------------\n",
            "Loss:  1.1993873566389084 \n",
            "Classification loss: 2.275189161300659 \n",
            "Regression loss: 0.061792775988578796\n",
            "----------------\n",
            "Loss:  1.2724640183150768 \n",
            "Classification loss: 2.48465895652771 \n",
            "Regression loss: 0.030134540051221848\n",
            "----------------\n",
            "Loss:  1.2484939694404602 \n",
            "Classification loss: 2.419203042984009 \n",
            "Regression loss: 0.03889244794845581\n",
            "----------------\n",
            "Loss:  1.2457152232527733 \n",
            "Classification loss: 2.3709869384765625 \n",
            "Regression loss: 0.060221754014492035\n",
            "----------------\n",
            "Loss:  1.2910487204790115 \n",
            "Classification loss: 2.4224085807800293 \n",
            "Regression loss: 0.07984443008899689\n",
            "----------------\n",
            "Loss:  1.224510870873928 \n",
            "Classification loss: 2.3134679794311523 \n",
            "Regression loss: 0.0677768811583519\n",
            "----------------\n",
            "Loss:  1.2986171953380108 \n",
            "Classification loss: 2.4791629314422607 \n",
            "Regression loss: 0.05903572961688042\n",
            "----------------\n",
            "Loss:  1.3208326250314713 \n",
            "Classification loss: 2.410783290863037 \n",
            "Regression loss: 0.1154409795999527\n",
            "----------------\n",
            "Loss:  1.2656313925981522 \n",
            "Classification loss: 2.4036617279052734 \n",
            "Regression loss: 0.06380052864551544\n",
            "----------------\n",
            "Loss:  1.1782433204352856 \n",
            "Classification loss: 2.260633945465088 \n",
            "Regression loss: 0.04792634770274162\n",
            "----------------\n",
            "Loss:  1.1311798878014088 \n",
            "Classification loss: 2.1911239624023438 \n",
            "Regression loss: 0.03561790660023689\n",
            "----------------\n",
            "Loss:  1.2065848670899868 \n",
            "Classification loss: 2.3272624015808105 \n",
            "Regression loss: 0.04295366629958153\n",
            "----------------\n",
            "Loss:  1.185829520225525 \n",
            "Classification loss: 2.2458302974700928 \n",
            "Regression loss: 0.06291437149047852\n",
            "----------------\n",
            "Loss:  1.32409305870533 \n",
            "Classification loss: 2.444765090942383 \n",
            "Regression loss: 0.10171051323413849\n",
            "----------------\n",
            "Loss:  1.2975565046072006 \n",
            "Classification loss: 2.422473430633545 \n",
            "Regression loss: 0.08631978929042816\n",
            "----------------\n",
            "Loss:  1.23296707123518 \n",
            "Classification loss: 2.3314571380615234 \n",
            "Regression loss: 0.06723850220441818\n",
            "----------------\n",
            "Loss:  1.3113426342606544 \n",
            "Classification loss: 2.497269868850708 \n",
            "Regression loss: 0.06270769983530045\n",
            "----------------\n",
            "Loss:  1.1717624515295029 \n",
            "Classification loss: 2.1994423866271973 \n",
            "Regression loss: 0.07204125821590424\n",
            "----------------\n",
            "Loss:  1.2063085213303566 \n",
            "Classification loss: 2.3016605377197266 \n",
            "Regression loss: 0.05547825247049332\n",
            "----------------\n",
            "Loss:  1.2227921970188618 \n",
            "Classification loss: 2.330888271331787 \n",
            "Regression loss: 0.057348061352968216\n",
            "----------------\n",
            "Loss:  1.2137752920389175 \n",
            "Classification loss: 2.3227977752685547 \n",
            "Regression loss: 0.0523764044046402\n",
            "----------------\n",
            "Loss:  1.1857503280043602 \n",
            "Classification loss: 2.2466750144958496 \n",
            "Regression loss: 0.062412820756435394\n",
            "----------------\n",
            "Loss:  1.1746916249394417 \n",
            "Classification loss: 2.238706111907959 \n",
            "Regression loss: 0.05533856898546219\n",
            "----------------\n",
            "Loss:  1.1845200583338737 \n",
            "Classification loss: 2.2629642486572266 \n",
            "Regression loss: 0.05303793400526047\n",
            "----------------\n",
            "Loss:  1.2188808917999268 \n",
            "Classification loss: 2.3227224349975586 \n",
            "Regression loss: 0.05751967430114746\n",
            "----------------\n",
            "Loss:  1.2504993379116058 \n",
            "Classification loss: 2.376399517059326 \n",
            "Regression loss: 0.06229957938194275\n",
            "----------------\n",
            "Loss:  1.2459372952580452 \n",
            "Classification loss: 2.3126587867736816 \n",
            "Regression loss: 0.08960790187120438\n",
            "----------------\n",
            "Loss:  1.0989361926913261 \n",
            "Classification loss: 2.023221969604492 \n",
            "Regression loss: 0.08732520788908005\n",
            "----------------\n",
            "Loss:  1.1775057464838028 \n",
            "Classification loss: 2.2103195190429688 \n",
            "Regression loss: 0.07234598696231842\n",
            "----------------\n",
            "Loss:  1.204683605581522 \n",
            "Classification loss: 2.305117130279541 \n",
            "Regression loss: 0.05212504044175148\n",
            "----------------\n",
            "Loss:  1.3278809115290642 \n",
            "Classification loss: 2.5286574363708496 \n",
            "Regression loss: 0.06355219334363937\n",
            "----------------\n",
            "Loss:  1.1430965811014175 \n",
            "Classification loss: 2.184140205383301 \n",
            "Regression loss: 0.05102647840976715\n",
            "----------------\n",
            "Loss:  1.1247548386454582 \n",
            "Classification loss: 2.140535354614258 \n",
            "Regression loss: 0.054487161338329315\n",
            "----------------\n",
            "Loss:  1.1687635742127895 \n",
            "Classification loss: 2.2579269409179688 \n",
            "Regression loss: 0.03980010375380516\n",
            "----------------\n",
            "Loss:  1.2498128116130829 \n",
            "Classification loss: 2.3896379470825195 \n",
            "Regression loss: 0.05499383807182312\n",
            "----------------\n",
            "Loss:  1.2137839868664742 \n",
            "Classification loss: 2.297603130340576 \n",
            "Regression loss: 0.06498242169618607\n",
            "----------------\n",
            "Loss:  1.1721412912011147 \n",
            "Classification loss: 2.2052435874938965 \n",
            "Regression loss: 0.06951949745416641\n",
            "----------------\n",
            "Loss:  1.094443403184414 \n",
            "Classification loss: 2.095818281173706 \n",
            "Regression loss: 0.04653426259756088\n",
            "----------------\n",
            "Loss:  1.2670060507953167 \n",
            "Classification loss: 2.4277920722961426 \n",
            "Regression loss: 0.05311001464724541\n",
            "----------------\n",
            "Loss:  1.1736218333244324 \n",
            "Classification loss: 2.2491095066070557 \n",
            "Regression loss: 0.04906708002090454\n",
            "----------------\n",
            "Loss:  1.216235212981701 \n",
            "Classification loss: 2.2723846435546875 \n",
            "Regression loss: 0.08004289120435715\n",
            "----------------\n",
            "Loss:  1.2524155229330063 \n",
            "Classification loss: 2.3872385025024414 \n",
            "Regression loss: 0.058796271681785583\n",
            "----------------\n",
            "Loss:  1.1780216842889786 \n",
            "Classification loss: 2.2244129180908203 \n",
            "Regression loss: 0.06581522524356842\n",
            "----------------\n",
            "Loss:  1.1514100804924965 \n",
            "Classification loss: 2.148146629333496 \n",
            "Regression loss: 0.07733676582574844\n",
            "----------------\n",
            "Loss:  1.2020802907645702 \n",
            "Classification loss: 2.3227076530456543 \n",
            "Regression loss: 0.04072646424174309\n",
            "----------------\n",
            "Loss:  1.2267534211277962 \n",
            "Classification loss: 2.2824535369873047 \n",
            "Regression loss: 0.08552665263414383\n",
            "----------------\n",
            "Loss:  1.1520840674638748 \n",
            "Classification loss: 2.2513370513916016 \n",
            "Regression loss: 0.026415541768074036\n",
            "----------------\n",
            "Loss:  1.1769152209162712 \n",
            "Classification loss: 2.2551567554473877 \n",
            "Regression loss: 0.04933684319257736\n",
            "----------------\n",
            "Loss:  1.2905076369643211 \n",
            "Classification loss: 2.472627639770508 \n",
            "Regression loss: 0.05419381707906723\n",
            "----------------\n",
            "Loss:  1.2462320402264595 \n",
            "Classification loss: 2.368655204772949 \n",
            "Regression loss: 0.061904437839984894\n",
            "----------------\n",
            "Loss:  1.1565938144922256 \n",
            "Classification loss: 2.2095537185668945 \n",
            "Regression loss: 0.05181695520877838\n",
            "----------------\n",
            "Loss:  1.129140816628933 \n",
            "Classification loss: 2.1727373600006104 \n",
            "Regression loss: 0.04277213662862778\n",
            "----------------\n",
            "Loss:  1.2803288996219635 \n",
            "Classification loss: 2.422555685043335 \n",
            "Regression loss: 0.06905105710029602\n",
            "----------------\n",
            "Loss:  1.2083635404706001 \n",
            "Classification loss: 2.2674171924591064 \n",
            "Regression loss: 0.0746549442410469\n",
            "----------------\n",
            "Loss:  1.1800724491477013 \n",
            "Classification loss: 2.2885985374450684 \n",
            "Regression loss: 0.035773180425167084\n",
            "----------------\n",
            "Loss:  1.1644694171845913 \n",
            "Classification loss: 2.2540736198425293 \n",
            "Regression loss: 0.037432607263326645\n",
            "----------------\n",
            "Loss:  1.142035685479641 \n",
            "Classification loss: 2.1546268463134766 \n",
            "Regression loss: 0.06472226232290268\n",
            "----------------\n",
            "Loss:  1.3030692338943481 \n",
            "Classification loss: 2.453556537628174 \n",
            "Regression loss: 0.07629096508026123\n",
            "----------------\n",
            "Loss:  1.1871438957750797 \n",
            "Classification loss: 2.258286952972412 \n",
            "Regression loss: 0.05800041928887367\n",
            "----------------\n",
            "Loss:  1.1464276984333992 \n",
            "Classification loss: 2.1618971824645996 \n",
            "Regression loss: 0.0654791072010994\n",
            "----------------\n",
            "Loss:  1.1724312528967857 \n",
            "Classification loss: 2.1892971992492676 \n",
            "Regression loss: 0.07778265327215195\n",
            "----------------\n",
            "Loss:  1.2043531760573387 \n",
            "Classification loss: 2.269254207611084 \n",
            "Regression loss: 0.06972607225179672\n",
            "----------------\n",
            "Loss:  1.154585875570774 \n",
            "Classification loss: 2.151487112045288 \n",
            "Regression loss: 0.07884231954813004\n",
            "----------------\n",
            "Loss:  1.2059025466442108 \n",
            "Classification loss: 2.243309497833252 \n",
            "Regression loss: 0.08424779772758484\n",
            "----------------\n",
            "Loss:  1.2414268553256989 \n",
            "Classification loss: 2.3917109966278076 \n",
            "Regression loss: 0.045571357011795044\n",
            "----------------\n",
            "Loss:  1.1934651769697666 \n",
            "Classification loss: 2.283266544342041 \n",
            "Regression loss: 0.05183190479874611\n",
            "----------------\n",
            "Loss:  1.3131579905748367 \n",
            "Classification loss: 2.4685516357421875 \n",
            "Regression loss: 0.07888217270374298\n",
            "----------------\n",
            "Loss:  1.2246813923120499 \n",
            "Classification loss: 2.359463930130005 \n",
            "Regression loss: 0.044949427247047424\n",
            "----------------\n",
            "Loss:  1.1472279131412506 \n",
            "Classification loss: 2.2068588733673096 \n",
            "Regression loss: 0.043798476457595825\n",
            "----------------\n",
            "Loss:  1.1459929943084717 \n",
            "Classification loss: 2.181824207305908 \n",
            "Regression loss: 0.05508089065551758\n",
            "----------------\n",
            "Loss:  1.189876925200224 \n",
            "Classification loss: 2.2871432304382324 \n",
            "Regression loss: 0.04630530998110771\n",
            "----------------\n",
            "Loss:  0.9772247523069382 \n",
            "Classification loss: 1.8794176578521729 \n",
            "Regression loss: 0.037515923380851746\n",
            "----------------\n",
            "Loss:  1.166955754160881 \n",
            "Classification loss: 2.186634063720703 \n",
            "Regression loss: 0.07363872230052948\n",
            "----------------\n",
            "Loss:  1.1957460641860962 \n",
            "Classification loss: 2.2448232173919678 \n",
            "Regression loss: 0.0733344554901123\n",
            "----------------\n",
            "Loss:  1.2058285623788834 \n",
            "Classification loss: 2.2702081203460693 \n",
            "Regression loss: 0.0707245022058487\n",
            "----------------\n",
            "Loss:  1.1643576696515083 \n",
            "Classification loss: 2.2388715744018555 \n",
            "Regression loss: 0.0449218824505806\n",
            "----------------\n",
            "Loss:  1.1672150008380413 \n",
            "Classification loss: 2.228311061859131 \n",
            "Regression loss: 0.053059469908475876\n",
            "----------------\n",
            "Loss:  1.2954196259379387 \n",
            "Classification loss: 2.4065539836883545 \n",
            "Regression loss: 0.09214263409376144\n",
            "----------------\n",
            "Loss:  1.1621981672942638 \n",
            "Classification loss: 2.204422950744629 \n",
            "Regression loss: 0.05998669192194939\n",
            "----------------\n",
            "Loss:  1.23100845515728 \n",
            "Classification loss: 2.3779754638671875 \n",
            "Regression loss: 0.04202072322368622\n",
            "----------------\n",
            "Loss:  1.2577366046607494 \n",
            "Classification loss: 2.398404836654663 \n",
            "Regression loss: 0.05853418633341789\n",
            "----------------\n",
            "Loss:  1.215281032025814 \n",
            "Classification loss: 2.2913756370544434 \n",
            "Regression loss: 0.06959321349859238\n",
            "----------------\n",
            "Loss:  1.1708600670099258 \n",
            "Classification loss: 2.172010898590088 \n",
            "Regression loss: 0.0848546177148819\n",
            "----------------\n",
            "Loss:  1.2233627922832966 \n",
            "Classification loss: 2.3378169536590576 \n",
            "Regression loss: 0.054454315453767776\n",
            "----------------\n",
            "Loss:  1.2146490067243576 \n",
            "Classification loss: 2.2465991973876953 \n",
            "Regression loss: 0.09134940803050995\n",
            "----------------\n",
            "Loss:  1.2821188420057297 \n",
            "Classification loss: 2.4110445976257324 \n",
            "Regression loss: 0.07659654319286346\n",
            "----------------\n",
            "Loss:  1.2420483231544495 \n",
            "Classification loss: 2.3179123401641846 \n",
            "Regression loss: 0.08309215307235718\n",
            "----------------\n",
            "Loss:  1.115726500749588 \n",
            "Classification loss: 2.1056456565856934 \n",
            "Regression loss: 0.06290367245674133\n",
            "----------------\n",
            "Loss:  1.0853817574679852 \n",
            "Classification loss: 2.09922456741333 \n",
            "Regression loss: 0.035769473761320114\n",
            "----------------\n",
            "Loss:  1.2449546158313751 \n",
            "Classification loss: 2.3251142501831055 \n",
            "Regression loss: 0.08239749073982239\n",
            "----------------\n",
            "Loss:  1.0893822349607944 \n",
            "Classification loss: 2.069261074066162 \n",
            "Regression loss: 0.054751697927713394\n",
            "----------------\n",
            "Loss:  1.1179463937878609 \n",
            "Classification loss: 2.1434178352355957 \n",
            "Regression loss: 0.04623747617006302\n",
            "----------------\n",
            "Loss:  1.298614390194416 \n",
            "Classification loss: 2.434488296508789 \n",
            "Regression loss: 0.08137024194002151\n",
            "----------------\n",
            "Loss:  1.1373534724116325 \n",
            "Classification loss: 2.1427388191223145 \n",
            "Regression loss: 0.06598406285047531\n",
            "----------------\n",
            "Loss:  1.1957061886787415 \n",
            "Classification loss: 2.262348175048828 \n",
            "Regression loss: 0.06453210115432739\n",
            "----------------\n",
            "Loss:  1.070035096257925 \n",
            "Classification loss: 2.0489964485168457 \n",
            "Regression loss: 0.04553687199950218\n",
            "----------------\n",
            "Loss:  1.135468952357769 \n",
            "Classification loss: 2.1683802604675293 \n",
            "Regression loss: 0.051278822124004364\n",
            "----------------\n",
            "Loss:  1.1530453898012638 \n",
            "Classification loss: 2.192274808883667 \n",
            "Regression loss: 0.05690798535943031\n",
            "----------------\n",
            "Loss:  1.1947186291217804 \n",
            "Classification loss: 2.247446298599243 \n",
            "Regression loss: 0.07099547982215881\n",
            "----------------\n",
            "Loss:  1.2098888978362083 \n",
            "Classification loss: 2.303715229034424 \n",
            "Regression loss: 0.05803128331899643\n",
            "----------------\n",
            "Loss:  1.1864414885640144 \n",
            "Classification loss: 2.2535417079925537 \n",
            "Regression loss: 0.05967063456773758\n",
            "----------------\n",
            "Loss:  1.1776936911046505 \n",
            "Classification loss: 2.2483057975769043 \n",
            "Regression loss: 0.05354079231619835\n",
            "----------------\n",
            "Loss:  1.21904718875885 \n",
            "Classification loss: 2.306262493133545 \n",
            "Regression loss: 0.06591594219207764\n",
            "----------------\n",
            "Loss:  1.1148517914116383 \n",
            "Classification loss: 2.1310670375823975 \n",
            "Regression loss: 0.04931827262043953\n",
            "----------------\n",
            "Loss:  1.117200531065464 \n",
            "Classification loss: 2.1550824642181396 \n",
            "Regression loss: 0.039659298956394196\n",
            "----------------\n",
            "Loss:  1.1226882860064507 \n",
            "Classification loss: 2.12752628326416 \n",
            "Regression loss: 0.058925144374370575\n",
            "----------------\n",
            "Loss:  1.1557573564350605 \n",
            "Classification loss: 2.2056398391723633 \n",
            "Regression loss: 0.05293743684887886\n",
            "----------------\n",
            "Loss:  1.186605155467987 \n",
            "Classification loss: 2.2169899940490723 \n",
            "Regression loss: 0.07811015844345093\n",
            "----------------\n",
            "Loss:  1.1177506186068058 \n",
            "Classification loss: 2.150261878967285 \n",
            "Regression loss: 0.04261967912316322\n",
            "----------------\n",
            "Loss:  1.2530339807271957 \n",
            "Classification loss: 2.3743834495544434 \n",
            "Regression loss: 0.06584225594997406\n",
            "----------------\n",
            "Loss:  1.2006312161684036 \n",
            "Classification loss: 2.2935078144073486 \n",
            "Regression loss: 0.05387730896472931\n",
            "----------------\n",
            "Loss:  1.2096965312957764 \n",
            "Classification loss: 2.26253080368042 \n",
            "Regression loss: 0.0784311294555664\n",
            "----------------\n",
            "Loss:  1.11078779399395 \n",
            "Classification loss: 2.1427721977233887 \n",
            "Regression loss: 0.039401695132255554\n",
            "----------------\n",
            "Loss:  1.254221186041832 \n",
            "Classification loss: 2.3702516555786133 \n",
            "Regression loss: 0.06909535825252533\n",
            "----------------\n",
            "Loss:  1.1562743484973907 \n",
            "Classification loss: 2.185359001159668 \n",
            "Regression loss: 0.06359484791755676\n",
            "----------------\n",
            "Loss:  0.9973367601633072 \n",
            "Classification loss: 1.9291398525238037 \n",
            "Regression loss: 0.032766833901405334\n",
            "----------------\n",
            "Loss:  1.2466596141457558 \n",
            "Classification loss: 2.3808984756469727 \n",
            "Regression loss: 0.05621037632226944\n",
            "----------------\n",
            "Loss:  1.111620333045721 \n",
            "Classification loss: 2.1316514015197754 \n",
            "Regression loss: 0.04579463228583336\n",
            "----------------\n",
            "Loss:  1.1950680613517761 \n",
            "Classification loss: 2.260270833969116 \n",
            "Regression loss: 0.06493264436721802\n",
            "----------------\n",
            "Loss:  1.0668469741940498 \n",
            "Classification loss: 2.021010160446167 \n",
            "Regression loss: 0.05634189397096634\n",
            "----------------\n",
            "Loss:  1.116729460656643 \n",
            "Classification loss: 2.1256213188171387 \n",
            "Regression loss: 0.05391880124807358\n",
            "----------------\n",
            "Loss:  1.1468426287174225 \n",
            "Classification loss: 2.0708115100860596 \n",
            "Regression loss: 0.1114368736743927\n",
            "----------------\n",
            "Loss:  1.1405007913708687 \n",
            "Classification loss: 2.203580141067505 \n",
            "Regression loss: 0.03871072083711624\n",
            "----------------\n",
            "Loss:  1.21220201253891 \n",
            "Classification loss: 2.318912982940674 \n",
            "Regression loss: 0.052745521068573\n",
            "----------------\n",
            "Loss:  1.229288510978222 \n",
            "Classification loss: 2.344216823577881 \n",
            "Regression loss: 0.057180099189281464\n",
            "----------------\n",
            "Loss:  1.0526791289448738 \n",
            "Classification loss: 2.00573468208313 \n",
            "Regression loss: 0.04981178790330887\n",
            "----------------\n",
            "Loss:  1.1357988342642784 \n",
            "Classification loss: 2.167203187942505 \n",
            "Regression loss: 0.05219724029302597\n",
            "----------------\n",
            "Loss:  1.247285082936287 \n",
            "Classification loss: 2.3926467895507812 \n",
            "Regression loss: 0.0509616881608963\n",
            "----------------\n",
            "Loss:  1.243365041911602 \n",
            "Classification loss: 2.3582892417907715 \n",
            "Regression loss: 0.06422042101621628\n",
            "----------------\n",
            "Loss:  1.123145431280136 \n",
            "Classification loss: 2.175877094268799 \n",
            "Regression loss: 0.035206884145736694\n",
            "----------------\n",
            "Loss:  1.2381507083773613 \n",
            "Classification loss: 2.3758535385131836 \n",
            "Regression loss: 0.0502239391207695\n",
            "----------------\n",
            "Loss:  1.1036941073834896 \n",
            "Classification loss: 2.1217281818389893 \n",
            "Regression loss: 0.04283001646399498\n",
            "----------------\n",
            "Loss:  1.1847597137093544 \n",
            "Classification loss: 2.264941453933716 \n",
            "Regression loss: 0.05228898674249649\n",
            "----------------\n",
            "Loss:  1.2227454856038094 \n",
            "Classification loss: 2.3220877647399902 \n",
            "Regression loss: 0.06170160323381424\n",
            "----------------\n",
            "Loss:  1.222669344395399 \n",
            "Classification loss: 2.3204028606414795 \n",
            "Regression loss: 0.06246791407465935\n",
            "----------------\n",
            "Loss:  1.0588633939623833 \n",
            "Classification loss: 1.9937798976898193 \n",
            "Regression loss: 0.0619734451174736\n",
            "----------------\n",
            "Loss:  1.1892424672842026 \n",
            "Classification loss: 2.2578866481781006 \n",
            "Regression loss: 0.06029914319515228\n",
            "----------------\n",
            "Loss:  1.0928819626569748 \n",
            "Classification loss: 2.0978615283966064 \n",
            "Regression loss: 0.04395119845867157\n",
            "----------------\n",
            "Loss:  1.155656822025776 \n",
            "Classification loss: 2.198451280593872 \n",
            "Regression loss: 0.056431181728839874\n",
            "----------------\n",
            "Loss:  1.0861445888876915 \n",
            "Classification loss: 2.0633859634399414 \n",
            "Regression loss: 0.054451607167720795\n",
            "----------------\n",
            "Loss:  1.1349759623408318 \n",
            "Classification loss: 2.1741230487823486 \n",
            "Regression loss: 0.04791443794965744\n",
            "----------------\n",
            "Loss:  1.202978901565075 \n",
            "Classification loss: 2.274218797683716 \n",
            "Regression loss: 0.06586950272321701\n",
            "----------------\n",
            "Loss:  1.207569606602192 \n",
            "Classification loss: 2.267850875854492 \n",
            "Regression loss: 0.07364416867494583\n",
            "----------------\n",
            "Loss:  1.1772749796509743 \n",
            "Classification loss: 2.213061809539795 \n",
            "Regression loss: 0.07074407488107681\n",
            "----------------\n",
            "Loss:  1.081468153744936 \n",
            "Classification loss: 2.0511436462402344 \n",
            "Regression loss: 0.0558963306248188\n",
            "----------------\n",
            "Loss:  1.2874139249324799 \n",
            "Classification loss: 2.439664363861084 \n",
            "Regression loss: 0.06758174300193787\n",
            "----------------\n",
            "Loss:  1.075621847063303 \n",
            "Classification loss: 2.070450782775879 \n",
            "Regression loss: 0.04039645567536354\n",
            "----------------\n",
            "Loss:  1.2432568222284317 \n",
            "Classification loss: 2.3527607917785645 \n",
            "Regression loss: 0.06687642633914948\n",
            "----------------\n",
            "Loss:  1.2527127340435982 \n",
            "Classification loss: 2.3802385330200195 \n",
            "Regression loss: 0.06259346753358841\n",
            "----------------\n",
            "Loss:  1.2383663579821587 \n",
            "Classification loss: 2.315932273864746 \n",
            "Regression loss: 0.08040022104978561\n",
            "----------------\n",
            "Loss:  1.0969209372997284 \n",
            "Classification loss: 2.0625460147857666 \n",
            "Regression loss: 0.06564792990684509\n",
            "----------------\n",
            "Loss:  1.1216194331645966 \n",
            "Classification loss: 2.098383903503418 \n",
            "Regression loss: 0.07242748141288757\n",
            "----------------\n",
            "Loss:  1.288118600845337 \n",
            "Classification loss: 2.4506702423095703 \n",
            "Regression loss: 0.06278347969055176\n",
            "----------------\n",
            "Loss:  1.085333950817585 \n",
            "Classification loss: 2.0899462699890137 \n",
            "Regression loss: 0.040360815823078156\n",
            "----------------\n",
            "Loss:  1.2213708125054836 \n",
            "Classification loss: 2.3471102714538574 \n",
            "Regression loss: 0.047815676778554916\n",
            "----------------\n",
            "Loss:  1.1858638226985931 \n",
            "Classification loss: 2.1783955097198486 \n",
            "Regression loss: 0.09666606783866882\n",
            "----------------\n",
            "Loss:  1.0736037120223045 \n",
            "Classification loss: 2.02413272857666 \n",
            "Regression loss: 0.06153734773397446\n",
            "----------------\n",
            "Loss:  1.2628851383924484 \n",
            "Classification loss: 2.373910427093506 \n",
            "Regression loss: 0.0759299248456955\n",
            "----------------\n",
            "Loss:  1.158925835043192 \n",
            "Classification loss: 2.194411277770996 \n",
            "Regression loss: 0.06172019615769386\n",
            "----------------\n",
            "Loss:  1.0327187553048134 \n",
            "Classification loss: 2.0141842365264893 \n",
            "Regression loss: 0.025626637041568756\n",
            "----------------\n",
            "Loss:  1.1064626350998878 \n",
            "Classification loss: 2.0974202156066895 \n",
            "Regression loss: 0.05775252729654312\n",
            "----------------\n",
            "Loss:  1.143479272723198 \n",
            "Classification loss: 2.072627067565918 \n",
            "Regression loss: 0.10716573894023895\n",
            "----------------\n",
            "Loss:  1.2712546065449715 \n",
            "Classification loss: 2.4088783264160156 \n",
            "Regression loss: 0.06681544333696365\n",
            "----------------\n",
            "Loss:  1.2373139783740044 \n",
            "Classification loss: 2.3760581016540527 \n",
            "Regression loss: 0.049284927546978\n",
            "----------------\n",
            "Loss:  1.1917382515966892 \n",
            "Classification loss: 2.276296377182007 \n",
            "Regression loss: 0.053590063005685806\n",
            "----------------\n",
            "Loss:  1.1250196695327759 \n",
            "Classification loss: 2.108729839324951 \n",
            "Regression loss: 0.07065474987030029\n",
            "----------------\n",
            "Loss:  1.1117910407483578 \n",
            "Classification loss: 2.111239194869995 \n",
            "Regression loss: 0.056171443313360214\n",
            "----------------\n",
            "Loss:  1.1198345944285393 \n",
            "Classification loss: 2.151517629623413 \n",
            "Regression loss: 0.04407577961683273\n",
            "----------------\n",
            "Loss:  1.1920892596244812 \n",
            "Classification loss: 2.237349033355713 \n",
            "Regression loss: 0.07341474294662476\n",
            "----------------\n",
            "Loss:  1.0785656981170177 \n",
            "Classification loss: 2.0381948947906494 \n",
            "Regression loss: 0.05946825072169304\n",
            "----------------\n",
            "Loss:  1.1351639926433563 \n",
            "Classification loss: 2.136970043182373 \n",
            "Regression loss: 0.0666789710521698\n",
            "----------------\n",
            "Loss:  1.119910929352045 \n",
            "Classification loss: 2.146801471710205 \n",
            "Regression loss: 0.04651019349694252\n",
            "----------------\n",
            "Loss:  1.1810659058392048 \n",
            "Classification loss: 2.2652511596679688 \n",
            "Regression loss: 0.04844032600522041\n",
            "----------------\n",
            "Loss:  1.146445594727993 \n",
            "Classification loss: 2.157379150390625 \n",
            "Regression loss: 0.06775601953268051\n",
            "----------------\n",
            "Loss:  1.2368333041667938 \n",
            "Classification loss: 2.335082530975342 \n",
            "Regression loss: 0.06929203867912292\n",
            "----------------\n",
            "Loss:  1.060109704732895 \n",
            "Classification loss: 1.9781336784362793 \n",
            "Regression loss: 0.07104286551475525\n",
            "----------------\n",
            "Loss:  1.0703821070492268 \n",
            "Classification loss: 2.088200807571411 \n",
            "Regression loss: 0.026281703263521194\n",
            "----------------\n",
            "Loss:  1.2443174570798874 \n",
            "Classification loss: 2.34403657913208 \n",
            "Regression loss: 0.07229916751384735\n",
            "----------------\n",
            "Loss:  1.0173878371715546 \n",
            "Classification loss: 1.9331134557724 \n",
            "Regression loss: 0.050831109285354614\n",
            "----------------\n",
            "Loss:  1.1285624578595161 \n",
            "Classification loss: 2.1244168281555176 \n",
            "Regression loss: 0.06635404378175735\n",
            "----------------\n",
            "Loss:  1.0324580371379852 \n",
            "Classification loss: 1.9809120893478394 \n",
            "Regression loss: 0.04200199246406555\n",
            "----------------\n",
            "Loss:  1.1275422498583794 \n",
            "Classification loss: 2.1003365516662598 \n",
            "Regression loss: 0.07737397402524948\n",
            "----------------\n",
            "Loss:  1.1094037592411041 \n",
            "Classification loss: 2.089066982269287 \n",
            "Regression loss: 0.06487026810646057\n",
            "----------------\n",
            "Loss:  1.1090791188180447 \n",
            "Classification loss: 2.111510753631592 \n",
            "Regression loss: 0.053323742002248764\n",
            "----------------\n",
            "Loss:  1.1241820827126503 \n",
            "Classification loss: 2.0666613578796387 \n",
            "Regression loss: 0.09085140377283096\n",
            "----------------\n",
            "Loss:  1.2190012112259865 \n",
            "Classification loss: 2.322981357574463 \n",
            "Regression loss: 0.057510532438755035\n",
            "----------------\n",
            "Loss:  1.0937123745679855 \n",
            "Classification loss: 2.0873589515686035 \n",
            "Regression loss: 0.05003289878368378\n",
            "----------------\n",
            "Loss:  1.113213263452053 \n",
            "Classification loss: 2.1145060062408447 \n",
            "Regression loss: 0.05596026033163071\n",
            "----------------\n",
            "Loss:  1.2768919169902802 \n",
            "Classification loss: 2.3765382766723633 \n",
            "Regression loss: 0.08862277865409851\n",
            "----------------\n",
            "Loss:  1.1139465123414993 \n",
            "Classification loss: 2.125835418701172 \n",
            "Regression loss: 0.05102880299091339\n",
            "----------------\n",
            "Loss:  1.085593357682228 \n",
            "Classification loss: 2.0484459400177 \n",
            "Regression loss: 0.06137038767337799\n",
            "----------------\n",
            "Loss:  1.0830868408083916 \n",
            "Classification loss: 2.0308101177215576 \n",
            "Regression loss: 0.06768178194761276\n",
            "----------------\n",
            "Loss:  1.124212060123682 \n",
            "Classification loss: 2.1904242038726807 \n",
            "Regression loss: 0.02899995818734169\n",
            "----------------\n",
            "Loss:  1.220624953508377 \n",
            "Classification loss: 2.2637293338775635 \n",
            "Regression loss: 0.08876028656959534\n",
            "----------------\n",
            "Loss:  1.1643081381917 \n",
            "Classification loss: 2.1783018112182617 \n",
            "Regression loss: 0.07515723258256912\n",
            "----------------\n",
            "Loss:  1.1617716029286385 \n",
            "Classification loss: 2.1944594383239746 \n",
            "Regression loss: 0.06454188376665115\n",
            "----------------\n",
            "Loss:  1.1486148238182068 \n",
            "Classification loss: 2.1569337844848633 \n",
            "Regression loss: 0.07014793157577515\n",
            "----------------\n",
            "Loss:  1.1431898474693298 \n",
            "Classification loss: 2.1509108543395996 \n",
            "Regression loss: 0.06773442029953003\n",
            "----------------\n",
            "Loss:  1.2490718625485897 \n",
            "Classification loss: 2.3936171531677246 \n",
            "Regression loss: 0.0522632859647274\n",
            "----------------\n",
            "Loss:  1.1842588409781456 \n",
            "Classification loss: 2.2092864513397217 \n",
            "Regression loss: 0.07961561530828476\n",
            "----------------\n",
            "Loss:  1.1151505410671234 \n",
            "Classification loss: 2.0737485885620117 \n",
            "Regression loss: 0.07827624678611755\n",
            "----------------\n",
            "Loss:  1.0109397433698177 \n",
            "Classification loss: 1.9491826295852661 \n",
            "Regression loss: 0.03634842857718468\n",
            "----------------\n",
            "Loss:  1.076193816959858 \n",
            "Classification loss: 2.0679879188537598 \n",
            "Regression loss: 0.04219985753297806\n",
            "----------------\n",
            "Loss:  1.1179733872413635 \n",
            "Classification loss: 2.0987861156463623 \n",
            "Regression loss: 0.06858032941818237\n",
            "----------------\n",
            "Loss:  1.0645136274397373 \n",
            "Classification loss: 2.0214734077453613 \n",
            "Regression loss: 0.053776923567056656\n",
            "----------------\n",
            "Loss:  1.1291001588106155 \n",
            "Classification loss: 2.1967406272888184 \n",
            "Regression loss: 0.03072984516620636\n",
            "----------------\n",
            "Loss:  1.2363190650939941 \n",
            "Classification loss: 2.2945568561553955 \n",
            "Regression loss: 0.08904063701629639\n",
            "----------------\n",
            "Loss:  1.0596572235226631 \n",
            "Classification loss: 2.018146514892578 \n",
            "Regression loss: 0.050583966076374054\n",
            "----------------\n",
            "Loss:  1.1191128939390182 \n",
            "Classification loss: 2.079655408859253 \n",
            "Regression loss: 0.07928518950939178\n",
            "----------------\n",
            "Loss:  1.1174251660704613 \n",
            "Classification loss: 2.105062961578369 \n",
            "Regression loss: 0.0648936852812767\n",
            "----------------\n",
            "Loss:  1.1035157181322575 \n",
            "Classification loss: 2.1262011528015137 \n",
            "Regression loss: 0.040415141731500626\n",
            "----------------\n",
            "Loss:  1.0282463803887367 \n",
            "Classification loss: 1.9811949729919434 \n",
            "Regression loss: 0.037648893892765045\n",
            "----------------\n",
            "Loss:  1.1187267452478409 \n",
            "Classification loss: 2.0789060592651367 \n",
            "Regression loss: 0.07927371561527252\n",
            "----------------\n",
            "Loss:  1.039537724107504 \n",
            "Classification loss: 1.9818565845489502 \n",
            "Regression loss: 0.04860943183302879\n",
            "----------------\n",
            "Loss:  1.249119982123375 \n",
            "Classification loss: 2.3386690616607666 \n",
            "Regression loss: 0.07978545129299164\n",
            "----------------\n",
            "Loss:  1.1726117953658104 \n",
            "Classification loss: 2.1998281478881836 \n",
            "Regression loss: 0.0726977214217186\n",
            "----------------\n",
            "Loss:  1.1035969443619251 \n",
            "Classification loss: 2.1402132511138916 \n",
            "Regression loss: 0.033490318804979324\n",
            "----------------\n",
            "Loss:  1.1413620114326477 \n",
            "Classification loss: 2.188936948776245 \n",
            "Regression loss: 0.046893537044525146\n",
            "----------------\n",
            "Loss:  1.078893281519413 \n",
            "Classification loss: 2.065793037414551 \n",
            "Regression loss: 0.045996762812137604\n",
            "----------------\n",
            "Loss:  1.155466366559267 \n",
            "Classification loss: 2.1938815116882324 \n",
            "Regression loss: 0.05852561071515083\n",
            "----------------\n",
            "Loss:  1.0766798108816147 \n",
            "Classification loss: 2.0565381050109863 \n",
            "Regression loss: 0.04841075837612152\n",
            "----------------\n",
            "Loss:  1.1213418841362 \n",
            "Classification loss: 2.1120223999023438 \n",
            "Regression loss: 0.06533068418502808\n",
            "----------------\n",
            "Loss:  1.149436429142952 \n",
            "Classification loss: 2.1248886585235596 \n",
            "Regression loss: 0.08699209988117218\n",
            "----------------\n",
            "Loss:  0.9890020787715912 \n",
            "Classification loss: 1.8980478048324585 \n",
            "Regression loss: 0.03997817635536194\n",
            "----------------\n",
            "Loss:  1.1169775500893593 \n",
            "Classification loss: 2.1476731300354004 \n",
            "Regression loss: 0.04314098507165909\n",
            "----------------\n",
            "Loss:  1.1049614399671555 \n",
            "Classification loss: 2.0901970863342285 \n",
            "Regression loss: 0.0598628968000412\n",
            "----------------\n",
            "Loss:  1.077281940728426 \n",
            "Classification loss: 2.068726062774658 \n",
            "Regression loss: 0.04291890934109688\n",
            "----------------\n",
            "Loss:  1.1014565117657185 \n",
            "Classification loss: 2.0789341926574707 \n",
            "Regression loss: 0.06198941543698311\n",
            "----------------\n",
            "Loss:  1.0185003727674484 \n",
            "Classification loss: 1.9016263484954834 \n",
            "Regression loss: 0.06768719851970673\n",
            "----------------\n",
            "Loss:  1.0631157159805298 \n",
            "Classification loss: 1.974721074104309 \n",
            "Regression loss: 0.07575517892837524\n",
            "----------------\n",
            "Loss:  1.1121124178171158 \n",
            "Classification loss: 2.1036736965179443 \n",
            "Regression loss: 0.060275569558143616\n",
            "----------------\n",
            "Loss:  1.04823350161314 \n",
            "Classification loss: 1.970524787902832 \n",
            "Regression loss: 0.06297110766172409\n",
            "----------------\n",
            "Loss:  1.138791672885418 \n",
            "Classification loss: 2.156318187713623 \n",
            "Regression loss: 0.060632579028606415\n",
            "----------------\n",
            "Loss:  1.1013588272035122 \n",
            "Classification loss: 2.099010467529297 \n",
            "Regression loss: 0.051853593438863754\n",
            "----------------\n",
            "Loss:  1.01666709035635 \n",
            "Classification loss: 1.9444409608840942 \n",
            "Regression loss: 0.044446609914302826\n",
            "----------------\n",
            "Loss:  1.082301590591669 \n",
            "Classification loss: 2.0605995655059814 \n",
            "Regression loss: 0.05200180783867836\n",
            "----------------\n",
            "Loss:  1.1312831342220306 \n",
            "Classification loss: 2.124786376953125 \n",
            "Regression loss: 0.06888994574546814\n",
            "----------------\n",
            "Loss:  1.1461834758520126 \n",
            "Classification loss: 2.173682689666748 \n",
            "Regression loss: 0.05934213101863861\n",
            "----------------\n",
            "Loss:  1.170191913843155 \n",
            "Classification loss: 2.2160825729370117 \n",
            "Regression loss: 0.06215062737464905\n",
            "----------------\n",
            "Loss:  1.0331977233290672 \n",
            "Classification loss: 1.9572837352752686 \n",
            "Regression loss: 0.05455585569143295\n",
            "----------------\n",
            "Loss:  1.2809266448020935 \n",
            "Classification loss: 2.428410053253174 \n",
            "Regression loss: 0.06672161817550659\n",
            "----------------\n",
            "Loss:  1.1710141003131866 \n",
            "Classification loss: 2.2392172813415527 \n",
            "Regression loss: 0.05140545964241028\n",
            "----------------\n",
            "Loss:  1.1740350052714348 \n",
            "Classification loss: 2.213880777359009 \n",
            "Regression loss: 0.06709461659193039\n",
            "----------------\n",
            "Loss:  1.151021882891655 \n",
            "Classification loss: 2.109839916229248 \n",
            "Regression loss: 0.09610192477703094\n",
            "----------------\n",
            "Loss:  1.1301865316927433 \n",
            "Classification loss: 2.139918804168701 \n",
            "Regression loss: 0.060227129608392715\n",
            "----------------\n",
            "Loss:  0.9892990365624428 \n",
            "Classification loss: 1.8730525970458984 \n",
            "Regression loss: 0.05277273803949356\n",
            "----------------\n",
            "Loss:  1.2256339527666569 \n",
            "Classification loss: 2.3338422775268555 \n",
            "Regression loss: 0.05871281400322914\n",
            "----------------\n",
            "Loss:  1.0796044915914536 \n",
            "Classification loss: 1.9940109252929688 \n",
            "Regression loss: 0.08259902894496918\n",
            "----------------\n",
            "Loss:  1.3049414977431297 \n",
            "Classification loss: 2.3814380168914795 \n",
            "Regression loss: 0.11422248929738998\n",
            "----------------\n",
            "Loss:  1.088940568268299 \n",
            "Classification loss: 2.084425926208496 \n",
            "Regression loss: 0.046727605164051056\n",
            "----------------\n",
            "Loss:  1.153610721230507 \n",
            "Classification loss: 2.168792724609375 \n",
            "Regression loss: 0.0692143589258194\n",
            "----------------\n",
            "Loss:  1.246098279953003 \n",
            "Classification loss: 2.356027364730835 \n",
            "Regression loss: 0.06808459758758545\n",
            "----------------\n",
            "Loss:  1.0807669684290886 \n",
            "Classification loss: 2.050609588623047 \n",
            "Regression loss: 0.055462174117565155\n",
            "----------------\n",
            "Loss:  1.0978199541568756 \n",
            "Classification loss: 2.119274616241455 \n",
            "Regression loss: 0.03818264603614807\n",
            "----------------\n",
            "Loss:  1.1465305462479591 \n",
            "Classification loss: 2.1849050521850586 \n",
            "Regression loss: 0.05407802015542984\n",
            "----------------\n",
            "Loss:  1.056661732494831 \n",
            "Classification loss: 2.0269346237182617 \n",
            "Regression loss: 0.043194420635700226\n",
            "----------------\n",
            "Loss:  1.0959860794246197 \n",
            "Classification loss: 2.0896100997924805 \n",
            "Regression loss: 0.05118102952837944\n",
            "----------------\n",
            "Loss:  1.1490864008665085 \n",
            "Classification loss: 2.1788558959960938 \n",
            "Regression loss: 0.05965845286846161\n",
            "----------------\n",
            "Loss:  1.104057390242815 \n",
            "Classification loss: 2.1004066467285156 \n",
            "Regression loss: 0.053854066878557205\n",
            "----------------\n",
            "Loss:  1.191214680671692 \n",
            "Classification loss: 2.2543020248413086 \n",
            "Regression loss: 0.0640636682510376\n",
            "----------------\n",
            "Loss:  1.1504244804382324 \n",
            "Classification loss: 2.181864023208618 \n",
            "Regression loss: 0.05949246883392334\n",
            "----------------\n",
            "Loss:  1.2101069502532482 \n",
            "Classification loss: 2.300713062286377 \n",
            "Regression loss: 0.05975041911005974\n",
            "----------------\n",
            "Loss:  1.1950839012861252 \n",
            "Classification loss: 2.2634057998657227 \n",
            "Regression loss: 0.06338100135326385\n",
            "----------------\n",
            "Loss:  1.086218573153019 \n",
            "Classification loss: 2.0703418254852295 \n",
            "Regression loss: 0.051047660410404205\n",
            "----------------\n",
            "Loss:  1.025606658309698 \n",
            "Classification loss: 1.9793689250946045 \n",
            "Regression loss: 0.03592219576239586\n",
            "----------------\n",
            "Loss:  1.1062076315283775 \n",
            "Classification loss: 2.0760726928710938 \n",
            "Regression loss: 0.06817128509283066\n",
            "----------------\n",
            "Loss:  1.0532427672296762 \n",
            "Classification loss: 2.0533294677734375 \n",
            "Regression loss: 0.026578033342957497\n",
            "----------------\n",
            "Loss:  1.0538572743535042 \n",
            "Classification loss: 1.994767427444458 \n",
            "Regression loss: 0.05647356063127518\n",
            "----------------\n",
            "Loss:  1.0932946801185608 \n",
            "Classification loss: 2.0529403686523438 \n",
            "Regression loss: 0.06682449579238892\n",
            "----------------\n",
            "Loss:  1.131525494158268 \n",
            "Classification loss: 2.156360626220703 \n",
            "Regression loss: 0.05334518104791641\n",
            "----------------\n",
            "Loss:  1.0448841229081154 \n",
            "Classification loss: 1.9340767860412598 \n",
            "Regression loss: 0.0778457298874855\n",
            "----------------\n",
            "Loss:  1.1281610280275345 \n",
            "Classification loss: 2.1186203956604004 \n",
            "Regression loss: 0.06885083019733429\n",
            "----------------\n",
            "Loss:  1.0994513630867004 \n",
            "Classification loss: 2.0741868019104004 \n",
            "Regression loss: 0.062357962131500244\n",
            "----------------\n",
            "Loss:  1.0792309641838074 \n",
            "Classification loss: 2.0292370319366455 \n",
            "Regression loss: 0.06461244821548462\n",
            "----------------\n",
            "Loss:  1.1031705290079117 \n",
            "Classification loss: 2.120356321334839 \n",
            "Regression loss: 0.04299236834049225\n",
            "----------------\n",
            "Loss:  1.1102946251630783 \n",
            "Classification loss: 2.053967237472534 \n",
            "Regression loss: 0.08331100642681122\n",
            "----------------\n",
            "Loss:  1.0889751091599464 \n",
            "Classification loss: 2.031717300415039 \n",
            "Regression loss: 0.07311645895242691\n",
            "----------------\n",
            "Loss:  1.1603731103241444 \n",
            "Classification loss: 2.235220193862915 \n",
            "Regression loss: 0.042763013392686844\n",
            "----------------\n",
            "Loss:  1.0056099779903889 \n",
            "Classification loss: 1.9429450035095215 \n",
            "Regression loss: 0.03413747623562813\n",
            "----------------\n",
            "Loss:  1.1362486854195595 \n",
            "Classification loss: 2.1881442070007324 \n",
            "Regression loss: 0.04217658191919327\n",
            "----------------\n",
            "Loss:  0.9986618235707283 \n",
            "Classification loss: 1.8848668336868286 \n",
            "Regression loss: 0.056228406727313995\n",
            "----------------\n",
            "Loss:  1.1253191903233528 \n",
            "Classification loss: 2.157224416732788 \n",
            "Regression loss: 0.04670698195695877\n",
            "----------------\n",
            "Loss:  0.9937036111950874 \n",
            "Classification loss: 1.884006381034851 \n",
            "Regression loss: 0.051700420677661896\n",
            "----------------\n",
            "Loss:  1.1724356152117252 \n",
            "Classification loss: 2.246645927429199 \n",
            "Regression loss: 0.049112651497125626\n",
            "----------------\n",
            "Loss:  1.1518745869398117 \n",
            "Classification loss: 2.209735870361328 \n",
            "Regression loss: 0.047006651759147644\n",
            "----------------\n",
            "Loss:  1.187383383512497 \n",
            "Classification loss: 2.2337517738342285 \n",
            "Regression loss: 0.07050749659538269\n",
            "----------------\n",
            "Loss:  1.1355020143091679 \n",
            "Classification loss: 2.159165620803833 \n",
            "Regression loss: 0.05591920390725136\n",
            "----------------\n",
            "Loss:  1.1081898920238018 \n",
            "Classification loss: 2.0954127311706543 \n",
            "Regression loss: 0.060483526438474655\n",
            "----------------\n",
            "Loss:  1.09354580193758 \n",
            "Classification loss: 2.072453022003174 \n",
            "Regression loss: 0.057319290935993195\n",
            "----------------\n",
            "Loss:  1.0045179575681686 \n",
            "Classification loss: 1.8883652687072754 \n",
            "Regression loss: 0.060335323214530945\n",
            "----------------\n",
            "Loss:  1.2078123539686203 \n",
            "Classification loss: 2.2438201904296875 \n",
            "Regression loss: 0.08590225875377655\n",
            "----------------\n",
            "Loss:  1.03169085085392 \n",
            "Classification loss: 1.9246652126312256 \n",
            "Regression loss: 0.06935824453830719\n",
            "----------------\n",
            "Loss:  1.1521730795502663 \n",
            "Classification loss: 2.186880588531494 \n",
            "Regression loss: 0.058732785284519196\n",
            "----------------\n",
            "Loss:  1.1769920662045479 \n",
            "Classification loss: 2.2200927734375 \n",
            "Regression loss: 0.06694567948579788\n",
            "----------------\n",
            "Loss:  1.0838736928999424 \n",
            "Classification loss: 2.05719256401062 \n",
            "Regression loss: 0.05527741089463234\n",
            "----------------\n",
            "Loss:  1.0353116504848003 \n",
            "Classification loss: 1.9928159713745117 \n",
            "Regression loss: 0.03890366479754448\n",
            "----------------\n",
            "Loss:  1.1465158686041832 \n",
            "Classification loss: 2.1631429195404053 \n",
            "Regression loss: 0.06494440883398056\n",
            "----------------\n",
            "Loss:  1.1402475535869598 \n",
            "Classification loss: 2.1456098556518555 \n",
            "Regression loss: 0.0674426257610321\n",
            "----------------\n",
            "Loss:  1.1329678446054459 \n",
            "Classification loss: 2.170448064804077 \n",
            "Regression loss: 0.04774381220340729\n",
            "----------------\n",
            "Loss:  1.1791308894753456 \n",
            "Classification loss: 2.226898431777954 \n",
            "Regression loss: 0.06568167358636856\n",
            "----------------\n",
            "Loss:  1.077886439859867 \n",
            "Classification loss: 2.0532546043395996 \n",
            "Regression loss: 0.05125913769006729\n",
            "----------------\n",
            "Loss:  1.039474569261074 \n",
            "Classification loss: 1.9828523397445679 \n",
            "Regression loss: 0.04804839938879013\n",
            "----------------\n",
            "Loss:  0.9786947555840015 \n",
            "Classification loss: 1.868748426437378 \n",
            "Regression loss: 0.044320542365312576\n",
            "----------------\n",
            "Loss:  1.1634718403220177 \n",
            "Classification loss: 2.199625015258789 \n",
            "Regression loss: 0.06365933269262314\n",
            "----------------\n",
            "Loss:  1.142710529267788 \n",
            "Classification loss: 2.1415767669677734 \n",
            "Regression loss: 0.07192214578390121\n",
            "----------------\n",
            "Loss:  1.064221516251564 \n",
            "Classification loss: 1.9808173179626465 \n",
            "Regression loss: 0.07381285727024078\n",
            "----------------\n",
            "Loss:  1.1322774775326252 \n",
            "Classification loss: 2.149186611175537 \n",
            "Regression loss: 0.057684171944856644\n",
            "----------------\n",
            "Loss:  1.087338112294674 \n",
            "Classification loss: 2.0822479724884033 \n",
            "Regression loss: 0.04621412605047226\n",
            "----------------\n",
            "Loss:  0.9578711986541748 \n",
            "Classification loss: 1.787292242050171 \n",
            "Regression loss: 0.06422507762908936\n",
            "----------------\n",
            "Loss:  1.2502656057476997 \n",
            "Classification loss: 2.3620119094848633 \n",
            "Regression loss: 0.0692596510052681\n",
            "----------------\n",
            "Loss:  1.2144509218633175 \n",
            "Classification loss: 2.324524402618408 \n",
            "Regression loss: 0.05218872055411339\n",
            "----------------\n",
            "Loss:  1.0363177508115768 \n",
            "Classification loss: 1.976181983947754 \n",
            "Regression loss: 0.04822675883769989\n",
            "----------------\n",
            "Loss:  1.019550807774067 \n",
            "Classification loss: 1.9170053005218506 \n",
            "Regression loss: 0.06104815751314163\n",
            "----------------\n",
            "Loss:  1.131652608513832 \n",
            "Classification loss: 2.102078914642334 \n",
            "Regression loss: 0.0806131511926651\n",
            "----------------\n",
            "Loss:  1.1468477956950665 \n",
            "Classification loss: 2.199434280395508 \n",
            "Regression loss: 0.047130655497312546\n",
            "----------------\n",
            "Loss:  1.1600341871380806 \n",
            "Classification loss: 2.1983039379119873 \n",
            "Regression loss: 0.060882218182086945\n",
            "----------------\n",
            "Loss:  1.1777055189013481 \n",
            "Classification loss: 2.190662384033203 \n",
            "Regression loss: 0.08237432688474655\n",
            "----------------\n",
            "Loss:  1.102743685245514 \n",
            "Classification loss: 2.125375270843506 \n",
            "Regression loss: 0.040056049823760986\n",
            "----------------\n",
            "Loss:  1.1240022033452988 \n",
            "Classification loss: 2.1300246715545654 \n",
            "Regression loss: 0.05898986756801605\n",
            "----------------\n",
            "Loss:  1.0318400040268898 \n",
            "Classification loss: 2.003995895385742 \n",
            "Regression loss: 0.029842056334018707\n",
            "----------------\n",
            "Loss:  1.0574604868888855 \n",
            "Classification loss: 2.007657051086426 \n",
            "Regression loss: 0.05363196134567261\n",
            "----------------\n",
            "Loss:  0.9863734282553196 \n",
            "Classification loss: 1.8633798360824585 \n",
            "Regression loss: 0.05468351021409035\n",
            "----------------\n",
            "Loss:  1.0486762300133705 \n",
            "Classification loss: 2.022909164428711 \n",
            "Regression loss: 0.037221647799015045\n",
            "----------------\n",
            "Loss:  1.0401550270617008 \n",
            "Classification loss: 1.9983381032943726 \n",
            "Regression loss: 0.04098597541451454\n",
            "----------------\n",
            "Loss:  1.1245109140872955 \n",
            "Classification loss: 2.0929417610168457 \n",
            "Regression loss: 0.07804003357887268\n",
            "----------------\n",
            "Loss:  1.1128071174025536 \n",
            "Classification loss: 2.095930576324463 \n",
            "Regression loss: 0.06484182924032211\n",
            "----------------\n",
            "Loss:  1.050288088619709 \n",
            "Classification loss: 1.9941372871398926 \n",
            "Regression loss: 0.053219445049762726\n",
            "----------------\n",
            "Loss:  1.1024874821305275 \n",
            "Classification loss: 2.0581412315368652 \n",
            "Regression loss: 0.07341686636209488\n",
            "----------------\n",
            "Loss:  1.035700462758541 \n",
            "Classification loss: 1.9765052795410156 \n",
            "Regression loss: 0.047447822988033295\n",
            "----------------\n",
            "Loss:  1.0642346665263176 \n",
            "Classification loss: 2.0468668937683105 \n",
            "Regression loss: 0.04080121964216232\n",
            "----------------\n",
            "Loss:  1.00864939391613 \n",
            "Classification loss: 1.9152827262878418 \n",
            "Regression loss: 0.05100803077220917\n",
            "----------------\n",
            "Loss:  0.974649079144001 \n",
            "Classification loss: 1.8200263977050781 \n",
            "Regression loss: 0.06463588029146194\n",
            "----------------\n",
            "Loss:  1.0834156349301338 \n",
            "Classification loss: 2.0868334770202637 \n",
            "Regression loss: 0.039998896420001984\n",
            "----------------\n",
            "Loss:  1.0500527918338776 \n",
            "Classification loss: 2.0208001136779785 \n",
            "Regression loss: 0.039652734994888306\n",
            "----------------\n",
            "Loss:  1.1615833416581154 \n",
            "Classification loss: 2.177870988845825 \n",
            "Regression loss: 0.07264784723520279\n",
            "----------------\n",
            "Loss:  1.0782102793455124 \n",
            "Classification loss: 2.0227863788604736 \n",
            "Regression loss: 0.06681708991527557\n",
            "----------------\n",
            "Loss:  1.1457803696393967 \n",
            "Classification loss: 2.1645188331604004 \n",
            "Regression loss: 0.06352095305919647\n",
            "----------------\n",
            "Loss:  1.1094854436814785 \n",
            "Classification loss: 2.1152141094207764 \n",
            "Regression loss: 0.05187838897109032\n",
            "----------------\n",
            "Loss:  1.1726367101073265 \n",
            "Classification loss: 2.2085602283477783 \n",
            "Regression loss: 0.06835659593343735\n",
            "----------------\n",
            "Loss:  1.0828273743391037 \n",
            "Classification loss: 2.0535941123962402 \n",
            "Regression loss: 0.05603031814098358\n",
            "----------------\n",
            "Loss:  1.1938936710357666 \n",
            "Classification loss: 2.2456421852111816 \n",
            "Regression loss: 0.07107257843017578\n",
            "----------------\n",
            "Loss:  1.1134351938962936 \n",
            "Classification loss: 2.1463444232940674 \n",
            "Regression loss: 0.04026298224925995\n",
            "----------------\n",
            "Loss:  1.0142286494374275 \n",
            "Classification loss: 1.9402185678482056 \n",
            "Regression loss: 0.04411936551332474\n",
            "----------------\n",
            "Loss:  1.0717863216996193 \n",
            "Classification loss: 2.007725715637207 \n",
            "Regression loss: 0.06792346388101578\n",
            "----------------\n",
            "Loss:  1.0451870411634445 \n",
            "Classification loss: 1.9999761581420898 \n",
            "Regression loss: 0.0451989620923996\n",
            "----------------\n",
            "Loss:  1.0974358543753624 \n",
            "Classification loss: 2.116921901702881 \n",
            "Regression loss: 0.03897490352392197\n",
            "----------------\n",
            "Loss:  1.0622163638472557 \n",
            "Classification loss: 1.9757168292999268 \n",
            "Regression loss: 0.07435794919729233\n",
            "----------------\n",
            "Loss:  0.9959972985088825 \n",
            "Classification loss: 1.8970344066619873 \n",
            "Regression loss: 0.04748009517788887\n",
            "----------------\n",
            "Loss:  1.002462014555931 \n",
            "Classification loss: 1.9053157567977905 \n",
            "Regression loss: 0.04980413615703583\n",
            "----------------\n",
            "Loss:  1.0785134509205818 \n",
            "Classification loss: 2.0625555515289307 \n",
            "Regression loss: 0.047235675156116486\n",
            "----------------\n",
            "Loss:  1.1779681891202927 \n",
            "Classification loss: 2.1857452392578125 \n",
            "Regression loss: 0.08509556949138641\n",
            "----------------\n",
            "Loss:  1.0704023502767086 \n",
            "Classification loss: 2.0477638244628906 \n",
            "Regression loss: 0.04652043804526329\n",
            "----------------\n",
            "Loss:  1.1111670099198818 \n",
            "Classification loss: 2.1001791954040527 \n",
            "Regression loss: 0.061077412217855453\n",
            "----------------\n",
            "Loss:  1.0862769782543182 \n",
            "Classification loss: 2.0756678581237793 \n",
            "Regression loss: 0.04844304919242859\n",
            "----------------\n",
            "Loss:  1.0336618423461914 \n",
            "Classification loss: 1.982816219329834 \n",
            "Regression loss: 0.042253732681274414\n",
            "----------------\n",
            "Loss:  1.06950144469738 \n",
            "Classification loss: 2.023031234741211 \n",
            "Regression loss: 0.0579858273267746\n",
            "----------------\n",
            "Loss:  1.1387297287583351 \n",
            "Classification loss: 2.182018280029297 \n",
            "Regression loss: 0.047720588743686676\n",
            "----------------\n",
            "Loss:  1.0665107183158398 \n",
            "Classification loss: 2.053194999694824 \n",
            "Regression loss: 0.03991321846842766\n",
            "----------------\n",
            "Loss:  0.9973118975758553 \n",
            "Classification loss: 1.9026410579681396 \n",
            "Regression loss: 0.04599136859178543\n",
            "----------------\n",
            "Loss:  1.0296497028321028 \n",
            "Classification loss: 2.0154385566711426 \n",
            "Regression loss: 0.021930424496531487\n",
            "----------------\n",
            "Loss:  1.1396955884993076 \n",
            "Classification loss: 2.2015390396118164 \n",
            "Regression loss: 0.03892606869339943\n",
            "----------------\n",
            "Loss:  1.059879370033741 \n",
            "Classification loss: 1.979295253753662 \n",
            "Regression loss: 0.07023174315690994\n",
            "----------------\n",
            "Loss:  1.1480603441596031 \n",
            "Classification loss: 2.165959358215332 \n",
            "Regression loss: 0.0650806650519371\n",
            "----------------\n",
            "Loss:  1.063835009932518 \n",
            "Classification loss: 2.04547119140625 \n",
            "Regression loss: 0.041099414229393005\n",
            "----------------\n",
            "Loss:  1.0395902022719383 \n",
            "Classification loss: 1.9445165395736694 \n",
            "Regression loss: 0.06733193248510361\n",
            "----------------\n",
            "Loss:  1.0173356980085373 \n",
            "Classification loss: 1.9693689346313477 \n",
            "Regression loss: 0.032651230692863464\n",
            "----------------\n",
            "Loss:  0.9998003020882607 \n",
            "Classification loss: 1.923130989074707 \n",
            "Regression loss: 0.038234807550907135\n",
            "----------------\n",
            "Loss:  1.0437528304755688 \n",
            "Classification loss: 1.978847622871399 \n",
            "Regression loss: 0.05432901903986931\n",
            "----------------\n",
            "Loss:  1.14906594902277 \n",
            "Classification loss: 2.1715493202209473 \n",
            "Regression loss: 0.0632912889122963\n",
            "----------------\n",
            "Loss:  1.0714097023010254 \n",
            "Classification loss: 2.0105466842651367 \n",
            "Regression loss: 0.06613636016845703\n",
            "----------------\n",
            "Loss:  0.9982769042253494 \n",
            "Classification loss: 1.8216581344604492 \n",
            "Regression loss: 0.08744783699512482\n",
            "----------------\n",
            "Loss:  1.10928775370121 \n",
            "Classification loss: 2.056300401687622 \n",
            "Regression loss: 0.08113755285739899\n",
            "----------------\n",
            "Loss:  1.0623236633837223 \n",
            "Classification loss: 2.003793239593506 \n",
            "Regression loss: 0.060427043586969376\n",
            "----------------\n",
            "Loss:  1.0166114419698715 \n",
            "Classification loss: 1.9663066864013672 \n",
            "Regression loss: 0.03345809876918793\n",
            "----------------\n",
            "Loss:  1.0800266452133656 \n",
            "Classification loss: 2.0630924701690674 \n",
            "Regression loss: 0.04848041012883186\n",
            "----------------\n",
            "Loss:  1.1183311827480793 \n",
            "Classification loss: 2.1491408348083496 \n",
            "Regression loss: 0.043760765343904495\n",
            "----------------\n",
            "Loss:  1.0976605489850044 \n",
            "Classification loss: 2.087918519973755 \n",
            "Regression loss: 0.053701288998126984\n",
            "----------------\n",
            "Loss:  1.0094454810023308 \n",
            "Classification loss: 1.9451076984405518 \n",
            "Regression loss: 0.0368916317820549\n",
            "----------------\n",
            "Loss:  1.1063838601112366 \n",
            "Classification loss: 2.087277889251709 \n",
            "Regression loss: 0.06274491548538208\n",
            "----------------\n",
            "Loss:  1.001725984737277 \n",
            "Classification loss: 1.9471139907836914 \n",
            "Regression loss: 0.028168989345431328\n",
            "----------------\n",
            "Loss:  0.9938383474946022 \n",
            "Classification loss: 1.8879992961883545 \n",
            "Regression loss: 0.04983869940042496\n",
            "----------------\n",
            "Loss:  1.1179395876824856 \n",
            "Classification loss: 2.157501220703125 \n",
            "Regression loss: 0.03918897733092308\n",
            "----------------\n",
            "Loss:  0.9863985553383827 \n",
            "Classification loss: 1.8854057788848877 \n",
            "Regression loss: 0.04369566589593887\n",
            "----------------\n",
            "Loss:  1.0936452895402908 \n",
            "Classification loss: 2.0602240562438965 \n",
            "Regression loss: 0.06353326141834259\n",
            "----------------\n",
            "Loss:  1.062743529677391 \n",
            "Classification loss: 1.9883739948272705 \n",
            "Regression loss: 0.0685565322637558\n",
            "----------------\n",
            "Loss:  1.2434481903910637 \n",
            "Classification loss: 2.4000062942504883 \n",
            "Regression loss: 0.04344504326581955\n",
            "----------------\n",
            "Loss:  1.052371859550476 \n",
            "Classification loss: 1.99408757686615 \n",
            "Regression loss: 0.05532807111740112\n",
            "----------------\n",
            "Loss:  1.1093968600034714 \n",
            "Classification loss: 2.0580453872680664 \n",
            "Regression loss: 0.08037416636943817\n",
            "----------------\n",
            "Loss:  0.9952245652675629 \n",
            "Classification loss: 1.8671021461486816 \n",
            "Regression loss: 0.061673492193222046\n",
            "----------------\n",
            "Loss:  1.1561405658721924 \n",
            "Classification loss: 2.1681017875671387 \n",
            "Regression loss: 0.07208967208862305\n",
            "----------------\n",
            "Loss:  1.1069144941866398 \n",
            "Classification loss: 2.1376662254333496 \n",
            "Regression loss: 0.03808138146996498\n",
            "----------------\n",
            "Loss:  1.0476386025547981 \n",
            "Classification loss: 2.001694679260254 \n",
            "Regression loss: 0.04679126292467117\n",
            "----------------\n",
            "Loss:  1.0731482692062855 \n",
            "Classification loss: 2.0299227237701416 \n",
            "Regression loss: 0.058186907321214676\n",
            "----------------\n",
            "Loss:  1.0857098046690226 \n",
            "Classification loss: 2.1146488189697266 \n",
            "Regression loss: 0.02838539518415928\n",
            "----------------\n",
            "Loss:  0.9849418066442013 \n",
            "Classification loss: 1.8778613805770874 \n",
            "Regression loss: 0.04601111635565758\n",
            "----------------\n",
            "Loss:  1.0442293286323547 \n",
            "Classification loss: 1.9586808681488037 \n",
            "Regression loss: 0.06488889455795288\n",
            "----------------\n",
            "Loss:  1.0852909460663795 \n",
            "Classification loss: 1.9837279319763184 \n",
            "Regression loss: 0.09342698007822037\n",
            "----------------\n",
            "Loss:  1.0995773896574974 \n",
            "Classification loss: 2.063396453857422 \n",
            "Regression loss: 0.06787916272878647\n",
            "----------------\n",
            "Loss:  0.9912622533738613 \n",
            "Classification loss: 1.8801534175872803 \n",
            "Regression loss: 0.051185544580221176\n",
            "----------------\n",
            "Loss:  0.9999026358127594 \n",
            "Classification loss: 1.8726892471313477 \n",
            "Regression loss: 0.06355801224708557\n",
            "----------------\n",
            "Loss:  1.0866021513938904 \n",
            "Classification loss: 2.0292744636535645 \n",
            "Regression loss: 0.07196491956710815\n",
            "----------------\n",
            "Loss:  1.0526891462504864 \n",
            "Classification loss: 2.0193653106689453 \n",
            "Regression loss: 0.04300649091601372\n",
            "----------------\n",
            "Loss:  1.0693659111857414 \n",
            "Classification loss: 2.020728349685669 \n",
            "Regression loss: 0.05900173634290695\n",
            "----------------\n",
            "Loss:  1.1145004481077194 \n",
            "Classification loss: 2.1521244049072266 \n",
            "Regression loss: 0.03843824565410614\n",
            "----------------\n",
            "Loss:  1.0870587714016438 \n",
            "Classification loss: 2.0535387992858887 \n",
            "Regression loss: 0.06028937175869942\n",
            "----------------\n",
            "Loss:  1.041860081255436 \n",
            "Classification loss: 1.9687697887420654 \n",
            "Regression loss: 0.05747518688440323\n",
            "----------------\n",
            "Loss:  1.0863983035087585 \n",
            "Classification loss: 2.082610607147217 \n",
            "Regression loss: 0.045092999935150146\n",
            "----------------\n",
            "Loss:  1.0248091965913773 \n",
            "Classification loss: 1.9517085552215576 \n",
            "Regression loss: 0.04895491898059845\n",
            "----------------\n",
            "Loss:  1.020709477365017 \n",
            "Classification loss: 1.942399024963379 \n",
            "Regression loss: 0.049509964883327484\n",
            "----------------\n",
            "Loss:  1.1076930463314056 \n",
            "Classification loss: 2.0501861572265625 \n",
            "Regression loss: 0.08259996771812439\n",
            "----------------\n",
            "Loss:  1.0676008611917496 \n",
            "Classification loss: 1.968957781791687 \n",
            "Regression loss: 0.08312197029590607\n",
            "----------------\n",
            "Loss:  0.9973505437374115 \n",
            "Classification loss: 1.8700954914093018 \n",
            "Regression loss: 0.06230279803276062\n",
            "----------------\n",
            "Loss:  1.1012532860040665 \n",
            "Classification loss: 2.0774762630462646 \n",
            "Regression loss: 0.06251515448093414\n",
            "----------------\n",
            "Loss:  1.1179542802274227 \n",
            "Classification loss: 2.1584973335266113 \n",
            "Regression loss: 0.03870561346411705\n",
            "----------------\n",
            "Loss:  1.0960036516189575 \n",
            "Classification loss: 2.060138702392578 \n",
            "Regression loss: 0.06593430042266846\n",
            "----------------\n",
            "Loss:  1.017117403447628 \n",
            "Classification loss: 1.9009854793548584 \n",
            "Regression loss: 0.06662466377019882\n",
            "----------------\n",
            "Loss:  1.1149865239858627 \n",
            "Classification loss: 2.0729427337646484 \n",
            "Regression loss: 0.07851515710353851\n",
            "----------------\n",
            "Loss:  1.087147381156683 \n",
            "Classification loss: 2.0942604541778564 \n",
            "Regression loss: 0.040017154067754745\n",
            "----------------\n",
            "Loss:  1.0769921317696571 \n",
            "Classification loss: 2.0589303970336914 \n",
            "Regression loss: 0.04752693325281143\n",
            "----------------\n",
            "Loss:  1.128881398588419 \n",
            "Classification loss: 2.1422204971313477 \n",
            "Regression loss: 0.05777115002274513\n",
            "----------------\n",
            "Loss:  1.0887672007083893 \n",
            "Classification loss: 2.0671820640563965 \n",
            "Regression loss: 0.05517616868019104\n",
            "----------------\n",
            "Loss:  1.0480449721217155 \n",
            "Classification loss: 1.9895919561386108 \n",
            "Regression loss: 0.053248994052410126\n",
            "----------------\n",
            "Loss:  1.1501740738749504 \n",
            "Classification loss: 2.227632522583008 \n",
            "Regression loss: 0.0363578125834465\n",
            "----------------\n",
            "Loss:  1.019702311605215 \n",
            "Classification loss: 1.971423864364624 \n",
            "Regression loss: 0.03399037942290306\n",
            "----------------\n",
            "Loss:  1.0703604146838188 \n",
            "Classification loss: 1.9961822032928467 \n",
            "Regression loss: 0.07226931303739548\n",
            "----------------\n",
            "Loss:  1.042426735162735 \n",
            "Classification loss: 1.9346415996551514 \n",
            "Regression loss: 0.0751059353351593\n",
            "----------------\n",
            "Loss:  1.1347039639949799 \n",
            "Classification loss: 2.1470437049865723 \n",
            "Regression loss: 0.061182111501693726\n",
            "----------------\n",
            "Loss:  1.1805581822991371 \n",
            "Classification loss: 2.1849422454833984 \n",
            "Regression loss: 0.0880870595574379\n",
            "----------------\n",
            "Loss:  1.0701971501111984 \n",
            "Classification loss: 2.0344741344451904 \n",
            "Regression loss: 0.05296008288860321\n",
            "----------------\n",
            "Loss:  1.115239828824997 \n",
            "Classification loss: 2.0598220825195312 \n",
            "Regression loss: 0.08532878756523132\n",
            "----------------\n",
            "Loss:  1.0310444310307503 \n",
            "Classification loss: 1.9471150636672974 \n",
            "Regression loss: 0.05748689919710159\n",
            "----------------\n",
            "Loss:  1.1246366500854492 \n",
            "Classification loss: 2.1142396926879883 \n",
            "Regression loss: 0.06751680374145508\n",
            "----------------\n",
            "Loss:  0.9933004081249237 \n",
            "Classification loss: 1.892210602760315 \n",
            "Regression loss: 0.047195106744766235\n",
            "----------------\n",
            "Loss:  1.0263731516897678 \n",
            "Classification loss: 1.9802641868591309 \n",
            "Regression loss: 0.03624105826020241\n",
            "----------------\n",
            "Loss:  1.0203096717596054 \n",
            "Classification loss: 1.963374376296997 \n",
            "Regression loss: 0.03862248361110687\n",
            "----------------\n",
            "Loss:  1.073754083365202 \n",
            "Classification loss: 2.0586957931518555 \n",
            "Regression loss: 0.044406186789274216\n",
            "----------------\n",
            "Loss:  1.0805736035108566 \n",
            "Classification loss: 2.0316152572631836 \n",
            "Regression loss: 0.06476597487926483\n",
            "----------------\n",
            "Loss:  1.1286598667502403 \n",
            "Classification loss: 2.1413538455963135 \n",
            "Regression loss: 0.05798294395208359\n",
            "----------------\n",
            "Loss:  1.020312450826168 \n",
            "Classification loss: 1.8841426372528076 \n",
            "Regression loss: 0.07824113219976425\n",
            "----------------\n",
            "Loss:  0.9630091115832329 \n",
            "Classification loss: 1.804611086845398 \n",
            "Regression loss: 0.060703568160533905\n",
            "----------------\n",
            "Loss:  1.031563013792038 \n",
            "Classification loss: 1.9654145240783691 \n",
            "Regression loss: 0.048855751752853394\n",
            "----------------\n",
            "Loss:  0.9976287335157394 \n",
            "Classification loss: 1.887305498123169 \n",
            "Regression loss: 0.05397598445415497\n",
            "----------------\n",
            "Loss:  1.0253942087292671 \n",
            "Classification loss: 1.9393527507781982 \n",
            "Regression loss: 0.055717833340168\n",
            "----------------\n",
            "Loss:  1.1546242535114288 \n",
            "Classification loss: 2.160594940185547 \n",
            "Regression loss: 0.0743267834186554\n",
            "----------------\n",
            "Loss:  1.1333970427513123 \n",
            "Classification loss: 2.1245615482330322 \n",
            "Regression loss: 0.07111626863479614\n",
            "----------------\n",
            "Loss:  1.0958626456558704 \n",
            "Classification loss: 2.1164424419403076 \n",
            "Regression loss: 0.03764142468571663\n",
            "----------------\n",
            "Loss:  0.9834954142570496 \n",
            "Classification loss: 1.8588091135025024 \n",
            "Regression loss: 0.05409085750579834\n",
            "----------------\n",
            "Loss:  1.1377481892704964 \n",
            "Classification loss: 2.140462875366211 \n",
            "Regression loss: 0.0675167515873909\n",
            "----------------\n",
            "Loss:  1.1158226877450943 \n",
            "Classification loss: 2.0829946994781494 \n",
            "Regression loss: 0.07432533800601959\n",
            "----------------\n",
            "Loss:  1.0316904038190842 \n",
            "Classification loss: 1.9596288204193115 \n",
            "Regression loss: 0.051875993609428406\n",
            "----------------\n",
            "Loss:  1.0793305337429047 \n",
            "Classification loss: 2.033413887023926 \n",
            "Regression loss: 0.06262359023094177\n",
            "----------------\n",
            "Loss:  1.0233019404113293 \n",
            "Classification loss: 1.9273347854614258 \n",
            "Regression loss: 0.05963454768061638\n",
            "----------------\n",
            "Loss:  1.0223287977278233 \n",
            "Classification loss: 1.9590905904769897 \n",
            "Regression loss: 0.042783502489328384\n",
            "----------------\n",
            "Loss:  1.029447264969349 \n",
            "Classification loss: 1.9034862518310547 \n",
            "Regression loss: 0.07770413905382156\n",
            "----------------\n",
            "Loss:  1.0655208677053452 \n",
            "Classification loss: 2.036355495452881 \n",
            "Regression loss: 0.047343119978904724\n",
            "----------------\n",
            "Loss:  1.0198104940354824 \n",
            "Classification loss: 1.9495171308517456 \n",
            "Regression loss: 0.045051928609609604\n",
            "----------------\n",
            "Loss:  1.1597884520888329 \n",
            "Classification loss: 2.1545939445495605 \n",
            "Regression loss: 0.08249147981405258\n",
            "----------------\n",
            "Loss:  1.0305408909916878 \n",
            "Classification loss: 1.9300262928009033 \n",
            "Regression loss: 0.06552774459123611\n",
            "----------------\n",
            "Loss:  1.049196321517229 \n",
            "Classification loss: 1.9762285947799683 \n",
            "Regression loss: 0.06108202412724495\n",
            "----------------\n",
            "Loss:  0.9820498377084732 \n",
            "Classification loss: 1.8579156398773193 \n",
            "Regression loss: 0.05309201776981354\n",
            "----------------\n",
            "Loss:  1.0962874963879585 \n",
            "Classification loss: 2.061854362487793 \n",
            "Regression loss: 0.06536031514406204\n",
            "----------------\n",
            "Loss:  1.1259511671960354 \n",
            "Classification loss: 2.177300453186035 \n",
            "Regression loss: 0.03730094060301781\n",
            "----------------\n",
            "Loss:  1.0951866023242474 \n",
            "Classification loss: 2.06766414642334 \n",
            "Regression loss: 0.06135452911257744\n",
            "----------------\n",
            "Loss:  1.0985896959900856 \n",
            "Classification loss: 2.073258876800537 \n",
            "Regression loss: 0.06196025758981705\n",
            "----------------\n",
            "Loss:  1.0702433735132217 \n",
            "Classification loss: 2.0561256408691406 \n",
            "Regression loss: 0.04218055307865143\n",
            "----------------\n",
            "Loss:  1.0522489696741104 \n",
            "Classification loss: 1.9783670902252197 \n",
            "Regression loss: 0.06306542456150055\n",
            "----------------\n",
            "Loss:  1.117709256708622 \n",
            "Classification loss: 2.1020116806030273 \n",
            "Regression loss: 0.0667034164071083\n",
            "----------------\n",
            "Loss:  0.9938108995556831 \n",
            "Classification loss: 1.8282843828201294 \n",
            "Regression loss: 0.07966870814561844\n",
            "----------------\n",
            "Loss:  1.1057501249015331 \n",
            "Classification loss: 2.1113765239715576 \n",
            "Regression loss: 0.05006186291575432\n",
            "----------------\n",
            "Loss:  1.083360768854618 \n",
            "Classification loss: 2.066789150238037 \n",
            "Regression loss: 0.04996619373559952\n",
            "----------------\n",
            "Loss:  0.9688120186328888 \n",
            "Classification loss: 1.8473186492919922 \n",
            "Regression loss: 0.0451526939868927\n",
            "----------------\n",
            "Loss:  1.1429742500185966 \n",
            "Classification loss: 2.216092109680176 \n",
            "Regression loss: 0.03492819517850876\n",
            "----------------\n",
            "Loss:  1.0712384805083275 \n",
            "Classification loss: 2.025707960128784 \n",
            "Regression loss: 0.058384500443935394\n",
            "----------------\n",
            "Loss:  1.0695579387247562 \n",
            "Classification loss: 2.0519323348999023 \n",
            "Regression loss: 0.04359177127480507\n",
            "----------------\n",
            "Loss:  1.0325752794742584 \n",
            "Classification loss: 1.938136100769043 \n",
            "Regression loss: 0.06350722908973694\n",
            "----------------\n",
            "Loss:  1.0709763318300247 \n",
            "Classification loss: 2.0172107219696045 \n",
            "Regression loss: 0.06237097084522247\n",
            "----------------\n",
            "Loss:  1.1260933130979538 \n",
            "Classification loss: 2.0995423793792725 \n",
            "Regression loss: 0.07632212340831757\n",
            "----------------\n",
            "Loss:  0.9779319688677788 \n",
            "Classification loss: 1.8120183944702148 \n",
            "Regression loss: 0.07192277163267136\n",
            "----------------\n",
            "Loss:  0.9231225289404392 \n",
            "Classification loss: 1.7387347221374512 \n",
            "Regression loss: 0.05375516787171364\n",
            "----------------\n",
            "Loss:  1.0428570285439491 \n",
            "Classification loss: 1.9463179111480713 \n",
            "Regression loss: 0.06969807296991348\n",
            "----------------\n",
            "Loss:  1.0549995824694633 \n",
            "Classification loss: 2.0189156532287598 \n",
            "Regression loss: 0.045541755855083466\n",
            "----------------\n",
            "Loss:  0.8804606944322586 \n",
            "Classification loss: 1.6893987655639648 \n",
            "Regression loss: 0.035761311650276184\n",
            "----------------\n",
            "Loss:  1.1442413702607155 \n",
            "Classification loss: 2.1974871158599854 \n",
            "Regression loss: 0.04549781233072281\n",
            "----------------\n",
            "Loss:  0.9636263065040112 \n",
            "Classification loss: 1.8610913753509521 \n",
            "Regression loss: 0.03308061882853508\n",
            "----------------\n",
            "Loss:  0.9449589401483536 \n",
            "Classification loss: 1.8045735359191895 \n",
            "Regression loss: 0.04267217218875885\n",
            "----------------\n",
            "Loss:  0.9800677075982094 \n",
            "Classification loss: 1.8715331554412842 \n",
            "Regression loss: 0.04430112987756729\n",
            "----------------\n",
            "Loss:  1.0580622851848602 \n",
            "Classification loss: 2.0017454624176025 \n",
            "Regression loss: 0.05718955397605896\n",
            "----------------\n",
            "Loss:  1.0605483129620552 \n",
            "Classification loss: 1.9359560012817383 \n",
            "Regression loss: 0.09257031232118607\n",
            "----------------\n",
            "Loss:  1.072727732360363 \n",
            "Classification loss: 2.0470831394195557 \n",
            "Regression loss: 0.049186162650585175\n",
            "----------------\n",
            "Loss:  1.0701665356755257 \n",
            "Classification loss: 2.0410819053649902 \n",
            "Regression loss: 0.04962558299303055\n",
            "----------------\n",
            "Loss:  1.2143690213561058 \n",
            "Classification loss: 2.2617716789245605 \n",
            "Regression loss: 0.08348318189382553\n",
            "----------------\n",
            "Loss:  1.255979873239994 \n",
            "Classification loss: 2.372793674468994 \n",
            "Regression loss: 0.06958303600549698\n",
            "----------------\n",
            "Loss:  1.0687423571944237 \n",
            "Classification loss: 1.99503493309021 \n",
            "Regression loss: 0.0712248906493187\n",
            "----------------\n",
            "Loss:  1.101732775568962 \n",
            "Classification loss: 2.069418430328369 \n",
            "Regression loss: 0.06702356040477753\n",
            "----------------\n",
            "Loss:  0.9649177268147469 \n",
            "Classification loss: 1.8620253801345825 \n",
            "Regression loss: 0.0339050367474556\n",
            "----------------\n",
            "Loss:  0.9708375297486782 \n",
            "Classification loss: 1.874799370765686 \n",
            "Regression loss: 0.03343784436583519\n",
            "----------------\n",
            "Loss:  0.969467930495739 \n",
            "Classification loss: 1.823418378829956 \n",
            "Regression loss: 0.057758741080760956\n",
            "----------------\n",
            "Loss:  1.0311629138886929 \n",
            "Classification loss: 1.983626127243042 \n",
            "Regression loss: 0.03934985026717186\n",
            "----------------\n",
            "Loss:  0.928551685065031 \n",
            "Classification loss: 1.7684937715530396 \n",
            "Regression loss: 0.044304799288511276\n",
            "----------------\n",
            "Loss:  0.9679705798625946 \n",
            "Classification loss: 1.786848545074463 \n",
            "Regression loss: 0.07454630732536316\n",
            "----------------\n",
            "Loss:  1.08201152831316 \n",
            "Classification loss: 2.037466049194336 \n",
            "Regression loss: 0.06327850371599197\n",
            "----------------\n",
            "Loss:  0.997268870472908 \n",
            "Classification loss: 1.8406068086624146 \n",
            "Regression loss: 0.07696546614170074\n",
            "----------------\n",
            "Loss:  1.0384784117341042 \n",
            "Classification loss: 1.9649285078048706 \n",
            "Regression loss: 0.056014157831668854\n",
            "----------------\n",
            "Loss:  0.9298449940979481 \n",
            "Classification loss: 1.798421859741211 \n",
            "Regression loss: 0.030634064227342606\n",
            "----------------\n",
            "Loss:  0.9626282304525375 \n",
            "Classification loss: 1.8555502891540527 \n",
            "Regression loss: 0.03485308587551117\n",
            "----------------\n",
            "Loss:  1.0305423885583878 \n",
            "Classification loss: 1.8907248973846436 \n",
            "Regression loss: 0.08517993986606598\n",
            "----------------\n",
            "Loss:  1.1239012479782104 \n",
            "Classification loss: 2.1304080486297607 \n",
            "Regression loss: 0.05869722366333008\n",
            "----------------\n",
            "Loss:  1.139922346919775 \n",
            "Classification loss: 2.1792845726013184 \n",
            "Regression loss: 0.05028006061911583\n",
            "----------------\n",
            "Loss:  1.0779495388269424 \n",
            "Classification loss: 2.011430025100708 \n",
            "Regression loss: 0.07223452627658844\n",
            "----------------\n",
            "Loss:  1.068593829870224 \n",
            "Classification loss: 2.0606372356414795 \n",
            "Regression loss: 0.03827521204948425\n",
            "----------------\n",
            "Loss:  1.0468910485506058 \n",
            "Classification loss: 1.96738862991333 \n",
            "Regression loss: 0.06319673359394073\n",
            "----------------\n",
            "Loss:  1.1353385224938393 \n",
            "Classification loss: 2.0994272232055664 \n",
            "Regression loss: 0.08562491089105606\n",
            "----------------\n",
            "Loss:  0.9444359689950943 \n",
            "Classification loss: 1.7914397716522217 \n",
            "Regression loss: 0.04871608316898346\n",
            "----------------\n",
            "Loss:  0.9553920775651932 \n",
            "Classification loss: 1.7823753356933594 \n",
            "Regression loss: 0.06420440971851349\n",
            "----------------\n",
            "Loss:  1.0073596313595772 \n",
            "Classification loss: 1.891390085220337 \n",
            "Regression loss: 0.06166458874940872\n",
            "----------------\n",
            "Loss:  1.024221584200859 \n",
            "Classification loss: 1.9115638732910156 \n",
            "Regression loss: 0.06843964755535126\n",
            "----------------\n",
            "Loss:  1.1270477138459682 \n",
            "Classification loss: 2.1379787921905518 \n",
            "Regression loss: 0.05805831775069237\n",
            "----------------\n",
            "Loss:  0.916278425604105 \n",
            "Classification loss: 1.739303708076477 \n",
            "Regression loss: 0.04662657156586647\n",
            "----------------\n",
            "Loss:  1.0021002069115639 \n",
            "Classification loss: 1.9037500619888306 \n",
            "Regression loss: 0.05022517591714859\n",
            "----------------\n",
            "Loss:  1.0031180456280708 \n",
            "Classification loss: 1.8669794797897339 \n",
            "Regression loss: 0.06962830573320389\n",
            "----------------\n",
            "Loss:  1.086305856704712 \n",
            "Classification loss: 2.0395631790161133 \n",
            "Regression loss: 0.06652426719665527\n",
            "----------------\n",
            "Loss:  1.0184609834104776 \n",
            "Classification loss: 1.9776545763015747 \n",
            "Regression loss: 0.029633695259690285\n",
            "----------------\n",
            "Loss:  1.0912265107035637 \n",
            "Classification loss: 2.0226211547851562 \n",
            "Regression loss: 0.07991593331098557\n",
            "----------------\n",
            "Loss:  0.9827467948198318 \n",
            "Classification loss: 1.8480994701385498 \n",
            "Regression loss: 0.058697059750556946\n",
            "----------------\n",
            "Loss:  0.927184421569109 \n",
            "Classification loss: 1.755706787109375 \n",
            "Regression loss: 0.04933102801442146\n",
            "----------------\n",
            "Loss:  1.075689159333706 \n",
            "Classification loss: 2.0447897911071777 \n",
            "Regression loss: 0.053294263780117035\n",
            "----------------\n",
            "Loss:  1.1207436844706535 \n",
            "Classification loss: 2.132812976837158 \n",
            "Regression loss: 0.05433719605207443\n",
            "----------------\n",
            "Loss:  1.0347537249326706 \n",
            "Classification loss: 1.9830869436264038 \n",
            "Regression loss: 0.04321025311946869\n",
            "----------------\n",
            "Loss:  0.9526918418705463 \n",
            "Classification loss: 1.8105281591415405 \n",
            "Regression loss: 0.04742776229977608\n",
            "----------------\n",
            "Loss:  1.1334366984665394 \n",
            "Classification loss: 2.153789758682251 \n",
            "Regression loss: 0.056541819125413895\n",
            "----------------\n",
            "Loss:  1.0766257382929325 \n",
            "Classification loss: 2.0840816497802734 \n",
            "Regression loss: 0.03458491340279579\n",
            "----------------\n",
            "Loss:  1.1711984127759933 \n",
            "Classification loss: 2.2229504585266113 \n",
            "Regression loss: 0.05972318351268768\n",
            "----------------\n",
            "Loss:  0.9883348345756531 \n",
            "Classification loss: 1.8375011682510376 \n",
            "Regression loss: 0.06958425045013428\n",
            "----------------\n",
            "Loss:  1.0368349067866802 \n",
            "Classification loss: 2.0269219875335693 \n",
            "Regression loss: 0.023373913019895554\n",
            "----------------\n",
            "Loss:  0.8972946591675282 \n",
            "Classification loss: 1.7083290815353394 \n",
            "Regression loss: 0.043130118399858475\n",
            "----------------\n",
            "Loss:  1.0179574266076088 \n",
            "Classification loss: 1.9043476581573486 \n",
            "Regression loss: 0.06578359752893448\n",
            "----------------\n",
            "Loss:  0.9683663547039032 \n",
            "Classification loss: 1.8725990056991577 \n",
            "Regression loss: 0.03206685185432434\n",
            "----------------\n",
            "Loss:  1.084823101758957 \n",
            "Classification loss: 2.015897512435913 \n",
            "Regression loss: 0.07687434554100037\n",
            "----------------\n",
            "Loss:  1.002825953066349 \n",
            "Classification loss: 1.9372881650924683 \n",
            "Regression loss: 0.0341818705201149\n",
            "----------------\n",
            "Loss:  1.0741419307887554 \n",
            "Classification loss: 2.0332047939300537 \n",
            "Regression loss: 0.05753953382372856\n",
            "----------------\n",
            "Loss:  1.006560329347849 \n",
            "Classification loss: 1.908353328704834 \n",
            "Regression loss: 0.0523836649954319\n",
            "----------------\n",
            "Loss:  1.1323166638612747 \n",
            "Classification loss: 2.124000310897827 \n",
            "Regression loss: 0.07031650841236115\n",
            "----------------\n",
            "Loss:  1.0564921498298645 \n",
            "Classification loss: 1.963466763496399 \n",
            "Regression loss: 0.07475876808166504\n",
            "----------------\n",
            "Loss:  0.9996288493275642 \n",
            "Classification loss: 1.8613262176513672 \n",
            "Regression loss: 0.06896574050188065\n",
            "----------------\n",
            "Loss:  1.1108336821198463 \n",
            "Classification loss: 2.1022863388061523 \n",
            "Regression loss: 0.05969051271677017\n",
            "----------------\n",
            "Loss:  1.0352932512760162 \n",
            "Classification loss: 1.9675945043563843 \n",
            "Regression loss: 0.0514959990978241\n",
            "----------------\n",
            "Loss:  1.0611120909452438 \n",
            "Classification loss: 1.9904524087905884 \n",
            "Regression loss: 0.06588588654994965\n",
            "----------------\n",
            "Loss:  0.9297053143382072 \n",
            "Classification loss: 1.7391726970672607 \n",
            "Regression loss: 0.060118965804576874\n",
            "----------------\n",
            "Loss:  1.0829132795333862 \n",
            "Classification loss: 2.0147550106048584 \n",
            "Regression loss: 0.07553577423095703\n",
            "----------------\n",
            "Loss:  1.0438481718301773 \n",
            "Classification loss: 2.0063161849975586 \n",
            "Regression loss: 0.04069007933139801\n",
            "----------------\n",
            "Loss:  1.0072195306420326 \n",
            "Classification loss: 1.9165221452713013 \n",
            "Regression loss: 0.04895845800638199\n",
            "----------------\n",
            "Loss:  1.0378967821598053 \n",
            "Classification loss: 1.9765262603759766 \n",
            "Regression loss: 0.04963365197181702\n",
            "----------------\n",
            "Loss:  1.0411132462322712 \n",
            "Classification loss: 1.9707443714141846 \n",
            "Regression loss: 0.05574106052517891\n",
            "----------------\n",
            "Loss:  1.046873576939106 \n",
            "Classification loss: 1.9621540307998657 \n",
            "Regression loss: 0.06579656153917313\n",
            "----------------\n",
            "Loss:  0.9160723611712456 \n",
            "Classification loss: 1.7861616611480713 \n",
            "Regression loss: 0.02299153059720993\n",
            "----------------\n",
            "Loss:  0.9974052309989929 \n",
            "Classification loss: 1.8638688325881958 \n",
            "Regression loss: 0.06547081470489502\n",
            "----------------\n",
            "Loss:  0.9883674830198288 \n",
            "Classification loss: 1.8818185329437256 \n",
            "Regression loss: 0.047458216547966\n",
            "----------------\n",
            "Loss:  1.0540988557040691 \n",
            "Classification loss: 2.0102086067199707 \n",
            "Regression loss: 0.048994552344083786\n",
            "----------------\n",
            "Loss:  1.0906845703721046 \n",
            "Classification loss: 2.0291175842285156 \n",
            "Regression loss: 0.07612577825784683\n",
            "----------------\n",
            "Loss:  0.92062933370471 \n",
            "Classification loss: 1.7896196842193604 \n",
            "Regression loss: 0.02581949159502983\n",
            "----------------\n",
            "Loss:  1.0848178416490555 \n",
            "Classification loss: 2.0137205123901367 \n",
            "Regression loss: 0.07795758545398712\n",
            "----------------\n",
            "Loss:  0.897909615188837 \n",
            "Classification loss: 1.6963269710540771 \n",
            "Regression loss: 0.04974612966179848\n",
            "----------------\n",
            "Loss:  1.0430170968174934 \n",
            "Classification loss: 1.9498906135559082 \n",
            "Regression loss: 0.06807179003953934\n",
            "----------------\n",
            "Loss:  0.9867056682705879 \n",
            "Classification loss: 1.8395130634307861 \n",
            "Regression loss: 0.06694913655519485\n",
            "----------------\n",
            "Loss:  1.1367762386798859 \n",
            "Classification loss: 2.133057117462158 \n",
            "Regression loss: 0.07024767994880676\n",
            "----------------\n",
            "Loss:  1.0621718987822533 \n",
            "Classification loss: 1.9592673778533936 \n",
            "Regression loss: 0.08253820985555649\n",
            "----------------\n",
            "Loss:  0.9931500591337681 \n",
            "Classification loss: 1.8725941181182861 \n",
            "Regression loss: 0.056853000074625015\n",
            "----------------\n",
            "Loss:  1.165029563009739 \n",
            "Classification loss: 2.1659905910491943 \n",
            "Regression loss: 0.08203426748514175\n",
            "----------------\n",
            "Loss:  1.069591410458088 \n",
            "Classification loss: 2.027611255645752 \n",
            "Regression loss: 0.055785782635211945\n",
            "----------------\n",
            "Loss:  0.8680447787046432 \n",
            "Classification loss: 1.6574405431747437 \n",
            "Regression loss: 0.03932450711727142\n",
            "----------------\n",
            "Loss:  0.9743701256811619 \n",
            "Classification loss: 1.8426353931427002 \n",
            "Regression loss: 0.05305242910981178\n",
            "----------------\n",
            "Loss:  1.0192558616399765 \n",
            "Classification loss: 1.9335918426513672 \n",
            "Regression loss: 0.05245994031429291\n",
            "----------------\n",
            "Loss:  1.0612284541130066 \n",
            "Classification loss: 2.024775743484497 \n",
            "Regression loss: 0.04884058237075806\n",
            "----------------\n",
            "Loss:  1.0705254897475243 \n",
            "Classification loss: 2.043257474899292 \n",
            "Regression loss: 0.048896752297878265\n",
            "----------------\n",
            "Loss:  0.9038921892642975 \n",
            "Classification loss: 1.6916232109069824 \n",
            "Regression loss: 0.058080583810806274\n",
            "----------------\n",
            "Loss:  0.9549049697816372 \n",
            "Classification loss: 1.8169193267822266 \n",
            "Regression loss: 0.04644530639052391\n",
            "----------------\n",
            "Loss:  0.9979458600282669 \n",
            "Classification loss: 1.9219985008239746 \n",
            "Regression loss: 0.0369466096162796\n",
            "----------------\n",
            "Loss:  0.9710205532610416 \n",
            "Classification loss: 1.8513681888580322 \n",
            "Regression loss: 0.04533645883202553\n",
            "----------------\n",
            "Loss:  1.005231937393546 \n",
            "Classification loss: 1.9710745811462402 \n",
            "Regression loss: 0.019694646820425987\n",
            "----------------\n",
            "Loss:  1.1490096375346184 \n",
            "Classification loss: 2.152294635772705 \n",
            "Regression loss: 0.07286231964826584\n",
            "----------------\n",
            "Loss:  1.1015519946813583 \n",
            "Classification loss: 2.043576955795288 \n",
            "Regression loss: 0.0797635167837143\n",
            "----------------\n",
            "Loss:  0.9210017882287502 \n",
            "Classification loss: 1.7763161659240723 \n",
            "Regression loss: 0.032843705266714096\n",
            "----------------\n",
            "Loss:  1.052896909415722 \n",
            "Classification loss: 1.9730557203292847 \n",
            "Regression loss: 0.06636904925107956\n",
            "----------------\n",
            "Loss:  1.0825077593326569 \n",
            "Classification loss: 2.0218734741210938 \n",
            "Regression loss: 0.07157102227210999\n",
            "----------------\n",
            "Loss:  1.0035182274878025 \n",
            "Classification loss: 1.897873878479004 \n",
            "Regression loss: 0.05458128824830055\n",
            "----------------\n",
            "Loss:  0.9524259492754936 \n",
            "Classification loss: 1.816258430480957 \n",
            "Regression loss: 0.044296734035015106\n",
            "----------------\n",
            "Loss:  1.0706023573875427 \n",
            "Classification loss: 2.0030457973480225 \n",
            "Regression loss: 0.0690794587135315\n",
            "----------------\n",
            "Loss:  1.0521498918533325 \n",
            "Classification loss: 1.9552708864212036 \n",
            "Regression loss: 0.07451444864273071\n",
            "----------------\n",
            "Loss:  1.098133496940136 \n",
            "Classification loss: 2.0693888664245605 \n",
            "Regression loss: 0.06343906372785568\n",
            "----------------\n",
            "Loss:  1.1225171461701393 \n",
            "Classification loss: 2.168691873550415 \n",
            "Regression loss: 0.03817120939493179\n",
            "----------------\n",
            "Loss:  0.8942391835153103 \n",
            "Classification loss: 1.7297874689102173 \n",
            "Regression loss: 0.029345449060201645\n",
            "----------------\n",
            "Loss:  1.0593435540795326 \n",
            "Classification loss: 1.9849607944488525 \n",
            "Regression loss: 0.06686315685510635\n",
            "----------------\n",
            "Loss:  1.081261083483696 \n",
            "Classification loss: 2.0117011070251465 \n",
            "Regression loss: 0.07541052997112274\n",
            "----------------\n",
            "Loss:  0.9952705055475235 \n",
            "Classification loss: 1.8727589845657349 \n",
            "Regression loss: 0.05889101326465607\n",
            "----------------\n",
            "Loss:  0.9690782576799393 \n",
            "Classification loss: 1.8671679496765137 \n",
            "Regression loss: 0.035494282841682434\n",
            "----------------\n",
            "Loss:  1.1055625677108765 \n",
            "Classification loss: 2.1378302574157715 \n",
            "Regression loss: 0.03664743900299072\n",
            "----------------\n",
            "Loss:  1.0165651068091393 \n",
            "Classification loss: 1.8728080987930298 \n",
            "Regression loss: 0.08016105741262436\n",
            "----------------\n",
            "Loss:  1.0363605953752995 \n",
            "Classification loss: 1.96832275390625 \n",
            "Regression loss: 0.052199218422174454\n",
            "----------------\n",
            "Loss:  0.9999960474669933 \n",
            "Classification loss: 1.9206994771957397 \n",
            "Regression loss: 0.03964630886912346\n",
            "----------------\n",
            "Loss:  1.0066834688186646 \n",
            "Classification loss: 1.8471609354019165 \n",
            "Regression loss: 0.0831030011177063\n",
            "----------------\n",
            "Loss:  1.0661491230130196 \n",
            "Classification loss: 1.9778130054473877 \n",
            "Regression loss: 0.07724262028932571\n",
            "----------------\n",
            "Loss:  0.9294172301888466 \n",
            "Classification loss: 1.7205064296722412 \n",
            "Regression loss: 0.06916401535272598\n",
            "----------------\n",
            "Loss:  0.902311697602272 \n",
            "Classification loss: 1.6996638774871826 \n",
            "Regression loss: 0.052479758858680725\n",
            "----------------\n",
            "Loss:  1.0720730125904083 \n",
            "Classification loss: 2.017421007156372 \n",
            "Regression loss: 0.06336250901222229\n",
            "----------------\n",
            "Loss:  0.9647766537964344 \n",
            "Classification loss: 1.8331987857818604 \n",
            "Regression loss: 0.04817726090550423\n",
            "----------------\n",
            "Loss:  1.0204941257834435 \n",
            "Classification loss: 1.922692060470581 \n",
            "Regression loss: 0.059148095548152924\n",
            "----------------\n",
            "Loss:  0.9170706160366535 \n",
            "Classification loss: 1.7364540100097656 \n",
            "Regression loss: 0.048843611031770706\n",
            "----------------\n",
            "Loss:  0.9704718738794327 \n",
            "Classification loss: 1.815686821937561 \n",
            "Regression loss: 0.06262846291065216\n",
            "----------------\n",
            "Loss:  0.9538418315351009 \n",
            "Classification loss: 1.8087031841278076 \n",
            "Regression loss: 0.04949023947119713\n",
            "----------------\n",
            "Loss:  1.0584019087255 \n",
            "Classification loss: 1.994492530822754 \n",
            "Regression loss: 0.061155643314123154\n",
            "----------------\n",
            "Loss:  0.9988753125071526 \n",
            "Classification loss: 1.8987855911254883 \n",
            "Regression loss: 0.04948251694440842\n",
            "----------------\n",
            "Loss:  0.9864322692155838 \n",
            "Classification loss: 1.8402354717254639 \n",
            "Regression loss: 0.06631453335285187\n",
            "----------------\n",
            "Loss:  0.8994596675038338 \n",
            "Classification loss: 1.739856243133545 \n",
            "Regression loss: 0.02953154593706131\n",
            "----------------\n",
            "Loss:  0.9990278482437134 \n",
            "Classification loss: 1.9014742374420166 \n",
            "Regression loss: 0.04829072952270508\n",
            "----------------\n",
            "Loss:  1.0870633944869041 \n",
            "Classification loss: 2.031905174255371 \n",
            "Regression loss: 0.0711108073592186\n",
            "----------------\n",
            "Loss:  0.8733364380896091 \n",
            "Classification loss: 1.6581741571426392 \n",
            "Regression loss: 0.044249359518289566\n",
            "----------------\n",
            "Loss:  1.0189922973513603 \n",
            "Classification loss: 1.9400813579559326 \n",
            "Regression loss: 0.04895161837339401\n",
            "----------------\n",
            "Loss:  1.1878298930823803 \n",
            "Classification loss: 2.263073444366455 \n",
            "Regression loss: 0.056293170899152756\n",
            "----------------\n",
            "Loss:  1.0395962446928024 \n",
            "Classification loss: 1.9305121898651123 \n",
            "Regression loss: 0.07434014976024628\n",
            "----------------\n",
            "Loss:  1.067679986357689 \n",
            "Classification loss: 2.055427074432373 \n",
            "Regression loss: 0.03996644914150238\n",
            "----------------\n",
            "Loss:  1.0184968337416649 \n",
            "Classification loss: 1.9121702909469604 \n",
            "Regression loss: 0.06241168826818466\n",
            "----------------\n",
            "Loss:  1.0488088391721249 \n",
            "Classification loss: 2.01497220993042 \n",
            "Regression loss: 0.0413227342069149\n",
            "----------------\n",
            "Loss:  0.9317323714494705 \n",
            "Classification loss: 1.7763818502426147 \n",
            "Regression loss: 0.04354144632816315\n",
            "----------------\n",
            "Loss:  0.9999862611293793 \n",
            "Classification loss: 1.8420754671096802 \n",
            "Regression loss: 0.07894852757453918\n",
            "----------------\n",
            "Loss:  0.9979991614818573 \n",
            "Classification loss: 1.909195899963379 \n",
            "Regression loss: 0.04340121150016785\n",
            "----------------\n",
            "Loss:  1.0095665119588375 \n",
            "Classification loss: 1.9141126871109009 \n",
            "Regression loss: 0.05251016840338707\n",
            "----------------\n",
            "Loss:  0.8930363841354847 \n",
            "Classification loss: 1.6780288219451904 \n",
            "Regression loss: 0.05402197316288948\n",
            "----------------\n",
            "Loss:  0.9936440512537956 \n",
            "Classification loss: 1.889204502105713 \n",
            "Regression loss: 0.04904180020093918\n",
            "----------------\n",
            "Loss:  1.0444996431469917 \n",
            "Classification loss: 1.9379936456680298 \n",
            "Regression loss: 0.07550282031297684\n",
            "----------------\n",
            "Loss:  1.0489009767770767 \n",
            "Classification loss: 1.96345055103302 \n",
            "Regression loss: 0.06717570126056671\n",
            "----------------\n",
            "Loss:  1.048444725573063 \n",
            "Classification loss: 1.9940770864486694 \n",
            "Regression loss: 0.05140618234872818\n",
            "----------------\n",
            "Loss:  1.0661243125796318 \n",
            "Classification loss: 2.0280795097351074 \n",
            "Regression loss: 0.052084557712078094\n",
            "----------------\n",
            "Loss:  1.253745675086975 \n",
            "Classification loss: 2.3355307579040527 \n",
            "Regression loss: 0.08598029613494873\n",
            "----------------\n",
            "Loss:  1.0092733427882195 \n",
            "Classification loss: 1.867020845413208 \n",
            "Regression loss: 0.07576292008161545\n",
            "----------------\n",
            "Loss:  1.0606824774295092 \n",
            "Classification loss: 2.061915397644043 \n",
            "Regression loss: 0.02972477860748768\n",
            "----------------\n",
            "Loss:  1.0492757856845856 \n",
            "Classification loss: 1.9789443016052246 \n",
            "Regression loss: 0.05980363488197327\n",
            "----------------\n",
            "Loss:  1.0613978728652 \n",
            "Classification loss: 1.9355539083480835 \n",
            "Regression loss: 0.0936209186911583\n",
            "----------------\n",
            "Loss:  1.0427790433168411 \n",
            "Classification loss: 1.9578936100006104 \n",
            "Regression loss: 0.06383223831653595\n",
            "----------------\n",
            "Loss:  0.8972009420394897 \n",
            "Classification loss: 1.7162017822265625 \n",
            "Regression loss: 0.039100050926208496\n",
            "----------------\n",
            "Loss:  1.129502434283495 \n",
            "Classification loss: 2.1370744705200195 \n",
            "Regression loss: 0.060965199023485184\n",
            "----------------\n",
            "Loss:  0.9307524710893631 \n",
            "Classification loss: 1.7088427543640137 \n",
            "Regression loss: 0.07633109390735626\n",
            "----------------\n",
            "Loss:  1.0421774573624134 \n",
            "Classification loss: 2.0121877193450928 \n",
            "Regression loss: 0.03608359768986702\n",
            "----------------\n",
            "Loss:  1.0904425010085106 \n",
            "Classification loss: 2.0811562538146973 \n",
            "Regression loss: 0.04986437410116196\n",
            "----------------\n",
            "Loss:  1.0455294102430344 \n",
            "Classification loss: 1.9814612865447998 \n",
            "Regression loss: 0.05479876697063446\n",
            "----------------\n",
            "Loss:  1.0460251495242119 \n",
            "Classification loss: 1.956131935119629 \n",
            "Regression loss: 0.06795918196439743\n",
            "----------------\n",
            "Loss:  0.9920312091708183 \n",
            "Classification loss: 1.9126602411270142 \n",
            "Regression loss: 0.03570108860731125\n",
            "----------------\n",
            "Loss:  0.8283872380852699 \n",
            "Classification loss: 1.591581106185913 \n",
            "Regression loss: 0.032596684992313385\n",
            "----------------\n",
            "Loss:  0.9682310856878757 \n",
            "Classification loss: 1.8539526462554932 \n",
            "Regression loss: 0.041254762560129166\n",
            "----------------\n",
            "Loss:  1.1246836110949516 \n",
            "Classification loss: 2.159182548522949 \n",
            "Regression loss: 0.04509233683347702\n",
            "----------------\n",
            "Loss:  1.0675035677850246 \n",
            "Classification loss: 2.0548465251922607 \n",
            "Regression loss: 0.04008030518889427\n",
            "----------------\n",
            "Loss:  0.9754543714225292 \n",
            "Classification loss: 1.828097939491272 \n",
            "Regression loss: 0.061405401676893234\n",
            "----------------\n",
            "Loss:  1.014482144266367 \n",
            "Classification loss: 1.9137426614761353 \n",
            "Regression loss: 0.05761081352829933\n",
            "----------------\n",
            "Loss:  1.0106366202235222 \n",
            "Classification loss: 1.9038795232772827 \n",
            "Regression loss: 0.05869685858488083\n",
            "----------------\n",
            "Loss:  0.9780650064349174 \n",
            "Classification loss: 1.8345201015472412 \n",
            "Regression loss: 0.060804955661296844\n",
            "----------------\n",
            "Loss:  0.9301547706127167 \n",
            "Classification loss: 1.7685385942459106 \n",
            "Regression loss: 0.04588547348976135\n",
            "----------------\n",
            "Loss:  0.9406707435846329 \n",
            "Classification loss: 1.7434625625610352 \n",
            "Regression loss: 0.0689394623041153\n",
            "----------------\n",
            "Loss:  0.9508765935897827 \n",
            "Classification loss: 1.8214571475982666 \n",
            "Regression loss: 0.040148019790649414\n",
            "----------------\n",
            "Loss:  1.0382836535573006 \n",
            "Classification loss: 1.9641510248184204 \n",
            "Regression loss: 0.05620814114809036\n",
            "----------------\n",
            "Loss:  1.0235658660531044 \n",
            "Classification loss: 1.9420578479766846 \n",
            "Regression loss: 0.052536942064762115\n",
            "----------------\n",
            "Loss:  1.063086099922657 \n",
            "Classification loss: 2.0116372108459473 \n",
            "Regression loss: 0.05726749449968338\n",
            "----------------\n",
            "Loss:  0.9341420158743858 \n",
            "Classification loss: 1.7763718366622925 \n",
            "Regression loss: 0.045956097543239594\n",
            "----------------\n",
            "Loss:  1.0382230207324028 \n",
            "Classification loss: 1.9129314422607422 \n",
            "Regression loss: 0.08175729960203171\n",
            "----------------\n",
            "Loss:  1.001958578824997 \n",
            "Classification loss: 1.8867676258087158 \n",
            "Regression loss: 0.05857476592063904\n",
            "----------------\n",
            "Loss:  1.002830870449543 \n",
            "Classification loss: 1.8804166316986084 \n",
            "Regression loss: 0.0626225546002388\n",
            "----------------\n",
            "Loss:  1.0828910395503044 \n",
            "Classification loss: 2.0702595710754395 \n",
            "Regression loss: 0.047761254012584686\n",
            "----------------\n",
            "Loss:  0.9676671028137207 \n",
            "Classification loss: 1.8776259422302246 \n",
            "Regression loss: 0.0288541316986084\n",
            "----------------\n",
            "Loss:  0.8582902699708939 \n",
            "Classification loss: 1.6491284370422363 \n",
            "Regression loss: 0.033726051449775696\n",
            "----------------\n",
            "Loss:  1.0137646049261093 \n",
            "Classification loss: 1.915542483329773 \n",
            "Regression loss: 0.05599336326122284\n",
            "----------------\n",
            "Loss:  0.8564606569707394 \n",
            "Classification loss: 1.6121957302093506 \n",
            "Regression loss: 0.05036279186606407\n",
            "----------------\n",
            "Loss:  0.9649993106722832 \n",
            "Classification loss: 1.8592242002487183 \n",
            "Regression loss: 0.03538721054792404\n",
            "----------------\n",
            "Loss:  0.8757708109915257 \n",
            "Classification loss: 1.6493340730667114 \n",
            "Regression loss: 0.05110377445816994\n",
            "----------------\n",
            "Loss:  1.0737384036183357 \n",
            "Classification loss: 2.0592877864837646 \n",
            "Regression loss: 0.0440945103764534\n",
            "----------------\n",
            "Loss:  1.0490096360445023 \n",
            "Classification loss: 1.999952793121338 \n",
            "Regression loss: 0.04903323948383331\n",
            "----------------\n",
            "Loss:  0.9484356865286827 \n",
            "Classification loss: 1.7490280866622925 \n",
            "Regression loss: 0.07392164319753647\n",
            "----------------\n",
            "Loss:  1.010417066514492 \n",
            "Classification loss: 1.8914034366607666 \n",
            "Regression loss: 0.06471534818410873\n",
            "----------------\n",
            "Loss:  0.8982153013348579 \n",
            "Classification loss: 1.6761622428894043 \n",
            "Regression loss: 0.06013417989015579\n",
            "----------------\n",
            "Loss:  1.155155949294567 \n",
            "Classification loss: 2.165112257003784 \n",
            "Regression loss: 0.07259982079267502\n",
            "----------------\n",
            "Loss:  0.9067613296210766 \n",
            "Classification loss: 1.7156610488891602 \n",
            "Regression loss: 0.048930805176496506\n",
            "----------------\n",
            "Loss:  0.94832643866539 \n",
            "Classification loss: 1.7770495414733887 \n",
            "Regression loss: 0.05980166792869568\n",
            "----------------\n",
            "Loss:  1.0150818526744843 \n",
            "Classification loss: 1.915536880493164 \n",
            "Regression loss: 0.05731341242790222\n",
            "----------------\n",
            "Loss:  1.0149982497096062 \n",
            "Classification loss: 1.9527404308319092 \n",
            "Regression loss: 0.03862803429365158\n",
            "----------------\n",
            "Loss:  0.9274416789412498 \n",
            "Classification loss: 1.8059861660003662 \n",
            "Regression loss: 0.024448595941066742\n",
            "----------------\n",
            "Loss:  0.9429132863879204 \n",
            "Classification loss: 1.7997326850891113 \n",
            "Regression loss: 0.043046943843364716\n",
            "----------------\n",
            "Loss:  0.9490375034511089 \n",
            "Classification loss: 1.8033459186553955 \n",
            "Regression loss: 0.04736454412341118\n",
            "----------------\n",
            "Loss:  1.0853688791394234 \n",
            "Classification loss: 2.0388171672821045 \n",
            "Regression loss: 0.06596029549837112\n",
            "----------------\n",
            "Loss:  1.0190952010452747 \n",
            "Classification loss: 1.9485182762145996 \n",
            "Regression loss: 0.04483606293797493\n",
            "----------------\n",
            "Loss:  0.8943636827170849 \n",
            "Classification loss: 1.7214512825012207 \n",
            "Regression loss: 0.03363804146647453\n",
            "----------------\n",
            "Loss:  0.9512985348701477 \n",
            "Classification loss: 1.8050068616867065 \n",
            "Regression loss: 0.048795104026794434\n",
            "----------------\n",
            "Loss:  1.0011325031518936 \n",
            "Classification loss: 1.9194694757461548 \n",
            "Regression loss: 0.04139776527881622\n",
            "----------------\n",
            "Loss:  1.0887215845286846 \n",
            "Classification loss: 2.062725305557251 \n",
            "Regression loss: 0.05735893175005913\n",
            "----------------\n",
            "Loss:  0.9709471836686134 \n",
            "Classification loss: 1.846780776977539 \n",
            "Regression loss: 0.0475567951798439\n",
            "----------------\n",
            "Loss:  1.0732606314122677 \n",
            "Classification loss: 2.0418481826782227 \n",
            "Regression loss: 0.05233654007315636\n",
            "----------------\n",
            "Loss:  0.9323020204901695 \n",
            "Classification loss: 1.7731866836547852 \n",
            "Regression loss: 0.04570867866277695\n",
            "----------------\n",
            "Loss:  0.8759325072169304 \n",
            "Classification loss: 1.6668198108673096 \n",
            "Regression loss: 0.042522601783275604\n",
            "----------------\n",
            "Loss:  0.9845213703811169 \n",
            "Classification loss: 1.8699743747711182 \n",
            "Regression loss: 0.049534182995557785\n",
            "----------------\n",
            "Loss:  0.9384176433086395 \n",
            "Classification loss: 1.7642855644226074 \n",
            "Regression loss: 0.056274861097335815\n",
            "----------------\n",
            "Loss:  0.9250037521123886 \n",
            "Classification loss: 1.7185107469558716 \n",
            "Regression loss: 0.06574837863445282\n",
            "----------------\n",
            "Loss:  0.9602575898170471 \n",
            "Classification loss: 1.7992609739303589 \n",
            "Regression loss: 0.060627102851867676\n",
            "----------------\n",
            "Loss:  1.1078712493181229 \n",
            "Classification loss: 2.043095588684082 \n",
            "Regression loss: 0.08632345497608185\n",
            "----------------\n",
            "Loss:  0.982595257461071 \n",
            "Classification loss: 1.8297250270843506 \n",
            "Regression loss: 0.06773274391889572\n",
            "----------------\n",
            "Loss:  0.9912613332271576 \n",
            "Classification loss: 1.8684744834899902 \n",
            "Regression loss: 0.057024091482162476\n",
            "----------------\n",
            "Loss:  0.9841577522456646 \n",
            "Classification loss: 1.9030479192733765 \n",
            "Regression loss: 0.032633792608976364\n",
            "----------------\n",
            "Loss:  1.0258899182081223 \n",
            "Classification loss: 1.8999440670013428 \n",
            "Regression loss: 0.07591788470745087\n",
            "----------------\n",
            "Loss:  0.9005292989313602 \n",
            "Classification loss: 1.6782865524291992 \n",
            "Regression loss: 0.061386022716760635\n",
            "----------------\n",
            "Loss:  0.9754132069647312 \n",
            "Classification loss: 1.8285279273986816 \n",
            "Regression loss: 0.061149243265390396\n",
            "----------------\n",
            "Loss:  0.9923690631985664 \n",
            "Classification loss: 1.8675649166107178 \n",
            "Regression loss: 0.05858660489320755\n",
            "----------------\n",
            "Loss:  1.0896924249827862 \n",
            "Classification loss: 2.058100700378418 \n",
            "Regression loss: 0.060642074793577194\n",
            "----------------\n",
            "Loss:  0.9818169102072716 \n",
            "Classification loss: 1.8454155921936035 \n",
            "Regression loss: 0.05910911411046982\n",
            "----------------\n",
            "Loss:  1.0436083376407623 \n",
            "Classification loss: 1.9556691646575928 \n",
            "Regression loss: 0.06577375531196594\n",
            "----------------\n",
            "Loss:  1.0041650235652924 \n",
            "Classification loss: 1.8351726531982422 \n",
            "Regression loss: 0.08657869696617126\n",
            "----------------\n",
            "Loss:  0.9480085782706738 \n",
            "Classification loss: 1.8213160037994385 \n",
            "Regression loss: 0.037350576370954514\n",
            "----------------\n",
            "Loss:  0.8593688458204269 \n",
            "Classification loss: 1.6025556325912476 \n",
            "Regression loss: 0.05809102952480316\n",
            "----------------\n",
            "Loss:  0.9193422496318817 \n",
            "Classification loss: 1.7436619997024536 \n",
            "Regression loss: 0.04751124978065491\n",
            "----------------\n",
            "Loss:  0.9088100288063288 \n",
            "Classification loss: 1.762911081314087 \n",
            "Regression loss: 0.027354488149285316\n",
            "----------------\n",
            "Loss:  0.9607885405421257 \n",
            "Classification loss: 1.8149960041046143 \n",
            "Regression loss: 0.05329053848981857\n",
            "----------------\n",
            "Loss:  0.979214496910572 \n",
            "Classification loss: 1.8203754425048828 \n",
            "Regression loss: 0.06902677565813065\n",
            "----------------\n",
            "Loss:  0.9083236586302519 \n",
            "Classification loss: 1.7644926309585571 \n",
            "Regression loss: 0.02607734315097332\n",
            "----------------\n",
            "Loss:  1.0639969110488892 \n",
            "Classification loss: 2.0405917167663574 \n",
            "Regression loss: 0.04370105266571045\n",
            "----------------\n",
            "Loss:  0.9361402094364166 \n",
            "Classification loss: 1.765824556350708 \n",
            "Regression loss: 0.05322793126106262\n",
            "----------------\n",
            "Loss:  0.9131150394678116 \n",
            "Classification loss: 1.7138763666152954 \n",
            "Regression loss: 0.05617685616016388\n",
            "----------------\n",
            "Loss:  1.0335623770952225 \n",
            "Classification loss: 1.8878750801086426 \n",
            "Regression loss: 0.08962483704090118\n",
            "----------------\n",
            "Loss:  0.9767640046775341 \n",
            "Classification loss: 1.8643407821655273 \n",
            "Regression loss: 0.04459361359477043\n",
            "----------------\n",
            "Loss:  1.0390423089265823 \n",
            "Classification loss: 1.9647698402404785 \n",
            "Regression loss: 0.05665738880634308\n",
            "----------------\n",
            "Loss:  0.9597555361688137 \n",
            "Classification loss: 1.826698660850525 \n",
            "Regression loss: 0.046406205743551254\n",
            "----------------\n",
            "Loss:  1.0565528348088264 \n",
            "Classification loss: 1.985700249671936 \n",
            "Regression loss: 0.06370270997285843\n",
            "----------------\n",
            "Loss:  0.96925950050354 \n",
            "Classification loss: 1.8045895099639893 \n",
            "Regression loss: 0.06696474552154541\n",
            "----------------\n",
            "Loss:  1.0756572894752026 \n",
            "Classification loss: 2.0504791736602783 \n",
            "Regression loss: 0.0504177026450634\n",
            "----------------\n",
            "Loss:  1.0596167892217636 \n",
            "Classification loss: 1.9197478294372559 \n",
            "Regression loss: 0.09974287450313568\n",
            "----------------\n",
            "Loss:  1.0049403607845306 \n",
            "Classification loss: 1.8677141666412354 \n",
            "Regression loss: 0.07108327746391296\n",
            "----------------\n",
            "Loss:  1.0410868600010872 \n",
            "Classification loss: 1.9472236633300781 \n",
            "Regression loss: 0.06747502833604813\n",
            "----------------\n",
            "Loss:  1.0000958815217018 \n",
            "Classification loss: 1.8548696041107178 \n",
            "Regression loss: 0.07266107946634293\n",
            "----------------\n",
            "Loss:  0.9987679272890091 \n",
            "Classification loss: 1.8549035787582397 \n",
            "Regression loss: 0.07131613790988922\n",
            "----------------\n",
            "Loss:  0.9829907640814781 \n",
            "Classification loss: 1.8420699834823608 \n",
            "Regression loss: 0.0619557723402977\n",
            "----------------\n",
            "Loss:  0.9896484278142452 \n",
            "Classification loss: 1.8996961116790771 \n",
            "Regression loss: 0.03980037197470665\n",
            "----------------\n",
            "Loss:  1.0002566501498222 \n",
            "Classification loss: 1.9147791862487793 \n",
            "Regression loss: 0.04286705702543259\n",
            "----------------\n",
            "Loss:  0.9126735255122185 \n",
            "Classification loss: 1.7348098754882812 \n",
            "Regression loss: 0.04526858776807785\n",
            "----------------\n",
            "Loss:  0.9799742139875889 \n",
            "Classification loss: 1.8651049137115479 \n",
            "Regression loss: 0.04742175713181496\n",
            "----------------\n",
            "Loss:  1.0003484040498734 \n",
            "Classification loss: 1.9054152965545654 \n",
            "Regression loss: 0.04764075577259064\n",
            "----------------\n",
            "Loss:  0.974442645907402 \n",
            "Classification loss: 1.8336024284362793 \n",
            "Regression loss: 0.05764143168926239\n",
            "----------------\n",
            "Loss:  0.9800762869417667 \n",
            "Classification loss: 1.8464324474334717 \n",
            "Regression loss: 0.0568600632250309\n",
            "----------------\n",
            "Loss:  0.9187028147280216 \n",
            "Classification loss: 1.7521159648895264 \n",
            "Regression loss: 0.04264483228325844\n",
            "----------------\n",
            "Loss:  0.9712120071053505 \n",
            "Classification loss: 1.842863917350769 \n",
            "Regression loss: 0.04978004842996597\n",
            "----------------\n",
            "Loss:  0.970374096184969 \n",
            "Classification loss: 1.87798273563385 \n",
            "Regression loss: 0.0313827283680439\n",
            "----------------\n",
            "Loss:  1.0364169776439667 \n",
            "Classification loss: 1.9397400617599487 \n",
            "Regression loss: 0.06654694676399231\n",
            "----------------\n",
            "Loss:  0.8777818530797958 \n",
            "Classification loss: 1.6511354446411133 \n",
            "Regression loss: 0.0522141307592392\n",
            "----------------\n",
            "Loss:  0.9287741109728813 \n",
            "Classification loss: 1.7233240604400635 \n",
            "Regression loss: 0.06711208075284958\n",
            "----------------\n",
            "Loss:  0.9198104273527861 \n",
            "Classification loss: 1.7901359796524048 \n",
            "Regression loss: 0.02474243752658367\n",
            "----------------\n",
            "Loss:  0.9444790259003639 \n",
            "Classification loss: 1.8064117431640625 \n",
            "Regression loss: 0.04127315431833267\n",
            "----------------\n",
            "Loss:  0.8955572359263897 \n",
            "Classification loss: 1.7462341785430908 \n",
            "Regression loss: 0.022440146654844284\n",
            "----------------\n",
            "Loss:  0.88718131929636 \n",
            "Classification loss: 1.6969796419143677 \n",
            "Regression loss: 0.03869149833917618\n",
            "----------------\n",
            "Loss:  1.001044936478138 \n",
            "Classification loss: 1.902237892150879 \n",
            "Regression loss: 0.04992599040269852\n",
            "----------------\n",
            "Loss:  0.8346419781446457 \n",
            "Classification loss: 1.5818930864334106 \n",
            "Regression loss: 0.04369543492794037\n",
            "----------------\n",
            "Loss:  1.0659505426883698 \n",
            "Classification loss: 2.0252745151519775 \n",
            "Regression loss: 0.05331328511238098\n",
            "----------------\n",
            "Loss:  0.9206493459641933 \n",
            "Classification loss: 1.730513572692871 \n",
            "Regression loss: 0.0553925596177578\n",
            "----------------\n",
            "Loss:  1.1534308642148972 \n",
            "Classification loss: 2.171219825744629 \n",
            "Regression loss: 0.0678209513425827\n",
            "----------------\n",
            "Loss:  1.0232362560927868 \n",
            "Classification loss: 1.9554548263549805 \n",
            "Regression loss: 0.045508842915296555\n",
            "----------------\n",
            "Loss:  1.0465239696204662 \n",
            "Classification loss: 1.9980945587158203 \n",
            "Regression loss: 0.047476690262556076\n",
            "----------------\n",
            "Loss:  0.8767985478043556 \n",
            "Classification loss: 1.6370831727981567 \n",
            "Regression loss: 0.05825696140527725\n",
            "----------------\n",
            "Loss:  0.9784131161868572 \n",
            "Classification loss: 1.863502025604248 \n",
            "Regression loss: 0.0466621033847332\n",
            "----------------\n",
            "Loss:  1.0314425453543663 \n",
            "Classification loss: 1.9510515928268433 \n",
            "Regression loss: 0.05591674894094467\n",
            "----------------\n",
            "Loss:  0.9161877259612083 \n",
            "Classification loss: 1.7397369146347046 \n",
            "Regression loss: 0.04631926864385605\n",
            "----------------\n",
            "Loss:  1.0374209582805634 \n",
            "Classification loss: 1.9440281391143799 \n",
            "Regression loss: 0.06540688872337341\n",
            "----------------\n",
            "Loss:  1.0645142644643784 \n",
            "Classification loss: 2.00209903717041 \n",
            "Regression loss: 0.06346474587917328\n",
            "----------------\n",
            "Loss:  1.0203215070068836 \n",
            "Classification loss: 1.971240520477295 \n",
            "Regression loss: 0.03470124676823616\n",
            "----------------\n",
            "Loss:  0.8354031965136528 \n",
            "Classification loss: 1.5941684246063232 \n",
            "Regression loss: 0.03831898421049118\n",
            "----------------\n",
            "Loss:  0.8187206014990807 \n",
            "Classification loss: 1.550684928894043 \n",
            "Regression loss: 0.043378137052059174\n",
            "----------------\n",
            "Loss:  0.9136603139340878 \n",
            "Classification loss: 1.72782301902771 \n",
            "Regression loss: 0.04974880442023277\n",
            "----------------\n",
            "Loss:  0.95469781011343 \n",
            "Classification loss: 1.8138518333435059 \n",
            "Regression loss: 0.047771893441677094\n",
            "----------------\n",
            "Loss:  1.0619659945368767 \n",
            "Classification loss: 1.9755420684814453 \n",
            "Regression loss: 0.07419496029615402\n",
            "----------------\n",
            "Loss:  1.1014754846692085 \n",
            "Classification loss: 2.0674619674682617 \n",
            "Regression loss: 0.06774450093507767\n",
            "----------------\n",
            "Loss:  0.9706358797848225 \n",
            "Classification loss: 1.8268628120422363 \n",
            "Regression loss: 0.0572044737637043\n",
            "----------------\n",
            "Loss:  0.996632531285286 \n",
            "Classification loss: 1.8842189311981201 \n",
            "Regression loss: 0.05452306568622589\n",
            "----------------\n",
            "Loss:  0.973923072218895 \n",
            "Classification loss: 1.8503080606460571 \n",
            "Regression loss: 0.048769041895866394\n",
            "----------------\n",
            "Loss:  0.8245301339775324 \n",
            "Classification loss: 1.6013381481170654 \n",
            "Regression loss: 0.023861059918999672\n",
            "----------------\n",
            "Loss:  1.0819445848464966 \n",
            "Classification loss: 2.0156285762786865 \n",
            "Regression loss: 0.07413029670715332\n",
            "----------------\n",
            "Loss:  1.0155851542949677 \n",
            "Classification loss: 1.9219869375228882 \n",
            "Regression loss: 0.05459168553352356\n",
            "----------------\n",
            "Loss:  0.8866144493222237 \n",
            "Classification loss: 1.6784324645996094 \n",
            "Regression loss: 0.047398217022418976\n",
            "----------------\n",
            "Loss:  1.0472102612257004 \n",
            "Classification loss: 1.952218770980835 \n",
            "Regression loss: 0.0711008757352829\n",
            "----------------\n",
            "Loss:  0.92813466116786 \n",
            "Classification loss: 1.7391637563705444 \n",
            "Regression loss: 0.058552782982587814\n",
            "----------------\n",
            "Loss:  1.0202106684446335 \n",
            "Classification loss: 1.9324450492858887 \n",
            "Regression loss: 0.05398814380168915\n",
            "----------------\n",
            "Loss:  1.0593046993017197 \n",
            "Classification loss: 1.9916688203811646 \n",
            "Regression loss: 0.06347028911113739\n",
            "----------------\n",
            "Loss:  1.0495187491178513 \n",
            "Classification loss: 1.9724700450897217 \n",
            "Regression loss: 0.06328372657299042\n",
            "----------------\n",
            "Loss:  0.9995033740997314 \n",
            "Classification loss: 1.8539561033248901 \n",
            "Regression loss: 0.07252532243728638\n",
            "----------------\n",
            "Loss:  0.9231435656547546 \n",
            "Classification loss: 1.7518622875213623 \n",
            "Regression loss: 0.047212421894073486\n",
            "----------------\n",
            "Loss:  1.0449383780360222 \n",
            "Classification loss: 1.9715776443481445 \n",
            "Regression loss: 0.05914955586194992\n",
            "----------------\n",
            "Loss:  0.9997828081250191 \n",
            "Classification loss: 1.9190051555633545 \n",
            "Regression loss: 0.04028023034334183\n",
            "----------------\n",
            "Loss:  0.9505545981228352 \n",
            "Classification loss: 1.8212249279022217 \n",
            "Regression loss: 0.03994213417172432\n",
            "----------------\n",
            "Loss:  1.0586848855018616 \n",
            "Classification loss: 1.9810556173324585 \n",
            "Regression loss: 0.06815707683563232\n",
            "----------------\n",
            "Loss:  0.9521000310778618 \n",
            "Classification loss: 1.8010361194610596 \n",
            "Regression loss: 0.051581971347332\n",
            "----------------\n",
            "Loss:  0.9826600775122643 \n",
            "Classification loss: 1.8486065864562988 \n",
            "Regression loss: 0.05835678428411484\n",
            "----------------\n",
            "Loss:  0.9572068899869919 \n",
            "Classification loss: 1.7788028717041016 \n",
            "Regression loss: 0.0678054541349411\n",
            "----------------\n",
            "Loss:  0.9017858430743217 \n",
            "Classification loss: 1.652782917022705 \n",
            "Regression loss: 0.07539438456296921\n",
            "----------------\n",
            "Loss:  0.8555023744702339 \n",
            "Classification loss: 1.6579902172088623 \n",
            "Regression loss: 0.026507265865802765\n",
            "----------------\n",
            "Loss:  0.8862131610512733 \n",
            "Classification loss: 1.7217068672180176 \n",
            "Regression loss: 0.025359727442264557\n",
            "----------------\n",
            "Loss:  0.8625285886228085 \n",
            "Classification loss: 1.6246793270111084 \n",
            "Regression loss: 0.05018892511725426\n",
            "----------------\n",
            "Loss:  0.8831123001873493 \n",
            "Classification loss: 1.6604981422424316 \n",
            "Regression loss: 0.0528632290661335\n",
            "----------------\n",
            "Loss:  0.9747735559940338 \n",
            "Classification loss: 1.808522343635559 \n",
            "Regression loss: 0.07051238417625427\n",
            "----------------\n",
            "Loss:  0.9859549626708031 \n",
            "Classification loss: 1.8305549621582031 \n",
            "Regression loss: 0.07067748159170151\n",
            "----------------\n",
            "Loss:  1.0467488542199135 \n",
            "Classification loss: 2.015368938446045 \n",
            "Regression loss: 0.03906438499689102\n",
            "----------------\n",
            "Loss:  0.9662185646593571 \n",
            "Classification loss: 1.8476521968841553 \n",
            "Regression loss: 0.042392466217279434\n",
            "----------------\n",
            "Loss:  1.0031004212796688 \n",
            "Classification loss: 1.8840069770812988 \n",
            "Regression loss: 0.061096932739019394\n",
            "----------------\n",
            "Loss:  0.8979949876666069 \n",
            "Classification loss: 1.694072961807251 \n",
            "Regression loss: 0.050958506762981415\n",
            "----------------\n",
            "Loss:  0.9210586585104465 \n",
            "Classification loss: 1.7475378513336182 \n",
            "Regression loss: 0.047289732843637466\n",
            "----------------\n",
            "Loss:  0.9999483451247215 \n",
            "Classification loss: 1.8941136598587036 \n",
            "Regression loss: 0.05289151519536972\n",
            "----------------\n",
            "Loss:  0.9592662900686264 \n",
            "Classification loss: 1.7780020236968994 \n",
            "Regression loss: 0.0702652782201767\n",
            "----------------\n",
            "Loss:  0.9128475114703178 \n",
            "Classification loss: 1.762647271156311 \n",
            "Regression loss: 0.03152387589216232\n",
            "----------------\n",
            "Loss:  1.0676906555891037 \n",
            "Classification loss: 1.9709498882293701 \n",
            "Regression loss: 0.08221571147441864\n",
            "----------------\n",
            "Loss:  1.041026871651411 \n",
            "Classification loss: 1.9917032718658447 \n",
            "Regression loss: 0.04517523571848869\n",
            "----------------\n",
            "Loss:  0.9638581573963165 \n",
            "Classification loss: 1.8472026586532593 \n",
            "Regression loss: 0.04025682806968689\n",
            "----------------\n",
            "Loss:  0.9163969233632088 \n",
            "Classification loss: 1.7699792385101318 \n",
            "Regression loss: 0.03140730410814285\n",
            "----------------\n",
            "Loss:  0.9900290183722973 \n",
            "Classification loss: 1.859193205833435 \n",
            "Regression loss: 0.06043241545557976\n",
            "----------------\n",
            "Loss:  0.9578281082212925 \n",
            "Classification loss: 1.843646764755249 \n",
            "Regression loss: 0.036004725843667984\n",
            "----------------\n",
            "Loss:  1.0005987957119942 \n",
            "Classification loss: 1.9161241054534912 \n",
            "Regression loss: 0.042536742985248566\n",
            "----------------\n",
            "Loss:  1.0100823231041431 \n",
            "Classification loss: 1.9263856410980225 \n",
            "Regression loss: 0.04688950255513191\n",
            "----------------\n",
            "Loss:  0.9917787835001945 \n",
            "Classification loss: 1.8608355522155762 \n",
            "Regression loss: 0.061361007392406464\n",
            "----------------\n",
            "Loss:  1.114823341369629 \n",
            "Classification loss: 2.0921425819396973 \n",
            "Regression loss: 0.06875205039978027\n",
            "----------------\n",
            "Loss:  1.0478417053818703 \n",
            "Classification loss: 1.986222267150879 \n",
            "Regression loss: 0.05473057180643082\n",
            "----------------\n",
            "Loss:  1.0253069400787354 \n",
            "Classification loss: 1.8898558616638184 \n",
            "Regression loss: 0.08037900924682617\n",
            "----------------\n",
            "Loss:  0.8947838768362999 \n",
            "Classification loss: 1.7054826021194458 \n",
            "Regression loss: 0.042042575776576996\n",
            "----------------\n",
            "Loss:  1.0254682563245296 \n",
            "Classification loss: 1.9473929405212402 \n",
            "Regression loss: 0.05177178606390953\n",
            "----------------\n",
            "Loss:  0.8993300572037697 \n",
            "Classification loss: 1.7149313688278198 \n",
            "Regression loss: 0.04186437278985977\n",
            "----------------\n",
            "Loss:  1.0006703026592731 \n",
            "Classification loss: 1.9024449586868286 \n",
            "Regression loss: 0.04944782331585884\n",
            "----------------\n",
            "Loss:  0.9454079195857048 \n",
            "Classification loss: 1.7738802433013916 \n",
            "Regression loss: 0.058467797935009\n",
            "----------------\n",
            "Loss:  0.9275787137448788 \n",
            "Classification loss: 1.756767749786377 \n",
            "Regression loss: 0.04919483885169029\n",
            "----------------\n",
            "Loss:  0.9379003867506981 \n",
            "Classification loss: 1.7624856233596802 \n",
            "Regression loss: 0.056657575070858\n",
            "----------------\n",
            "Loss:  0.9302928298711777 \n",
            "Classification loss: 1.8004403114318848 \n",
            "Regression loss: 0.03007267415523529\n",
            "----------------\n",
            "Loss:  1.0155060142278671 \n",
            "Classification loss: 1.8852792978286743 \n",
            "Regression loss: 0.07286636531352997\n",
            "----------------\n",
            "Loss:  0.9365589842200279 \n",
            "Classification loss: 1.7847472429275513 \n",
            "Regression loss: 0.04418536275625229\n",
            "----------------\n",
            "Loss:  0.9673329815268517 \n",
            "Classification loss: 1.8442155122756958 \n",
            "Regression loss: 0.045225225389003754\n",
            "----------------\n",
            "Loss:  0.9297889247536659 \n",
            "Classification loss: 1.7511024475097656 \n",
            "Regression loss: 0.05423770099878311\n",
            "----------------\n",
            "Loss:  0.946511048823595 \n",
            "Classification loss: 1.77544367313385 \n",
            "Regression loss: 0.05878921225667\n",
            "----------------\n",
            "Loss:  0.9696252755820751 \n",
            "Classification loss: 1.8269435167312622 \n",
            "Regression loss: 0.056153517216444016\n",
            "----------------\n",
            "Loss:  1.053202047944069 \n",
            "Classification loss: 1.9876238107681274 \n",
            "Regression loss: 0.05939014256000519\n",
            "----------------\n",
            "Loss:  0.9435597807168961 \n",
            "Classification loss: 1.6989271640777588 \n",
            "Regression loss: 0.09409619867801666\n",
            "----------------\n",
            "Loss:  1.0317639112472534 \n",
            "Classification loss: 1.9173779487609863 \n",
            "Regression loss: 0.07307493686676025\n",
            "----------------\n",
            "Loss:  0.8626841902732849 \n",
            "Classification loss: 1.6114017963409424 \n",
            "Regression loss: 0.05698329210281372\n",
            "----------------\n",
            "Loss:  0.9151165969669819 \n",
            "Classification loss: 1.7331905364990234 \n",
            "Regression loss: 0.04852132871747017\n",
            "----------------\n",
            "Loss:  0.9372109062969685 \n",
            "Classification loss: 1.7883102893829346 \n",
            "Regression loss: 0.043055761605501175\n",
            "----------------\n",
            "Loss:  0.8333461731672287 \n",
            "Classification loss: 1.5795557498931885 \n",
            "Regression loss: 0.04356829822063446\n",
            "----------------\n",
            "Loss:  0.9216951578855515 \n",
            "Classification loss: 1.6964010000228882 \n",
            "Regression loss: 0.07349465787410736\n",
            "----------------\n",
            "Loss:  0.8744962438941002 \n",
            "Classification loss: 1.6332688331604004 \n",
            "Regression loss: 0.057861827313899994\n",
            "----------------\n",
            "Loss:  0.95424859598279 \n",
            "Classification loss: 1.809048056602478 \n",
            "Regression loss: 0.04972456768155098\n",
            "----------------\n",
            "Loss:  0.9984560087323189 \n",
            "Classification loss: 1.846158742904663 \n",
            "Regression loss: 0.07537663727998734\n",
            "----------------\n",
            "Loss:  0.9953566491603851 \n",
            "Classification loss: 1.905731201171875 \n",
            "Regression loss: 0.04249104857444763\n",
            "----------------\n",
            "Loss:  0.898650262504816 \n",
            "Classification loss: 1.731702208518982 \n",
            "Regression loss: 0.03279915824532509\n",
            "----------------\n",
            "Loss:  0.936309739947319 \n",
            "Classification loss: 1.7531907558441162 \n",
            "Regression loss: 0.059714362025260925\n",
            "----------------\n",
            "Loss:  0.9814028218388557 \n",
            "Classification loss: 1.853559136390686 \n",
            "Regression loss: 0.054623253643512726\n",
            "----------------\n",
            "Loss:  1.0057390928268433 \n",
            "Classification loss: 1.8722807168960571 \n",
            "Regression loss: 0.0695987343788147\n",
            "----------------\n",
            "Loss:  0.9142328798770905 \n",
            "Classification loss: 1.7488701343536377 \n",
            "Regression loss: 0.039797812700271606\n",
            "----------------\n",
            "Loss:  0.8358711637556553 \n",
            "Classification loss: 1.584473729133606 \n",
            "Regression loss: 0.04363429918885231\n",
            "----------------\n",
            "Loss:  0.8665857538580894 \n",
            "Classification loss: 1.6491572856903076 \n",
            "Regression loss: 0.04200711101293564\n",
            "----------------\n",
            "Loss:  0.9716328084468842 \n",
            "Classification loss: 1.8158055543899536 \n",
            "Regression loss: 0.06373003125190735\n",
            "----------------\n",
            "Loss:  1.0181949846446514 \n",
            "Classification loss: 1.9165623188018799 \n",
            "Regression loss: 0.05991382524371147\n",
            "----------------\n",
            "Loss:  0.9652812778949738 \n",
            "Classification loss: 1.831580638885498 \n",
            "Regression loss: 0.04949095845222473\n",
            "----------------\n",
            "Loss:  1.0249606296420097 \n",
            "Classification loss: 1.935598373413086 \n",
            "Regression loss: 0.057161442935466766\n",
            "----------------\n",
            "Loss:  1.0326730161905289 \n",
            "Classification loss: 1.9555516242980957 \n",
            "Regression loss: 0.05489720404148102\n",
            "----------------\n",
            "Loss:  0.9134459160268307 \n",
            "Classification loss: 1.7118967771530151 \n",
            "Regression loss: 0.057497527450323105\n",
            "----------------\n",
            "Loss:  1.013631947338581 \n",
            "Classification loss: 1.8523942232131958 \n",
            "Regression loss: 0.08743483573198318\n",
            "----------------\n",
            "Loss:  0.9564248695969582 \n",
            "Classification loss: 1.8077330589294434 \n",
            "Regression loss: 0.05255834013223648\n",
            "----------------\n",
            "Loss:  0.9684301875531673 \n",
            "Classification loss: 1.8619412183761597 \n",
            "Regression loss: 0.03745957836508751\n",
            "----------------\n",
            "Loss:  0.9845951497554779 \n",
            "Classification loss: 1.8329031467437744 \n",
            "Regression loss: 0.0681435763835907\n",
            "----------------\n",
            "Loss:  1.0246488898992538 \n",
            "Classification loss: 1.89374577999115 \n",
            "Regression loss: 0.0777759999036789\n",
            "----------------\n",
            "Loss:  0.9982712864875793 \n",
            "Classification loss: 1.8984791040420532 \n",
            "Regression loss: 0.049031734466552734\n",
            "----------------\n",
            "Loss:  0.9871102273464203 \n",
            "Classification loss: 1.8437647819519043 \n",
            "Regression loss: 0.06522783637046814\n",
            "----------------\n",
            "Loss:  1.1069395989179611 \n",
            "Classification loss: 2.0425543785095215 \n",
            "Regression loss: 0.08566240966320038\n",
            "----------------\n",
            "Loss:  0.9453449547290802 \n",
            "Classification loss: 1.764383316040039 \n",
            "Regression loss: 0.06315329670906067\n",
            "----------------\n",
            "Loss:  1.0097361207008362 \n",
            "Classification loss: 1.8737645149230957 \n",
            "Regression loss: 0.07285386323928833\n",
            "----------------\n",
            "Loss:  0.9009161368012428 \n",
            "Classification loss: 1.7112350463867188 \n",
            "Regression loss: 0.04529861360788345\n",
            "----------------\n",
            "Loss:  0.961412712931633 \n",
            "Classification loss: 1.8189314603805542 \n",
            "Regression loss: 0.051946982741355896\n",
            "----------------\n",
            "Loss:  1.0608417727053165 \n",
            "Classification loss: 2.043677806854248 \n",
            "Regression loss: 0.03900286927819252\n",
            "----------------\n",
            "Loss:  1.0044558346271515 \n",
            "Classification loss: 1.8734785318374634 \n",
            "Regression loss: 0.0677165687084198\n",
            "----------------\n",
            "Loss:  0.8907652013003826 \n",
            "Classification loss: 1.7092911005020142 \n",
            "Regression loss: 0.036119651049375534\n",
            "----------------\n",
            "Loss:  0.8855522125959396 \n",
            "Classification loss: 1.687088966369629 \n",
            "Regression loss: 0.04200772941112518\n",
            "----------------\n",
            "Loss:  0.8545534089207649 \n",
            "Classification loss: 1.6323283910751343 \n",
            "Regression loss: 0.038389213383197784\n",
            "----------------\n",
            "Loss:  0.8869057781994343 \n",
            "Classification loss: 1.6781461238861084 \n",
            "Regression loss: 0.04783271625638008\n",
            "----------------\n",
            "Loss:  0.8712526150047779 \n",
            "Classification loss: 1.6218822002410889 \n",
            "Regression loss: 0.060311514884233475\n",
            "----------------\n",
            "Loss:  0.9269471690058708 \n",
            "Classification loss: 1.7586660385131836 \n",
            "Regression loss: 0.04761414974927902\n",
            "----------------\n",
            "Loss:  0.9189960099756718 \n",
            "Classification loss: 1.7647409439086914 \n",
            "Regression loss: 0.036625538021326065\n",
            "----------------\n",
            "Loss:  1.0126478858292103 \n",
            "Classification loss: 1.92815363407135 \n",
            "Regression loss: 0.04857106879353523\n",
            "----------------\n",
            "Loss:  0.9044937416911125 \n",
            "Classification loss: 1.6880226135253906 \n",
            "Regression loss: 0.060482434928417206\n",
            "----------------\n",
            "Loss:  0.9917920157313347 \n",
            "Classification loss: 1.899636149406433 \n",
            "Regression loss: 0.041973941028118134\n",
            "----------------\n",
            "Loss:  0.9841391444206238 \n",
            "Classification loss: 1.8759856224060059 \n",
            "Regression loss: 0.04614633321762085\n",
            "----------------\n",
            "Loss:  0.8855173215270042 \n",
            "Classification loss: 1.682544469833374 \n",
            "Regression loss: 0.04424508661031723\n",
            "----------------\n",
            "Loss:  1.0046494081616402 \n",
            "Classification loss: 1.8700882196426392 \n",
            "Regression loss: 0.06960529834032059\n",
            "----------------\n",
            "Loss:  0.8803094513714314 \n",
            "Classification loss: 1.6971851587295532 \n",
            "Regression loss: 0.03171687200665474\n",
            "----------------\n",
            "Loss:  1.017338365316391 \n",
            "Classification loss: 1.9085969924926758 \n",
            "Regression loss: 0.0630398690700531\n",
            "----------------\n",
            "Loss:  1.0415053013712168 \n",
            "Classification loss: 2.0379133224487305 \n",
            "Regression loss: 0.02254864014685154\n",
            "----------------\n",
            "Loss:  1.0795396268367767 \n",
            "Classification loss: 2.0240116119384766 \n",
            "Regression loss: 0.06753382086753845\n",
            "----------------\n",
            "Loss:  1.0710783153772354 \n",
            "Classification loss: 2.007321357727051 \n",
            "Regression loss: 0.06741763651371002\n",
            "----------------\n",
            "Loss:  0.9509241282939911 \n",
            "Classification loss: 1.802289605140686 \n",
            "Regression loss: 0.04977932572364807\n",
            "----------------\n",
            "Loss:  0.9586858600378036 \n",
            "Classification loss: 1.8105354309082031 \n",
            "Regression loss: 0.05341814458370209\n",
            "----------------\n",
            "Loss:  0.8491577543318272 \n",
            "Classification loss: 1.6163010597229004 \n",
            "Regression loss: 0.04100722447037697\n",
            "----------------\n",
            "Loss:  0.9246311523020267 \n",
            "Classification loss: 1.7748394012451172 \n",
            "Regression loss: 0.037211451679468155\n",
            "----------------\n",
            "Loss:  0.919226624071598 \n",
            "Classification loss: 1.6906181573867798 \n",
            "Regression loss: 0.07391754537820816\n",
            "----------------\n",
            "Loss:  0.9682939499616623 \n",
            "Classification loss: 1.8577682971954346 \n",
            "Regression loss: 0.03940980136394501\n",
            "----------------\n",
            "Loss:  1.044386513531208 \n",
            "Classification loss: 1.9761677980422974 \n",
            "Regression loss: 0.05630261451005936\n",
            "----------------\n",
            "Loss:  0.9116050284355879 \n",
            "Classification loss: 1.7656160593032837 \n",
            "Regression loss: 0.028796998783946037\n",
            "----------------\n",
            "Loss:  0.8866066448390484 \n",
            "Classification loss: 1.6779320240020752 \n",
            "Regression loss: 0.04764063283801079\n",
            "----------------\n",
            "Loss:  1.0409856475889683 \n",
            "Classification loss: 1.9987140893936157 \n",
            "Regression loss: 0.041628602892160416\n",
            "----------------\n",
            "Loss:  0.8426377475261688 \n",
            "Classification loss: 1.590871810913086 \n",
            "Regression loss: 0.047201842069625854\n",
            "----------------\n",
            "Loss:  0.9837403520941734 \n",
            "Classification loss: 1.855849266052246 \n",
            "Regression loss: 0.055815719068050385\n",
            "----------------\n",
            "Loss:  1.000411331653595 \n",
            "Classification loss: 1.8627090454101562 \n",
            "Regression loss: 0.06905680894851685\n",
            "----------------\n",
            "Loss:  0.8938989341259003 \n",
            "Classification loss: 1.6765729188919067 \n",
            "Regression loss: 0.0556124746799469\n",
            "----------------\n",
            "Loss:  0.9560240805149078 \n",
            "Classification loss: 1.7858095169067383 \n",
            "Regression loss: 0.0631193220615387\n",
            "----------------\n",
            "Loss:  0.9944256991147995 \n",
            "Classification loss: 1.8305354118347168 \n",
            "Regression loss: 0.0791579931974411\n",
            "----------------\n",
            "Loss:  0.9727863296866417 \n",
            "Classification loss: 1.8029911518096924 \n",
            "Regression loss: 0.0712907537817955\n",
            "----------------\n",
            "Loss:  1.1353435292840004 \n",
            "Classification loss: 2.1212944984436035 \n",
            "Regression loss: 0.07469628006219864\n",
            "----------------\n",
            "Loss:  1.0219488330185413 \n",
            "Classification loss: 1.9566078186035156 \n",
            "Regression loss: 0.043644923716783524\n",
            "----------------\n",
            "Loss:  1.0734758824110031 \n",
            "Classification loss: 1.9364012479782104 \n",
            "Regression loss: 0.10527525842189789\n",
            "----------------\n",
            "Loss:  0.9034476168453693 \n",
            "Classification loss: 1.7518749237060547 \n",
            "Regression loss: 0.027510154992341995\n",
            "----------------\n",
            "Loss:  1.0729254335165024 \n",
            "Classification loss: 2.074755907058716 \n",
            "Regression loss: 0.03554747998714447\n",
            "----------------\n",
            "Loss:  0.8886560127139091 \n",
            "Classification loss: 1.6780478954315186 \n",
            "Regression loss: 0.04963206499814987\n",
            "----------------\n",
            "Loss:  0.980997234582901 \n",
            "Classification loss: 1.8590209484100342 \n",
            "Regression loss: 0.05148676037788391\n",
            "----------------\n",
            "Loss:  1.00529246032238 \n",
            "Classification loss: 1.927096962928772 \n",
            "Regression loss: 0.04174397885799408\n",
            "----------------\n",
            "Loss:  0.8923610746860504 \n",
            "Classification loss: 1.6802551746368408 \n",
            "Regression loss: 0.052233487367630005\n",
            "----------------\n",
            "Loss:  0.9030507728457451 \n",
            "Classification loss: 1.6921892166137695 \n",
            "Regression loss: 0.05695616453886032\n",
            "----------------\n",
            "Loss:  0.8860750384628773 \n",
            "Classification loss: 1.6844531297683716 \n",
            "Regression loss: 0.04384847357869148\n",
            "----------------\n",
            "Loss:  0.9336215555667877 \n",
            "Classification loss: 1.762340784072876 \n",
            "Regression loss: 0.05245116353034973\n",
            "----------------\n",
            "Loss:  0.9202309921383858 \n",
            "Classification loss: 1.7481911182403564 \n",
            "Regression loss: 0.04613543301820755\n",
            "----------------\n",
            "Loss:  0.9800269901752472 \n",
            "Classification loss: 1.8683983087539673 \n",
            "Regression loss: 0.04582783579826355\n",
            "----------------\n",
            "Loss:  1.0542565695941448 \n",
            "Classification loss: 2.0438389778137207 \n",
            "Regression loss: 0.03233708068728447\n",
            "----------------\n",
            "Loss:  0.9405671432614326 \n",
            "Classification loss: 1.7833616733551025 \n",
            "Regression loss: 0.04888630658388138\n",
            "----------------\n",
            "Loss:  0.8663275837898254 \n",
            "Classification loss: 1.5976701974868774 \n",
            "Regression loss: 0.06749248504638672\n",
            "----------------\n",
            "Loss:  0.9402121119201183 \n",
            "Classification loss: 1.7813271284103394 \n",
            "Regression loss: 0.049548547714948654\n",
            "----------------\n",
            "Loss:  0.8699224963784218 \n",
            "Classification loss: 1.649170994758606 \n",
            "Regression loss: 0.045336998999118805\n",
            "----------------\n",
            "Loss:  0.9483732916414738 \n",
            "Classification loss: 1.7793536186218262 \n",
            "Regression loss: 0.058696482330560684\n",
            "----------------\n",
            "Loss:  0.9126397371292114 \n",
            "Classification loss: 1.7107081413269043 \n",
            "Regression loss: 0.05728566646575928\n",
            "----------------\n",
            "Loss:  0.9455487951636314 \n",
            "Classification loss: 1.748116135597229 \n",
            "Regression loss: 0.07149072736501694\n",
            "----------------\n",
            "Loss:  0.9346959814429283 \n",
            "Classification loss: 1.7774302959442139 \n",
            "Regression loss: 0.04598083347082138\n",
            "----------------\n",
            "Loss:  0.9026880823075771 \n",
            "Classification loss: 1.699188232421875 \n",
            "Regression loss: 0.05309396609663963\n",
            "----------------\n",
            "Loss:  0.9250044487416744 \n",
            "Classification loss: 1.769979476928711 \n",
            "Regression loss: 0.040014710277318954\n",
            "----------------\n",
            "Loss:  0.932283278554678 \n",
            "Classification loss: 1.768676996231079 \n",
            "Regression loss: 0.04794478043913841\n",
            "----------------\n",
            "Loss:  0.9187250286340714 \n",
            "Classification loss: 1.7744781970977783 \n",
            "Regression loss: 0.03148593008518219\n",
            "----------------\n",
            "Loss:  0.8738536760210991 \n",
            "Classification loss: 1.6453440189361572 \n",
            "Regression loss: 0.05118166655302048\n",
            "----------------\n",
            "Loss:  1.0134501829743385 \n",
            "Classification loss: 1.917946457862854 \n",
            "Regression loss: 0.05447695404291153\n",
            "----------------\n",
            "Loss:  0.889373766258359 \n",
            "Classification loss: 1.7195743322372437 \n",
            "Regression loss: 0.02958660013973713\n",
            "----------------\n",
            "Loss:  0.9534791745245457 \n",
            "Classification loss: 1.8283843994140625 \n",
            "Regression loss: 0.03928697481751442\n",
            "----------------\n",
            "Loss:  0.8881680741906166 \n",
            "Classification loss: 1.6797361373901367 \n",
            "Regression loss: 0.04830000549554825\n",
            "----------------\n",
            "Loss:  0.9641847461462021 \n",
            "Classification loss: 1.8272002935409546 \n",
            "Regression loss: 0.05058459937572479\n",
            "----------------\n",
            "Loss:  0.9953630864620209 \n",
            "Classification loss: 1.8419504165649414 \n",
            "Regression loss: 0.07438787817955017\n",
            "----------------\n",
            "Loss:  0.9335848763585091 \n",
            "Classification loss: 1.8083350658416748 \n",
            "Regression loss: 0.02941734343767166\n",
            "----------------\n",
            "Loss:  0.9485554620623589 \n",
            "Classification loss: 1.7754433155059814 \n",
            "Regression loss: 0.060833804309368134\n",
            "----------------\n",
            "Loss:  0.977235171943903 \n",
            "Classification loss: 1.8477420806884766 \n",
            "Regression loss: 0.05336413159966469\n",
            "----------------\n",
            "Loss:  0.942382138222456 \n",
            "Classification loss: 1.8000595569610596 \n",
            "Regression loss: 0.04235235974192619\n",
            "----------------\n",
            "Loss:  0.9350661151111126 \n",
            "Classification loss: 1.7695062160491943 \n",
            "Regression loss: 0.05031300708651543\n",
            "----------------\n",
            "Loss:  0.9436848796904087 \n",
            "Classification loss: 1.7724453210830688 \n",
            "Regression loss: 0.05746221914887428\n",
            "----------------\n",
            "Loss:  0.9324525929987431 \n",
            "Classification loss: 1.774924635887146 \n",
            "Regression loss: 0.04499027505517006\n",
            "----------------\n",
            "Loss:  0.9231838770210743 \n",
            "Classification loss: 1.7465064525604248 \n",
            "Regression loss: 0.04993065074086189\n",
            "----------------\n",
            "Loss:  0.9124082773923874 \n",
            "Classification loss: 1.7216405868530273 \n",
            "Regression loss: 0.05158798396587372\n",
            "----------------\n",
            "Loss:  0.9392807856202126 \n",
            "Classification loss: 1.793526530265808 \n",
            "Regression loss: 0.0425175204873085\n",
            "----------------\n",
            "Loss:  0.8931242823600769 \n",
            "Classification loss: 1.650447964668274 \n",
            "Regression loss: 0.06790030002593994\n",
            "----------------\n",
            "Loss:  0.9686245694756508 \n",
            "Classification loss: 1.7834982872009277 \n",
            "Regression loss: 0.07687542587518692\n",
            "----------------\n",
            "Loss:  0.8849522806704044 \n",
            "Classification loss: 1.7112700939178467 \n",
            "Regression loss: 0.029317233711481094\n",
            "----------------\n",
            "Loss:  0.9470823556184769 \n",
            "Classification loss: 1.7639673948287964 \n",
            "Regression loss: 0.06509865820407867\n",
            "----------------\n",
            "Loss:  0.890676774084568 \n",
            "Classification loss: 1.69132661819458 \n",
            "Regression loss: 0.045013464987277985\n",
            "----------------\n",
            "Loss:  0.8557174503803253 \n",
            "Classification loss: 1.5764002799987793 \n",
            "Regression loss: 0.06751731038093567\n",
            "----------------\n",
            "Loss:  0.9414210431277752 \n",
            "Classification loss: 1.7862067222595215 \n",
            "Regression loss: 0.04831768199801445\n",
            "----------------\n",
            "Loss:  0.884670652449131 \n",
            "Classification loss: 1.6621617078781128 \n",
            "Regression loss: 0.053589798510074615\n",
            "----------------\n",
            "Loss:  1.021700233221054 \n",
            "Classification loss: 1.9528405666351318 \n",
            "Regression loss: 0.04527994990348816\n",
            "----------------\n",
            "Loss:  0.9020998626947403 \n",
            "Classification loss: 1.7056362628936768 \n",
            "Regression loss: 0.049281731247901917\n",
            "----------------\n",
            "Loss:  0.9211841002106667 \n",
            "Classification loss: 1.7291922569274902 \n",
            "Regression loss: 0.05658797174692154\n",
            "----------------\n",
            "Loss:  0.9524207897484303 \n",
            "Classification loss: 1.8242692947387695 \n",
            "Regression loss: 0.040286142379045486\n",
            "----------------\n",
            "Loss:  1.0605118721723557 \n",
            "Classification loss: 1.9932953119277954 \n",
            "Regression loss: 0.06386421620845795\n",
            "----------------\n",
            "Loss:  0.9063859507441521 \n",
            "Classification loss: 1.6898174285888672 \n",
            "Regression loss: 0.061477236449718475\n",
            "----------------\n",
            "Loss:  0.8959226198494434 \n",
            "Classification loss: 1.6953144073486328 \n",
            "Regression loss: 0.04826541617512703\n",
            "----------------\n",
            "Loss:  0.8967857882380486 \n",
            "Classification loss: 1.7146613597869873 \n",
            "Regression loss: 0.0394551083445549\n",
            "----------------\n",
            "Loss:  1.003705620765686 \n",
            "Classification loss: 1.8718652725219727 \n",
            "Regression loss: 0.06777298450469971\n",
            "----------------\n",
            "Loss:  0.9797254502773285 \n",
            "Classification loss: 1.8825880289077759 \n",
            "Regression loss: 0.03843143582344055\n",
            "----------------\n",
            "Loss:  0.9207296818494797 \n",
            "Classification loss: 1.706878662109375 \n",
            "Regression loss: 0.06729035079479218\n",
            "----------------\n",
            "Loss:  0.9627680145204067 \n",
            "Classification loss: 1.8128925561904907 \n",
            "Regression loss: 0.05632173642516136\n",
            "----------------\n",
            "Loss:  1.0037298947572708 \n",
            "Classification loss: 1.8648960590362549 \n",
            "Regression loss: 0.07128186523914337\n",
            "----------------\n",
            "Loss:  0.9854599684476852 \n",
            "Classification loss: 1.844874382019043 \n",
            "Regression loss: 0.06302277743816376\n",
            "----------------\n",
            "Loss:  0.9685195535421371 \n",
            "Classification loss: 1.8455394506454468 \n",
            "Regression loss: 0.04574982821941376\n",
            "----------------\n",
            "Loss:  0.8623302020132542 \n",
            "Classification loss: 1.6235308647155762 \n",
            "Regression loss: 0.05056476965546608\n",
            "----------------\n",
            "Loss:  0.9491707310080528 \n",
            "Classification loss: 1.7679094076156616 \n",
            "Regression loss: 0.06521602720022202\n",
            "----------------\n",
            "Loss:  0.9309441111981869 \n",
            "Classification loss: 1.7913857698440552 \n",
            "Regression loss: 0.035251226276159286\n",
            "----------------\n",
            "Loss:  0.8861302137374878 \n",
            "Classification loss: 1.711448311805725 \n",
            "Regression loss: 0.030406057834625244\n",
            "----------------\n",
            "Loss:  0.9725202471017838 \n",
            "Classification loss: 1.8549292087554932 \n",
            "Regression loss: 0.04505564272403717\n",
            "----------------\n",
            "Loss:  0.8465473875403404 \n",
            "Classification loss: 1.630903959274292 \n",
            "Regression loss: 0.031095407903194427\n",
            "----------------\n",
            "Loss:  0.9371736980974674 \n",
            "Classification loss: 1.7702813148498535 \n",
            "Regression loss: 0.052033040672540665\n",
            "----------------\n",
            "Loss:  0.9585101716220379 \n",
            "Classification loss: 1.86440110206604 \n",
            "Regression loss: 0.026309620589017868\n",
            "----------------\n",
            "Loss:  1.023312121629715 \n",
            "Classification loss: 1.8687403202056885 \n",
            "Regression loss: 0.08894196152687073\n",
            "----------------\n",
            "Loss:  1.0690150186419487 \n",
            "Classification loss: 2.0236380100250244 \n",
            "Regression loss: 0.05719601362943649\n",
            "----------------\n",
            "Loss:  0.741524163633585 \n",
            "Classification loss: 1.423231601715088 \n",
            "Regression loss: 0.02990836277604103\n",
            "----------------\n",
            "Loss:  0.8693816140294075 \n",
            "Classification loss: 1.6515804529190063 \n",
            "Regression loss: 0.04359138756990433\n",
            "----------------\n",
            "Loss:  0.8799497112631798 \n",
            "Classification loss: 1.69935941696167 \n",
            "Regression loss: 0.030270002782344818\n",
            "----------------\n",
            "Loss:  1.0020378232002258 \n",
            "Classification loss: 1.9147331714630127 \n",
            "Regression loss: 0.04467123746871948\n",
            "----------------\n",
            "Loss:  0.9101344868540764 \n",
            "Classification loss: 1.6931251287460327 \n",
            "Regression loss: 0.06357192248106003\n",
            "----------------\n",
            "Loss:  0.8869590796530247 \n",
            "Classification loss: 1.6607269048690796 \n",
            "Regression loss: 0.05659562721848488\n",
            "----------------\n",
            "Loss:  0.9535141922533512 \n",
            "Classification loss: 1.799450159072876 \n",
            "Regression loss: 0.05378911271691322\n",
            "----------------\n",
            "Loss:  0.7400449812412262 \n",
            "Classification loss: 1.419600248336792 \n",
            "Regression loss: 0.0302448570728302\n",
            "----------------\n",
            "Loss:  0.9078188128769398 \n",
            "Classification loss: 1.7152212858200073 \n",
            "Regression loss: 0.05020816996693611\n",
            "----------------\n",
            "Loss:  1.0141783989965916 \n",
            "Classification loss: 1.9202791452407837 \n",
            "Regression loss: 0.05403882637619972\n",
            "----------------\n",
            "Loss:  0.986673180013895 \n",
            "Classification loss: 1.8937106132507324 \n",
            "Regression loss: 0.039817873388528824\n",
            "----------------\n",
            "Loss:  0.8435511067509651 \n",
            "Classification loss: 1.5986788272857666 \n",
            "Regression loss: 0.04421169310808182\n",
            "----------------\n",
            "Loss:  1.0517389327287674 \n",
            "Classification loss: 1.97084641456604 \n",
            "Regression loss: 0.06631572544574738\n",
            "----------------\n",
            "Loss:  0.8661262169480324 \n",
            "Classification loss: 1.6272549629211426 \n",
            "Regression loss: 0.05249873548746109\n",
            "----------------\n",
            "Loss:  0.9208777025341988 \n",
            "Classification loss: 1.755855917930603 \n",
            "Regression loss: 0.04294974356889725\n",
            "----------------\n",
            "Loss:  0.8361168503761292 \n",
            "Classification loss: 1.603209376335144 \n",
            "Regression loss: 0.03451216220855713\n",
            "----------------\n",
            "Loss:  1.0743122659623623 \n",
            "Classification loss: 2.057347297668457 \n",
            "Regression loss: 0.045638617128133774\n",
            "----------------\n",
            "Loss:  1.053427666425705 \n",
            "Classification loss: 1.9484214782714844 \n",
            "Regression loss: 0.07921692728996277\n",
            "----------------\n",
            "Loss:  0.9418672323226929 \n",
            "Classification loss: 1.7850370407104492 \n",
            "Regression loss: 0.04934871196746826\n",
            "----------------\n",
            "Loss:  0.890096103772521 \n",
            "Classification loss: 1.736948847770691 \n",
            "Regression loss: 0.02162167988717556\n",
            "----------------\n",
            "Loss:  0.9683447331190109 \n",
            "Classification loss: 1.8701212406158447 \n",
            "Regression loss: 0.03328411281108856\n",
            "----------------\n",
            "Loss:  0.8238484933972359 \n",
            "Classification loss: 1.605750322341919 \n",
            "Regression loss: 0.020973332226276398\n",
            "----------------\n",
            "Loss:  1.0339199230074883 \n",
            "Classification loss: 1.9339789152145386 \n",
            "Regression loss: 0.06693046540021896\n",
            "----------------\n",
            "Loss:  0.9520007148385048 \n",
            "Classification loss: 1.7900824546813965 \n",
            "Regression loss: 0.05695948749780655\n",
            "----------------\n",
            "Loss:  1.0119832754135132 \n",
            "Classification loss: 1.9323780536651611 \n",
            "Regression loss: 0.04579424858093262\n",
            "----------------\n",
            "Loss:  0.899334728717804 \n",
            "Classification loss: 1.6427257061004639 \n",
            "Regression loss: 0.07797187566757202\n",
            "----------------\n",
            "Loss:  0.7897459268569946 \n",
            "Classification loss: 1.533041000366211 \n",
            "Regression loss: 0.02322542667388916\n",
            "----------------\n",
            "Loss:  0.8237357810139656 \n",
            "Classification loss: 1.583958625793457 \n",
            "Regression loss: 0.03175646811723709\n",
            "----------------\n",
            "Loss:  0.9584637023508549 \n",
            "Classification loss: 1.7999898195266724 \n",
            "Regression loss: 0.05846879258751869\n",
            "----------------\n",
            "Loss:  0.9313849657773972 \n",
            "Classification loss: 1.7648155689239502 \n",
            "Regression loss: 0.04897718131542206\n",
            "----------------\n",
            "Loss:  0.9237313866615295 \n",
            "Classification loss: 1.6749716997146606 \n",
            "Regression loss: 0.08624553680419922\n",
            "----------------\n",
            "Loss:  1.013097070157528 \n",
            "Classification loss: 1.9193940162658691 \n",
            "Regression loss: 0.05340006202459335\n",
            "----------------\n",
            "Loss:  0.9260752163827419 \n",
            "Classification loss: 1.7376521825790405 \n",
            "Regression loss: 0.057249125093221664\n",
            "----------------\n",
            "Loss:  0.877704806625843 \n",
            "Classification loss: 1.6314423084259033 \n",
            "Regression loss: 0.06198365241289139\n",
            "----------------\n",
            "Loss:  1.127419799566269 \n",
            "Classification loss: 2.0842080116271973 \n",
            "Regression loss: 0.08531579375267029\n",
            "----------------\n",
            "Loss:  0.7928175404667854 \n",
            "Classification loss: 1.482444405555725 \n",
            "Regression loss: 0.05159533768892288\n",
            "----------------\n",
            "Loss:  0.8969375640153885 \n",
            "Classification loss: 1.6996515989303589 \n",
            "Regression loss: 0.047111764550209045\n",
            "----------------\n",
            "Loss:  0.8980217464268208 \n",
            "Classification loss: 1.7025293111801147 \n",
            "Regression loss: 0.04675709083676338\n",
            "----------------\n",
            "Loss:  0.9265041947364807 \n",
            "Classification loss: 1.7598862648010254 \n",
            "Regression loss: 0.04656106233596802\n",
            "----------------\n",
            "Loss:  0.9023724049329758 \n",
            "Classification loss: 1.6542384624481201 \n",
            "Regression loss: 0.07525317370891571\n",
            "----------------\n",
            "Loss:  0.9364615399390459 \n",
            "Classification loss: 1.8112163543701172 \n",
            "Regression loss: 0.030853362753987312\n",
            "----------------\n",
            "Loss:  0.9014249071478844 \n",
            "Classification loss: 1.683692455291748 \n",
            "Regression loss: 0.059578679502010345\n",
            "----------------\n",
            "Loss:  0.8537655137479305 \n",
            "Classification loss: 1.6197748184204102 \n",
            "Regression loss: 0.04387810453772545\n",
            "----------------\n",
            "Loss:  0.8515187725424767 \n",
            "Classification loss: 1.6393959522247314 \n",
            "Regression loss: 0.03182079643011093\n",
            "----------------\n",
            "Loss:  0.8702473938465118 \n",
            "Classification loss: 1.592454433441162 \n",
            "Regression loss: 0.07402017712593079\n",
            "----------------\n",
            "Loss:  0.9366470947861671 \n",
            "Classification loss: 1.790776014328003 \n",
            "Regression loss: 0.04125908762216568\n",
            "----------------\n",
            "Loss:  0.9040341675281525 \n",
            "Classification loss: 1.7441620826721191 \n",
            "Regression loss: 0.031953126192092896\n",
            "----------------\n",
            "Loss:  0.9407233856618404 \n",
            "Classification loss: 1.7779662609100342 \n",
            "Regression loss: 0.05174025520682335\n",
            "----------------\n",
            "Loss:  1.0281199179589748 \n",
            "Classification loss: 1.9454221725463867 \n",
            "Regression loss: 0.05540883168578148\n",
            "----------------\n",
            "Loss:  0.8700491189956665 \n",
            "Classification loss: 1.6339998245239258 \n",
            "Regression loss: 0.05304920673370361\n",
            "----------------\n",
            "Loss:  0.9572208262979984 \n",
            "Classification loss: 1.8231760263442993 \n",
            "Regression loss: 0.04563281312584877\n",
            "----------------\n",
            "Loss:  1.0240614600479603 \n",
            "Classification loss: 1.981784701347351 \n",
            "Regression loss: 0.033169109374284744\n",
            "----------------\n",
            "Loss:  0.9408207088708878 \n",
            "Classification loss: 1.8008815050125122 \n",
            "Regression loss: 0.04037995636463165\n",
            "----------------\n",
            "Loss:  0.9294173493981361 \n",
            "Classification loss: 1.776302456855774 \n",
            "Regression loss: 0.041266120970249176\n",
            "----------------\n",
            "Loss:  0.98368089646101 \n",
            "Classification loss: 1.8771426677703857 \n",
            "Regression loss: 0.04510956257581711\n",
            "----------------\n",
            "Loss:  0.9782872945070267 \n",
            "Classification loss: 1.8180707693099976 \n",
            "Regression loss: 0.0692519098520279\n",
            "----------------\n",
            "Loss:  0.842192754149437 \n",
            "Classification loss: 1.6030014753341675 \n",
            "Regression loss: 0.04069201648235321\n",
            "----------------\n",
            "Loss:  0.8771502245217562 \n",
            "Classification loss: 1.7098937034606934 \n",
            "Regression loss: 0.022203372791409492\n",
            "----------------\n",
            "Loss:  1.1143380478024483 \n",
            "Classification loss: 2.1204726696014404 \n",
            "Regression loss: 0.05410171300172806\n",
            "----------------\n",
            "Loss:  0.9448806867003441 \n",
            "Classification loss: 1.761240005493164 \n",
            "Regression loss: 0.06426068395376205\n",
            "----------------\n",
            "Loss:  0.8793477490544319 \n",
            "Classification loss: 1.6212059259414673 \n",
            "Regression loss: 0.06874478608369827\n",
            "----------------\n",
            "Loss:  0.8846052810549736 \n",
            "Classification loss: 1.735416293144226 \n",
            "Regression loss: 0.016897134482860565\n",
            "----------------\n",
            "Loss:  0.9324462339282036 \n",
            "Classification loss: 1.7823381423950195 \n",
            "Regression loss: 0.04127716273069382\n",
            "----------------\n",
            "Loss:  0.8417422920465469 \n",
            "Classification loss: 1.5566502809524536 \n",
            "Regression loss: 0.06341715157032013\n",
            "----------------\n",
            "Loss:  0.8954634293913841 \n",
            "Classification loss: 1.6730546951293945 \n",
            "Regression loss: 0.05893608182668686\n",
            "----------------\n",
            "Loss:  0.8674508705735207 \n",
            "Classification loss: 1.623091220855713 \n",
            "Regression loss: 0.055905260145664215\n",
            "----------------\n",
            "Loss:  0.9587330706417561 \n",
            "Classification loss: 1.8134037256240845 \n",
            "Regression loss: 0.05203120782971382\n",
            "----------------\n",
            "Loss:  1.011811338365078 \n",
            "Classification loss: 1.900846242904663 \n",
            "Regression loss: 0.06138821691274643\n",
            "----------------\n",
            "Loss:  1.0199692323803902 \n",
            "Classification loss: 1.894045114517212 \n",
            "Regression loss: 0.07294667512178421\n",
            "----------------\n",
            "Loss:  0.7926185913383961 \n",
            "Classification loss: 1.4891812801361084 \n",
            "Regression loss: 0.04802795127034187\n",
            "----------------\n",
            "Loss:  0.9814698696136475 \n",
            "Classification loss: 1.838996410369873 \n",
            "Regression loss: 0.06197166442871094\n",
            "----------------\n",
            "Loss:  1.0391340553760529 \n",
            "Classification loss: 1.9346363544464111 \n",
            "Regression loss: 0.07181587815284729\n",
            "----------------\n",
            "Loss:  0.8775657042860985 \n",
            "Classification loss: 1.659703254699707 \n",
            "Regression loss: 0.047714076936244965\n",
            "----------------\n",
            "Loss:  1.0262268967926502 \n",
            "Classification loss: 1.930068016052246 \n",
            "Regression loss: 0.061192888766527176\n",
            "----------------\n",
            "Loss:  0.9137113355100155 \n",
            "Classification loss: 1.7438255548477173 \n",
            "Regression loss: 0.041798558086156845\n",
            "----------------\n",
            "Loss:  0.8803226128220558 \n",
            "Classification loss: 1.6822268962860107 \n",
            "Regression loss: 0.039209164679050446\n",
            "----------------\n",
            "Loss:  1.0029605850577354 \n",
            "Classification loss: 1.855837106704712 \n",
            "Regression loss: 0.07504203170537949\n",
            "----------------\n",
            "Loss:  1.0380357503890991 \n",
            "Classification loss: 1.9502427577972412 \n",
            "Regression loss: 0.06291437149047852\n",
            "----------------\n",
            "Loss:  0.9506082013249397 \n",
            "Classification loss: 1.8207752704620361 \n",
            "Regression loss: 0.04022056609392166\n",
            "----------------\n",
            "Loss:  0.9352829083800316 \n",
            "Classification loss: 1.7425410747528076 \n",
            "Regression loss: 0.06401237100362778\n",
            "----------------\n",
            "Loss:  0.990358017385006 \n",
            "Classification loss: 1.8722825050354004 \n",
            "Regression loss: 0.054216764867305756\n",
            "----------------\n",
            "Loss:  0.9819840639829636 \n",
            "Classification loss: 1.821608066558838 \n",
            "Regression loss: 0.07118003070354462\n",
            "----------------\n",
            "Loss:  1.0028681606054306 \n",
            "Classification loss: 1.873159408569336 \n",
            "Regression loss: 0.06628845632076263\n",
            "----------------\n",
            "Loss:  0.8335339576005936 \n",
            "Classification loss: 1.5876497030258179 \n",
            "Regression loss: 0.03970910608768463\n",
            "----------------\n",
            "Loss:  0.9041404500603676 \n",
            "Classification loss: 1.6794962882995605 \n",
            "Regression loss: 0.06439230591058731\n",
            "----------------\n",
            "Loss:  1.0834891125559807 \n",
            "Classification loss: 2.0459718704223633 \n",
            "Regression loss: 0.06050317734479904\n",
            "----------------\n",
            "Loss:  0.9484217204153538 \n",
            "Classification loss: 1.7725610733032227 \n",
            "Regression loss: 0.06214118376374245\n",
            "----------------\n",
            "Loss:  0.8821511305868626 \n",
            "Classification loss: 1.6845496892929077 \n",
            "Regression loss: 0.03987628594040871\n",
            "----------------\n",
            "Loss:  1.0367866903543472 \n",
            "Classification loss: 1.947835922241211 \n",
            "Regression loss: 0.06286872923374176\n",
            "----------------\n",
            "Loss:  0.9003378599882126 \n",
            "Classification loss: 1.719122290611267 \n",
            "Regression loss: 0.04077671468257904\n",
            "----------------\n",
            "Loss:  1.0042247623205185 \n",
            "Classification loss: 1.8850548267364502 \n",
            "Regression loss: 0.061697348952293396\n",
            "----------------\n",
            "Loss:  0.898422259837389 \n",
            "Classification loss: 1.7339954376220703 \n",
            "Regression loss: 0.031424541026353836\n",
            "----------------\n",
            "Loss:  0.9935449287295341 \n",
            "Classification loss: 1.8678134679794312 \n",
            "Regression loss: 0.05963819473981857\n",
            "----------------\n",
            "Loss:  0.8708357289433479 \n",
            "Classification loss: 1.6353627443313599 \n",
            "Regression loss: 0.053154356777668\n",
            "----------------\n",
            "Loss:  0.811719935387373 \n",
            "Classification loss: 1.5489239692687988 \n",
            "Regression loss: 0.037257950752973557\n",
            "----------------\n",
            "Loss:  1.0246657878160477 \n",
            "Classification loss: 1.8893332481384277 \n",
            "Regression loss: 0.0799991637468338\n",
            "----------------\n",
            "Loss:  0.9964518994092941 \n",
            "Classification loss: 1.899404525756836 \n",
            "Regression loss: 0.04674963653087616\n",
            "----------------\n",
            "Loss:  0.9100889414548874 \n",
            "Classification loss: 1.7154850959777832 \n",
            "Regression loss: 0.05234639346599579\n",
            "----------------\n",
            "Loss:  0.9390924870967865 \n",
            "Classification loss: 1.7945950031280518 \n",
            "Regression loss: 0.04179498553276062\n",
            "----------------\n",
            "Loss:  0.9813824258744717 \n",
            "Classification loss: 1.8473150730133057 \n",
            "Regression loss: 0.05772488936781883\n",
            "----------------\n",
            "Loss:  0.9163196533918381 \n",
            "Classification loss: 1.7056398391723633 \n",
            "Regression loss: 0.06349973380565643\n",
            "----------------\n",
            "Loss:  1.01472257822752 \n",
            "Classification loss: 1.9362561702728271 \n",
            "Regression loss: 0.046594493091106415\n",
            "----------------\n",
            "Loss:  0.9899359531700611 \n",
            "Classification loss: 1.8861029148101807 \n",
            "Regression loss: 0.04688449576497078\n",
            "----------------\n",
            "Loss:  0.8731057569384575 \n",
            "Classification loss: 1.6328731775283813 \n",
            "Regression loss: 0.056669168174266815\n",
            "----------------\n",
            "Loss:  0.9367791637778282 \n",
            "Classification loss: 1.790949821472168 \n",
            "Regression loss: 0.04130425304174423\n",
            "----------------\n",
            "Loss:  0.9569225460290909 \n",
            "Classification loss: 1.7803953886032104 \n",
            "Regression loss: 0.06672485172748566\n",
            "----------------\n",
            "Loss:  0.9472316205501556 \n",
            "Classification loss: 1.7538092136383057 \n",
            "Regression loss: 0.07032701373100281\n",
            "----------------\n",
            "Loss:  1.0183456167578697 \n",
            "Classification loss: 1.8580979108810425 \n",
            "Regression loss: 0.08929666131734848\n",
            "----------------\n",
            "Loss:  1.050850197672844 \n",
            "Classification loss: 1.9989748001098633 \n",
            "Regression loss: 0.05136279761791229\n",
            "----------------\n",
            "Loss:  0.9127045683562756 \n",
            "Classification loss: 1.71494460105896 \n",
            "Regression loss: 0.05523226782679558\n",
            "----------------\n",
            "Loss:  0.9443013072013855 \n",
            "Classification loss: 1.754177451133728 \n",
            "Regression loss: 0.06721258163452148\n",
            "----------------\n",
            "Loss:  0.868822630494833 \n",
            "Classification loss: 1.6139450073242188 \n",
            "Regression loss: 0.06185012683272362\n",
            "----------------\n",
            "Loss:  0.7931069359183311 \n",
            "Classification loss: 1.5108641386032104 \n",
            "Regression loss: 0.03767486661672592\n",
            "----------------\n",
            "Loss:  0.9233496934175491 \n",
            "Classification loss: 1.6834357976913452 \n",
            "Regression loss: 0.08163179457187653\n",
            "----------------\n",
            "Loss:  0.8812870159745216 \n",
            "Classification loss: 1.6683393716812134 \n",
            "Regression loss: 0.04711733013391495\n",
            "----------------\n",
            "Loss:  0.8643607720732689 \n",
            "Classification loss: 1.654618501663208 \n",
            "Regression loss: 0.037051521241664886\n",
            "----------------\n",
            "Loss:  0.9882848113775253 \n",
            "Classification loss: 1.8410639762878418 \n",
            "Regression loss: 0.06775282323360443\n",
            "----------------\n",
            "Loss:  0.9726662151515484 \n",
            "Classification loss: 1.8677754402160645 \n",
            "Regression loss: 0.03877849504351616\n",
            "----------------\n",
            "Loss:  0.9064957238733768 \n",
            "Classification loss: 1.7095253467559814 \n",
            "Regression loss: 0.051733050495386124\n",
            "----------------\n",
            "Loss:  0.935670405626297 \n",
            "Classification loss: 1.7734102010726929 \n",
            "Regression loss: 0.04896530508995056\n",
            "----------------\n",
            "Loss:  0.9301005229353905 \n",
            "Classification loss: 1.783549427986145 \n",
            "Regression loss: 0.03832580894231796\n",
            "----------------\n",
            "Loss:  0.8551000095903873 \n",
            "Classification loss: 1.6479241847991943 \n",
            "Regression loss: 0.031137917190790176\n",
            "----------------\n",
            "Loss:  0.9210797846317291 \n",
            "Classification loss: 1.7432975769042969 \n",
            "Regression loss: 0.04943099617958069\n",
            "----------------\n",
            "Loss:  0.9892324917018414 \n",
            "Classification loss: 1.8730669021606445 \n",
            "Regression loss: 0.05269904062151909\n",
            "----------------\n",
            "Loss:  0.7933001592755318 \n",
            "Classification loss: 1.4959464073181152 \n",
            "Regression loss: 0.04532695561647415\n",
            "----------------\n",
            "Loss:  0.9111501723527908 \n",
            "Classification loss: 1.7342257499694824 \n",
            "Regression loss: 0.04403729736804962\n",
            "----------------\n",
            "Loss:  0.9059405103325844 \n",
            "Classification loss: 1.6647958755493164 \n",
            "Regression loss: 0.07354257255792618\n",
            "----------------\n",
            "Loss:  0.9813250154256821 \n",
            "Classification loss: 1.8764705657958984 \n",
            "Regression loss: 0.04308973252773285\n",
            "----------------\n",
            "Loss:  0.9700696393847466 \n",
            "Classification loss: 1.7918076515197754 \n",
            "Regression loss: 0.07416581362485886\n",
            "----------------\n",
            "Loss:  0.8755720108747482 \n",
            "Classification loss: 1.634248971939087 \n",
            "Regression loss: 0.05844752490520477\n",
            "----------------\n",
            "Loss:  0.9181619808077812 \n",
            "Classification loss: 1.763273000717163 \n",
            "Regression loss: 0.036525480449199677\n",
            "----------------\n",
            "Loss:  0.9386539161205292 \n",
            "Classification loss: 1.7295799255371094 \n",
            "Regression loss: 0.07386395335197449\n",
            "----------------\n",
            "Loss:  0.8813967667520046 \n",
            "Classification loss: 1.6527693271636963 \n",
            "Regression loss: 0.05501210317015648\n",
            "----------------\n",
            "Loss:  0.914020225405693 \n",
            "Classification loss: 1.7020820379257202 \n",
            "Regression loss: 0.06297920644283295\n",
            "----------------\n",
            "Loss:  0.8131365776062012 \n",
            "Classification loss: 1.523263692855835 \n",
            "Regression loss: 0.05150473117828369\n",
            "----------------\n",
            "Loss:  0.9683531150221825 \n",
            "Classification loss: 1.8174350261688232 \n",
            "Regression loss: 0.059635601937770844\n",
            "----------------\n",
            "Loss:  0.9360263384878635 \n",
            "Classification loss: 1.7624130249023438 \n",
            "Regression loss: 0.054819826036691666\n",
            "----------------\n",
            "Loss:  0.9353289417922497 \n",
            "Classification loss: 1.7766342163085938 \n",
            "Regression loss: 0.047011833637952805\n",
            "----------------\n",
            "Loss:  0.9104055166244507 \n",
            "Classification loss: 1.659998893737793 \n",
            "Regression loss: 0.0804060697555542\n",
            "----------------\n",
            "Loss:  0.9855501167476177 \n",
            "Classification loss: 1.8622369766235352 \n",
            "Regression loss: 0.05443162843585014\n",
            "----------------\n",
            "Loss:  0.9955099523067474 \n",
            "Classification loss: 1.821474313735962 \n",
            "Regression loss: 0.08477279543876648\n",
            "----------------\n",
            "Loss:  0.8489124961197376 \n",
            "Classification loss: 1.6256515979766846 \n",
            "Regression loss: 0.03608669713139534\n",
            "----------------\n",
            "Loss:  0.8430680446326733 \n",
            "Classification loss: 1.6149861812591553 \n",
            "Regression loss: 0.03557495400309563\n",
            "----------------\n",
            "Loss:  0.9512596763670444 \n",
            "Classification loss: 1.8211948871612549 \n",
            "Regression loss: 0.04066223278641701\n",
            "----------------\n",
            "Loss:  0.9274648278951645 \n",
            "Classification loss: 1.752246379852295 \n",
            "Regression loss: 0.05134163796901703\n",
            "----------------\n",
            "Loss:  0.9091278985142708 \n",
            "Classification loss: 1.707033395767212 \n",
            "Regression loss: 0.055611200630664825\n",
            "----------------\n",
            "Loss:  0.9418445602059364 \n",
            "Classification loss: 1.741724967956543 \n",
            "Regression loss: 0.07098207622766495\n",
            "----------------\n",
            "Loss:  0.9140933156013489 \n",
            "Classification loss: 1.7216328382492065 \n",
            "Regression loss: 0.053276896476745605\n",
            "----------------\n",
            "Loss:  0.8517346754670143 \n",
            "Classification loss: 1.632570505142212 \n",
            "Regression loss: 0.035449422895908356\n",
            "----------------\n",
            "Loss:  0.9830754809081554 \n",
            "Classification loss: 1.872917890548706 \n",
            "Regression loss: 0.046616535633802414\n",
            "----------------\n",
            "Loss:  0.8869397044181824 \n",
            "Classification loss: 1.6897274255752563 \n",
            "Regression loss: 0.0420759916305542\n",
            "----------------\n",
            "Loss:  1.0040455125272274 \n",
            "Classification loss: 1.8881661891937256 \n",
            "Regression loss: 0.05996241793036461\n",
            "----------------\n",
            "Loss:  0.8729253560304642 \n",
            "Classification loss: 1.6313543319702148 \n",
            "Regression loss: 0.05724819004535675\n",
            "----------------\n",
            "Loss:  0.9139292053878307 \n",
            "Classification loss: 1.728050947189331 \n",
            "Regression loss: 0.04990373179316521\n",
            "----------------\n",
            "Loss:  0.8810812905430794 \n",
            "Classification loss: 1.6519850492477417 \n",
            "Regression loss: 0.05508876591920853\n",
            "----------------\n",
            "Loss:  0.8847483545541763 \n",
            "Classification loss: 1.6838109493255615 \n",
            "Regression loss: 0.04284287989139557\n",
            "----------------\n",
            "Loss:  0.86312610283494 \n",
            "Classification loss: 1.6357421875 \n",
            "Regression loss: 0.04525500908493996\n",
            "----------------\n",
            "Loss:  0.9230971299111843 \n",
            "Classification loss: 1.776409387588501 \n",
            "Regression loss: 0.03489243611693382\n",
            "----------------\n",
            "Loss:  0.8887492530047894 \n",
            "Classification loss: 1.7020578384399414 \n",
            "Regression loss: 0.03772033378481865\n",
            "----------------\n",
            "Loss:  0.9005667455494404 \n",
            "Classification loss: 1.7126966714859009 \n",
            "Regression loss: 0.044218409806489944\n",
            "----------------\n",
            "Loss:  0.941067524254322 \n",
            "Classification loss: 1.800560712814331 \n",
            "Regression loss: 0.040787167847156525\n",
            "----------------\n",
            "Loss:  0.8933660332113504 \n",
            "Classification loss: 1.7309281826019287 \n",
            "Regression loss: 0.027901941910386086\n",
            "----------------\n",
            "Loss:  0.8525962308049202 \n",
            "Classification loss: 1.6235212087631226 \n",
            "Regression loss: 0.04083562642335892\n",
            "----------------\n",
            "Loss:  0.9376894608139992 \n",
            "Classification loss: 1.7526118755340576 \n",
            "Regression loss: 0.06138352304697037\n",
            "----------------\n",
            "Loss:  0.8874683044850826 \n",
            "Classification loss: 1.6871724128723145 \n",
            "Regression loss: 0.0438820980489254\n",
            "----------------\n",
            "Loss:  0.8545719683170319 \n",
            "Classification loss: 1.6250483989715576 \n",
            "Regression loss: 0.04204776883125305\n",
            "----------------\n",
            "Loss:  0.8941890895366669 \n",
            "Classification loss: 1.6773841381072998 \n",
            "Regression loss: 0.05549702048301697\n",
            "----------------\n",
            "Loss:  0.887611472979188 \n",
            "Classification loss: 1.726635217666626 \n",
            "Regression loss: 0.024293864145874977\n",
            "----------------\n",
            "Loss:  0.8332029208540916 \n",
            "Classification loss: 1.5904786586761475 \n",
            "Regression loss: 0.037963591516017914\n",
            "----------------\n",
            "Loss:  0.9301117621362209 \n",
            "Classification loss: 1.7806711196899414 \n",
            "Regression loss: 0.03977620229125023\n",
            "----------------\n",
            "Loss:  0.7424478717148304 \n",
            "Classification loss: 1.424729585647583 \n",
            "Regression loss: 0.030083078891038895\n",
            "----------------\n",
            "Loss:  0.9547058530151844 \n",
            "Classification loss: 1.8083873987197876 \n",
            "Regression loss: 0.050512153655290604\n",
            "----------------\n",
            "Loss:  0.8469244949519634 \n",
            "Classification loss: 1.626915693283081 \n",
            "Regression loss: 0.0334666483104229\n",
            "----------------\n",
            "Loss:  0.936088390648365 \n",
            "Classification loss: 1.698486566543579 \n",
            "Regression loss: 0.08684510737657547\n",
            "----------------\n",
            "Loss:  0.8386973738670349 \n",
            "Classification loss: 1.5926936864852905 \n",
            "Regression loss: 0.04235053062438965\n",
            "----------------\n",
            "Loss:  0.9238077476620674 \n",
            "Classification loss: 1.676975131034851 \n",
            "Regression loss: 0.08532018214464188\n",
            "----------------\n",
            "Loss:  0.8810473866760731 \n",
            "Classification loss: 1.6428332328796387 \n",
            "Regression loss: 0.05963077023625374\n",
            "----------------\n",
            "Loss:  0.8176322635263205 \n",
            "Classification loss: 1.573737621307373 \n",
            "Regression loss: 0.030763452872633934\n",
            "----------------\n",
            "Loss:  0.9495022594928741 \n",
            "Classification loss: 1.7928112745285034 \n",
            "Regression loss: 0.053096622228622437\n",
            "----------------\n",
            "Loss:  0.8486701808869839 \n",
            "Classification loss: 1.5777469873428345 \n",
            "Regression loss: 0.059796687215566635\n",
            "----------------\n",
            "Loss:  0.7775337398052216 \n",
            "Classification loss: 1.473376750946045 \n",
            "Regression loss: 0.0408453643321991\n",
            "----------------\n",
            "Loss:  0.776481170207262 \n",
            "Classification loss: 1.466932773590088 \n",
            "Regression loss: 0.043014783412218094\n",
            "----------------\n",
            "Loss:  0.8507202751934528 \n",
            "Classification loss: 1.6003389358520508 \n",
            "Regression loss: 0.050550807267427444\n",
            "----------------\n",
            "Loss:  0.9269410483539104 \n",
            "Classification loss: 1.759545087814331 \n",
            "Regression loss: 0.04716850444674492\n",
            "----------------\n",
            "Loss:  0.8653887398540974 \n",
            "Classification loss: 1.6248970031738281 \n",
            "Regression loss: 0.052940238267183304\n",
            "----------------\n",
            "Loss:  0.9535059407353401 \n",
            "Classification loss: 1.7879189252853394 \n",
            "Regression loss: 0.05954647809267044\n",
            "----------------\n",
            "Loss:  0.8876095339655876 \n",
            "Classification loss: 1.7050434350967407 \n",
            "Regression loss: 0.035087816417217255\n",
            "----------------\n",
            "Loss:  0.8893178887665272 \n",
            "Classification loss: 1.6757309436798096 \n",
            "Regression loss: 0.05145241692662239\n",
            "----------------\n",
            "Loss:  0.8244761973619461 \n",
            "Classification loss: 1.5614221096038818 \n",
            "Regression loss: 0.04376514256000519\n",
            "----------------\n",
            "Loss:  0.8385846596211195 \n",
            "Classification loss: 1.61501145362854 \n",
            "Regression loss: 0.03107893280684948\n",
            "----------------\n",
            "Loss:  0.8432682454586029 \n",
            "Classification loss: 1.5966551303863525 \n",
            "Regression loss: 0.044940680265426636\n",
            "----------------\n",
            "Loss:  0.8687207624316216 \n",
            "Classification loss: 1.6783806085586548 \n",
            "Regression loss: 0.02953045815229416\n",
            "----------------\n",
            "Loss:  0.9413892030715942 \n",
            "Classification loss: 1.8045415878295898 \n",
            "Regression loss: 0.039118409156799316\n",
            "----------------\n",
            "Loss:  0.9534952417016029 \n",
            "Classification loss: 1.8207135200500488 \n",
            "Regression loss: 0.04313848167657852\n",
            "----------------\n",
            "Loss:  0.780603900551796 \n",
            "Classification loss: 1.4517905712127686 \n",
            "Regression loss: 0.05470861494541168\n",
            "----------------\n",
            "Loss:  0.8563050888478756 \n",
            "Classification loss: 1.6217854022979736 \n",
            "Regression loss: 0.04541238769888878\n",
            "----------------\n",
            "Loss:  0.9633201286196709 \n",
            "Classification loss: 1.828383207321167 \n",
            "Regression loss: 0.04912852495908737\n",
            "----------------\n",
            "Loss:  0.9177795574069023 \n",
            "Classification loss: 1.703842043876648 \n",
            "Regression loss: 0.06585853546857834\n",
            "----------------\n",
            "Loss:  0.9304035417735577 \n",
            "Classification loss: 1.777162790298462 \n",
            "Regression loss: 0.041822146624326706\n",
            "----------------\n",
            "Loss:  0.8903669193387032 \n",
            "Classification loss: 1.708483338356018 \n",
            "Regression loss: 0.03612525016069412\n",
            "----------------\n",
            "Loss:  0.8952697217464447 \n",
            "Classification loss: 1.6272233724594116 \n",
            "Regression loss: 0.08165803551673889\n",
            "----------------\n",
            "Loss:  0.855861097574234 \n",
            "Classification loss: 1.6172616481781006 \n",
            "Regression loss: 0.047230273485183716\n",
            "----------------\n",
            "Loss:  0.9080454930663109 \n",
            "Classification loss: 1.6966941356658936 \n",
            "Regression loss: 0.059698425233364105\n",
            "----------------\n",
            "Loss:  0.9071558266878128 \n",
            "Classification loss: 1.687748908996582 \n",
            "Regression loss: 0.06328137218952179\n",
            "----------------\n",
            "Loss:  1.0693341493606567 \n",
            "Classification loss: 2.055387020111084 \n",
            "Regression loss: 0.041640639305114746\n",
            "----------------\n",
            "Loss:  1.038340825587511 \n",
            "Classification loss: 1.9777681827545166 \n",
            "Regression loss: 0.04945673421025276\n",
            "----------------\n",
            "Loss:  0.9043347463011742 \n",
            "Classification loss: 1.6943066120147705 \n",
            "Regression loss: 0.05718144029378891\n",
            "----------------\n",
            "Loss:  0.8643775284290314 \n",
            "Classification loss: 1.6349928379058838 \n",
            "Regression loss: 0.04688110947608948\n",
            "----------------\n",
            "Loss:  0.9793776571750641 \n",
            "Classification loss: 1.841587781906128 \n",
            "Regression loss: 0.05858376622200012\n",
            "----------------\n",
            "Loss:  0.9342405274510384 \n",
            "Classification loss: 1.7483503818511963 \n",
            "Regression loss: 0.060065336525440216\n",
            "----------------\n",
            "Loss:  1.035729181021452 \n",
            "Classification loss: 1.9725873470306396 \n",
            "Regression loss: 0.049435507506132126\n",
            "----------------\n",
            "Loss:  0.8058998994529247 \n",
            "Classification loss: 1.5185641050338745 \n",
            "Regression loss: 0.04661784693598747\n",
            "----------------\n",
            "Loss:  0.9399920366704464 \n",
            "Classification loss: 1.801889181137085 \n",
            "Regression loss: 0.039047446101903915\n",
            "----------------\n",
            "Loss:  0.9406498074531555 \n",
            "Classification loss: 1.7879307270050049 \n",
            "Regression loss: 0.046684443950653076\n",
            "----------------\n",
            "Loss:  0.9179397448897362 \n",
            "Classification loss: 1.7153074741363525 \n",
            "Regression loss: 0.060286007821559906\n",
            "----------------\n",
            "Loss:  0.8958346620202065 \n",
            "Classification loss: 1.6502821445465088 \n",
            "Regression loss: 0.07069358974695206\n",
            "----------------\n",
            "Loss:  0.9435483366250992 \n",
            "Classification loss: 1.8154988288879395 \n",
            "Regression loss: 0.035798922181129456\n",
            "----------------\n",
            "Loss:  0.9386920481920242 \n",
            "Classification loss: 1.8067086935043335 \n",
            "Regression loss: 0.03533770143985748\n",
            "----------------\n",
            "Loss:  1.0189870968461037 \n",
            "Classification loss: 1.8734028339385986 \n",
            "Regression loss: 0.08228567987680435\n",
            "----------------\n",
            "Loss:  0.9967217147350311 \n",
            "Classification loss: 1.8323748111724854 \n",
            "Regression loss: 0.08053430914878845\n",
            "----------------\n",
            "Loss:  0.9426145181059837 \n",
            "Classification loss: 1.7890708446502686 \n",
            "Regression loss: 0.04807909578084946\n",
            "----------------\n",
            "Loss:  0.9278347790241241 \n",
            "Classification loss: 1.7597295045852661 \n",
            "Regression loss: 0.04797002673149109\n",
            "----------------\n",
            "Loss:  0.9244863167405128 \n",
            "Classification loss: 1.728368878364563 \n",
            "Regression loss: 0.060301877558231354\n",
            "----------------\n",
            "Loss:  0.9041749574244022 \n",
            "Classification loss: 1.7229938507080078 \n",
            "Regression loss: 0.04267803207039833\n",
            "----------------\n",
            "Loss:  0.9536665268242359 \n",
            "Classification loss: 1.8361849784851074 \n",
            "Regression loss: 0.035574037581682205\n",
            "----------------\n",
            "Loss:  0.9188361763954163 \n",
            "Classification loss: 1.712447166442871 \n",
            "Regression loss: 0.06261259317398071\n",
            "----------------\n",
            "Loss:  0.978903666138649 \n",
            "Classification loss: 1.8665528297424316 \n",
            "Regression loss: 0.045627251267433167\n",
            "----------------\n",
            "Loss:  1.0070173889398575 \n",
            "Classification loss: 1.8575234413146973 \n",
            "Regression loss: 0.07825566828250885\n",
            "----------------\n",
            "Loss:  0.9832387566566467 \n",
            "Classification loss: 1.8695886135101318 \n",
            "Regression loss: 0.04844444990158081\n",
            "----------------\n",
            "Loss:  0.9587535820901394 \n",
            "Classification loss: 1.8206208944320679 \n",
            "Regression loss: 0.048443134874105453\n",
            "----------------\n",
            "Loss:  0.808329364284873 \n",
            "Classification loss: 1.5718358755111694 \n",
            "Regression loss: 0.022411426529288292\n",
            "----------------\n",
            "Loss:  0.9613145738840103 \n",
            "Classification loss: 1.782131314277649 \n",
            "Regression loss: 0.07024891674518585\n",
            "----------------\n",
            "Loss:  0.9220965132117271 \n",
            "Classification loss: 1.7174187898635864 \n",
            "Regression loss: 0.06338711827993393\n",
            "----------------\n",
            "Loss:  0.911712970584631 \n",
            "Classification loss: 1.7172069549560547 \n",
            "Regression loss: 0.05310949310660362\n",
            "----------------\n",
            "Loss:  0.9196288455277681 \n",
            "Classification loss: 1.7806637287139893 \n",
            "Regression loss: 0.029296981170773506\n",
            "----------------\n",
            "Loss:  0.877020351588726 \n",
            "Classification loss: 1.6780009269714355 \n",
            "Regression loss: 0.03801988810300827\n",
            "----------------\n",
            "Loss:  0.983436644077301 \n",
            "Classification loss: 1.8763742446899414 \n",
            "Regression loss: 0.04524952173233032\n",
            "----------------\n",
            "Loss:  0.7602261938154697 \n",
            "Classification loss: 1.4476161003112793 \n",
            "Regression loss: 0.03641814365983009\n",
            "----------------\n",
            "Loss:  0.9843560233712196 \n",
            "Classification loss: 1.8599005937576294 \n",
            "Regression loss: 0.05440572649240494\n",
            "----------------\n",
            "Loss:  0.8745955750346184 \n",
            "Classification loss: 1.5916911363601685 \n",
            "Regression loss: 0.07875000685453415\n",
            "----------------\n",
            "Loss:  0.9493267349898815 \n",
            "Classification loss: 1.7918405532836914 \n",
            "Regression loss: 0.05340645834803581\n",
            "----------------\n",
            "Loss:  0.9177203923463821 \n",
            "Classification loss: 1.6974742412567139 \n",
            "Regression loss: 0.06898327171802521\n",
            "----------------\n",
            "Loss:  0.9055673405528069 \n",
            "Classification loss: 1.7080585956573486 \n",
            "Regression loss: 0.05153804272413254\n",
            "----------------\n",
            "Loss:  0.8973704017698765 \n",
            "Classification loss: 1.731374979019165 \n",
            "Regression loss: 0.03168291226029396\n",
            "----------------\n",
            "Loss:  1.0269744545221329 \n",
            "Classification loss: 1.930108666419983 \n",
            "Regression loss: 0.06192012131214142\n",
            "----------------\n",
            "Loss:  0.9299971833825111 \n",
            "Classification loss: 1.7080237865447998 \n",
            "Regression loss: 0.07598529011011124\n",
            "----------------\n",
            "Loss:  0.8895211704075336 \n",
            "Classification loss: 1.697642207145691 \n",
            "Regression loss: 0.04070006683468819\n",
            "----------------\n",
            "Loss:  0.8616114836186171 \n",
            "Classification loss: 1.669107437133789 \n",
            "Regression loss: 0.027057765051722527\n",
            "----------------\n",
            "Loss:  0.9337843656539917 \n",
            "Classification loss: 1.782282829284668 \n",
            "Regression loss: 0.042642951011657715\n",
            "----------------\n",
            "Loss:  0.9005156010389328 \n",
            "Classification loss: 1.6850695610046387 \n",
            "Regression loss: 0.057980820536613464\n",
            "----------------\n",
            "Loss:  0.856656190007925 \n",
            "Classification loss: 1.6489020586013794 \n",
            "Regression loss: 0.032205160707235336\n",
            "----------------\n",
            "Loss:  0.8557911142706871 \n",
            "Classification loss: 1.6265792846679688 \n",
            "Regression loss: 0.04250147193670273\n",
            "----------------\n",
            "Loss:  1.0146199762821198 \n",
            "Classification loss: 1.9026079177856445 \n",
            "Regression loss: 0.06331601738929749\n",
            "----------------\n",
            "Loss:  0.8982384502887726 \n",
            "Classification loss: 1.6726245880126953 \n",
            "Regression loss: 0.06192615628242493\n",
            "----------------\n",
            "Loss:  0.9356627985835075 \n",
            "Classification loss: 1.754197120666504 \n",
            "Regression loss: 0.058564238250255585\n",
            "----------------\n",
            "Loss:  0.9194353818893433 \n",
            "Classification loss: 1.7033498287200928 \n",
            "Regression loss: 0.06776046752929688\n",
            "----------------\n",
            "Loss:  0.9373628385365009 \n",
            "Classification loss: 1.7519071102142334 \n",
            "Regression loss: 0.06140928342938423\n",
            "----------------\n",
            "Loss:  0.9047331288456917 \n",
            "Classification loss: 1.6940083503723145 \n",
            "Regression loss: 0.057728953659534454\n",
            "----------------\n",
            "Loss:  0.9275958687067032 \n",
            "Classification loss: 1.704599380493164 \n",
            "Regression loss: 0.07529617846012115\n",
            "----------------\n",
            "Loss:  0.8096181079745293 \n",
            "Classification loss: 1.5184355974197388 \n",
            "Regression loss: 0.05040030926465988\n",
            "----------------\n",
            "Loss:  0.870665155351162 \n",
            "Classification loss: 1.609305739402771 \n",
            "Regression loss: 0.06601228564977646\n",
            "----------------\n",
            "Loss:  0.8358335494995117 \n",
            "Classification loss: 1.602189064025879 \n",
            "Regression loss: 0.034739017486572266\n",
            "----------------\n",
            "Loss:  0.8965258151292801 \n",
            "Classification loss: 1.6992781162261963 \n",
            "Regression loss: 0.046886757016181946\n",
            "----------------\n",
            "Loss:  1.0136032775044441 \n",
            "Classification loss: 1.8885622024536133 \n",
            "Regression loss: 0.06932217627763748\n",
            "----------------\n",
            "Loss:  0.9813658893108368 \n",
            "Classification loss: 1.8713937997817993 \n",
            "Regression loss: 0.045668989419937134\n",
            "----------------\n",
            "Loss:  1.0172936655580997 \n",
            "Classification loss: 1.9490580558776855 \n",
            "Regression loss: 0.04276463761925697\n",
            "----------------\n",
            "Loss:  0.897656973451376 \n",
            "Classification loss: 1.7180304527282715 \n",
            "Regression loss: 0.03864174708724022\n",
            "----------------\n",
            "Loss:  0.9104031398892403 \n",
            "Classification loss: 1.666830062866211 \n",
            "Regression loss: 0.0769881084561348\n",
            "----------------\n",
            "Loss:  0.8279287777841091 \n",
            "Classification loss: 1.573867678642273 \n",
            "Regression loss: 0.04099493846297264\n",
            "----------------\n",
            "Loss:  0.8899333029985428 \n",
            "Classification loss: 1.676492691040039 \n",
            "Regression loss: 0.051686957478523254\n",
            "----------------\n",
            "Loss:  0.9219872206449509 \n",
            "Classification loss: 1.7622222900390625 \n",
            "Regression loss: 0.04087607562541962\n",
            "----------------\n",
            "Loss:  0.9983188807964325 \n",
            "Classification loss: 1.8833732604980469 \n",
            "Regression loss: 0.05663225054740906\n",
            "----------------\n",
            "Loss:  0.8524977639317513 \n",
            "Classification loss: 1.6254727840423584 \n",
            "Regression loss: 0.03976137191057205\n",
            "----------------\n",
            "Loss:  0.9596570990979671 \n",
            "Classification loss: 1.8007533550262451 \n",
            "Regression loss: 0.05928042158484459\n",
            "----------------\n",
            "Loss:  0.9421252533793449 \n",
            "Classification loss: 1.7231591939926147 \n",
            "Regression loss: 0.08054565638303757\n",
            "----------------\n",
            "Loss:  0.7982354499399662 \n",
            "Classification loss: 1.519107699394226 \n",
            "Regression loss: 0.038681600242853165\n",
            "----------------\n",
            "Loss:  0.9150027930736542 \n",
            "Classification loss: 1.6927469968795776 \n",
            "Regression loss: 0.06862929463386536\n",
            "----------------\n",
            "Loss:  0.9365821182727814 \n",
            "Classification loss: 1.7608451843261719 \n",
            "Regression loss: 0.056159526109695435\n",
            "----------------\n",
            "Loss:  0.9209306016564369 \n",
            "Classification loss: 1.759455919265747 \n",
            "Regression loss: 0.041202642023563385\n",
            "----------------\n",
            "Loss:  0.8998524248600006 \n",
            "Classification loss: 1.6799002885818481 \n",
            "Regression loss: 0.05990228056907654\n",
            "----------------\n",
            "Loss:  0.7979647666215897 \n",
            "Classification loss: 1.487764596939087 \n",
            "Regression loss: 0.054082468152046204\n",
            "----------------\n",
            "Loss:  0.8345332406461239 \n",
            "Classification loss: 1.5759996175765991 \n",
            "Regression loss: 0.046533431857824326\n",
            "----------------\n",
            "Loss:  0.9140279926359653 \n",
            "Classification loss: 1.7101271152496338 \n",
            "Regression loss: 0.05896443501114845\n",
            "----------------\n",
            "Loss:  0.8305448070168495 \n",
            "Classification loss: 1.589011788368225 \n",
            "Regression loss: 0.03603891283273697\n",
            "----------------\n",
            "Loss:  1.0071947798132896 \n",
            "Classification loss: 1.9326164722442627 \n",
            "Regression loss: 0.040886543691158295\n",
            "----------------\n",
            "Loss:  0.9155974127352238 \n",
            "Classification loss: 1.7111144065856934 \n",
            "Regression loss: 0.06004020944237709\n",
            "----------------\n",
            "Loss:  0.8851064369082451 \n",
            "Classification loss: 1.6609493494033813 \n",
            "Regression loss: 0.05463176220655441\n",
            "----------------\n",
            "Loss:  0.9285000748932362 \n",
            "Classification loss: 1.7593810558319092 \n",
            "Regression loss: 0.04880954697728157\n",
            "----------------\n",
            "Loss:  0.8189516365528107 \n",
            "Classification loss: 1.510435938835144 \n",
            "Regression loss: 0.06373366713523865\n",
            "----------------\n",
            "Loss:  0.8883969783782959 \n",
            "Classification loss: 1.643103837966919 \n",
            "Regression loss: 0.06684505939483643\n",
            "----------------\n",
            "Loss:  0.8259238079190254 \n",
            "Classification loss: 1.5395448207855225 \n",
            "Regression loss: 0.05615139752626419\n",
            "----------------\n",
            "Loss:  0.9325856566429138 \n",
            "Classification loss: 1.7249462604522705 \n",
            "Regression loss: 0.07011252641677856\n",
            "----------------\n",
            "Loss:  0.8031090050935745 \n",
            "Classification loss: 1.512341022491455 \n",
            "Regression loss: 0.046938493847846985\n",
            "----------------\n",
            "Loss:  0.7868923097848892 \n",
            "Classification loss: 1.5021833181381226 \n",
            "Regression loss: 0.03580065071582794\n",
            "----------------\n",
            "Loss:  0.8649114966392517 \n",
            "Classification loss: 1.6003282070159912 \n",
            "Regression loss: 0.0647473931312561\n",
            "----------------\n",
            "Loss:  0.8255493901669979 \n",
            "Classification loss: 1.5372593402862549 \n",
            "Regression loss: 0.05691972002387047\n",
            "----------------\n",
            "Loss:  0.7882680520415306 \n",
            "Classification loss: 1.493438720703125 \n",
            "Regression loss: 0.04154869168996811\n",
            "----------------\n",
            "Loss:  0.9532648995518684 \n",
            "Classification loss: 1.7753406763076782 \n",
            "Regression loss: 0.06559456139802933\n",
            "----------------\n",
            "Loss:  0.7942542284727097 \n",
            "Classification loss: 1.4592323303222656 \n",
            "Regression loss: 0.06463806331157684\n",
            "----------------\n",
            "Loss:  0.9437608160078526 \n",
            "Classification loss: 1.7908945083618164 \n",
            "Regression loss: 0.04831356182694435\n",
            "----------------\n",
            "Loss:  0.9214819520711899 \n",
            "Classification loss: 1.705349087715149 \n",
            "Regression loss: 0.06880740821361542\n",
            "----------------\n",
            "Loss:  0.8027753718197346 \n",
            "Classification loss: 1.5503175258636475 \n",
            "Regression loss: 0.027616608887910843\n",
            "----------------\n",
            "Loss:  0.9297391623258591 \n",
            "Classification loss: 1.775455117225647 \n",
            "Regression loss: 0.042011603713035583\n",
            "----------------\n",
            "Loss:  1.0006412081420422 \n",
            "Classification loss: 1.8776228427886963 \n",
            "Regression loss: 0.061829786747694016\n",
            "----------------\n",
            "Loss:  0.8770254924893379 \n",
            "Classification loss: 1.641139030456543 \n",
            "Regression loss: 0.05645597726106644\n",
            "----------------\n",
            "Loss:  0.8380061872303486 \n",
            "Classification loss: 1.6030279397964478 \n",
            "Regression loss: 0.03649221733212471\n",
            "----------------\n",
            "Loss:  0.8965855278074741 \n",
            "Classification loss: 1.6710913181304932 \n",
            "Regression loss: 0.061039868742227554\n",
            "----------------\n",
            "Loss:  0.8098403103649616 \n",
            "Classification loss: 1.4969031810760498 \n",
            "Regression loss: 0.06138871982693672\n",
            "----------------\n",
            "Loss:  1.0035008192062378 \n",
            "Classification loss: 1.8722530603408813 \n",
            "Regression loss: 0.06737428903579712\n",
            "----------------\n",
            "Loss:  0.7845681644976139 \n",
            "Classification loss: 1.4677448272705078 \n",
            "Regression loss: 0.05069575086236\n",
            "----------------\n",
            "Loss:  0.9398054517805576 \n",
            "Classification loss: 1.7709144353866577 \n",
            "Regression loss: 0.054348234087228775\n",
            "----------------\n",
            "Loss:  0.8905853889882565 \n",
            "Classification loss: 1.6836557388305664 \n",
            "Regression loss: 0.04875751957297325\n",
            "----------------\n",
            "Loss:  0.9541345462203026 \n",
            "Classification loss: 1.8146960735321045 \n",
            "Regression loss: 0.046786509454250336\n",
            "----------------\n",
            "Loss:  0.8693749867379665 \n",
            "Classification loss: 1.6198747158050537 \n",
            "Regression loss: 0.05943762883543968\n",
            "----------------\n",
            "Loss:  0.9684567414224148 \n",
            "Classification loss: 1.8289859294891357 \n",
            "Regression loss: 0.05396377667784691\n",
            "----------------\n",
            "Loss:  0.9158242791891098 \n",
            "Classification loss: 1.6849452257156372 \n",
            "Regression loss: 0.0733516663312912\n",
            "----------------\n",
            "Loss:  0.9203066378831863 \n",
            "Classification loss: 1.6479328870773315 \n",
            "Regression loss: 0.09634019434452057\n",
            "----------------\n",
            "Loss:  0.8904717788100243 \n",
            "Classification loss: 1.689677357673645 \n",
            "Regression loss: 0.04563309997320175\n",
            "----------------\n",
            "Loss:  0.8157689198851585 \n",
            "Classification loss: 1.5342144966125488 \n",
            "Regression loss: 0.048661671578884125\n",
            "----------------\n",
            "Loss:  0.9192947708070278 \n",
            "Classification loss: 1.7503087520599365 \n",
            "Regression loss: 0.044140394777059555\n",
            "----------------\n",
            "Loss:  0.8984682336449623 \n",
            "Classification loss: 1.709876298904419 \n",
            "Regression loss: 0.04353008419275284\n",
            "----------------\n",
            "Loss:  0.8604178354144096 \n",
            "Classification loss: 1.6394336223602295 \n",
            "Regression loss: 0.04070102423429489\n",
            "----------------\n",
            "Loss:  0.9520127475261688 \n",
            "Classification loss: 1.8122632503509521 \n",
            "Regression loss: 0.04588112235069275\n",
            "----------------\n",
            "Loss:  0.8321371152997017 \n",
            "Classification loss: 1.5864735841751099 \n",
            "Regression loss: 0.03890032321214676\n",
            "----------------\n",
            "Loss:  1.0289303362369537 \n",
            "Classification loss: 1.887861728668213 \n",
            "Regression loss: 0.08499947190284729\n",
            "----------------\n",
            "Loss:  0.9330933466553688 \n",
            "Classification loss: 1.799912929534912 \n",
            "Regression loss: 0.03313688188791275\n",
            "----------------\n",
            "Loss:  0.886869490146637 \n",
            "Classification loss: 1.646820306777954 \n",
            "Regression loss: 0.06345933675765991\n",
            "----------------\n",
            "Loss:  0.8052909336984158 \n",
            "Classification loss: 1.5367792844772339 \n",
            "Regression loss: 0.03690129145979881\n",
            "----------------\n",
            "Loss:  0.8104267194867134 \n",
            "Classification loss: 1.4732441902160645 \n",
            "Regression loss: 0.07380462437868118\n",
            "----------------\n",
            "Loss:  0.9145605117082596 \n",
            "Classification loss: 1.7734774351119995 \n",
            "Regression loss: 0.027821794152259827\n",
            "----------------\n",
            "Loss:  0.8470418117940426 \n",
            "Classification loss: 1.6247177124023438 \n",
            "Regression loss: 0.03468295559287071\n",
            "----------------\n",
            "Loss:  0.9204074442386627 \n",
            "Classification loss: 1.7093851566314697 \n",
            "Regression loss: 0.06571486592292786\n",
            "----------------\n",
            "Loss:  0.84861970692873 \n",
            "Classification loss: 1.6043375730514526 \n",
            "Regression loss: 0.04645092040300369\n",
            "----------------\n",
            "Loss:  0.9919507652521133 \n",
            "Classification loss: 1.8035128116607666 \n",
            "Regression loss: 0.09019435942173004\n",
            "----------------\n",
            "Loss:  0.9080592058598995 \n",
            "Classification loss: 1.7470762729644775 \n",
            "Regression loss: 0.03452106937766075\n",
            "----------------\n",
            "Loss:  0.9549428522586823 \n",
            "Classification loss: 1.6972002983093262 \n",
            "Regression loss: 0.10634270310401917\n",
            "----------------\n",
            "Loss:  0.9376169145107269 \n",
            "Classification loss: 1.7446157932281494 \n",
            "Regression loss: 0.06530901789665222\n",
            "----------------\n",
            "Loss:  0.9409997649490833 \n",
            "Classification loss: 1.7755241394042969 \n",
            "Regression loss: 0.05323769524693489\n",
            "----------------\n",
            "Loss:  0.7641446031630039 \n",
            "Classification loss: 1.4751620292663574 \n",
            "Regression loss: 0.02656358852982521\n",
            "----------------\n",
            "Loss:  0.8932771682739258 \n",
            "Classification loss: 1.687608242034912 \n",
            "Regression loss: 0.04947304725646973\n",
            "----------------\n",
            "Loss:  0.8813558556139469 \n",
            "Classification loss: 1.6614491939544678 \n",
            "Regression loss: 0.05063125863671303\n",
            "----------------\n",
            "Loss:  0.9282273836433887 \n",
            "Classification loss: 1.7721357345581055 \n",
            "Regression loss: 0.042159516364336014\n",
            "----------------\n",
            "Loss:  0.9017356894910336 \n",
            "Classification loss: 1.6850409507751465 \n",
            "Regression loss: 0.05921521410346031\n",
            "----------------\n",
            "Loss:  0.9979095458984375 \n",
            "Classification loss: 1.8566910028457642 \n",
            "Regression loss: 0.06956404447555542\n",
            "----------------\n",
            "Loss:  0.9971106424927711 \n",
            "Classification loss: 1.8565771579742432 \n",
            "Regression loss: 0.06882206350564957\n",
            "----------------\n",
            "Loss:  0.8166819214820862 \n",
            "Classification loss: 1.515874981880188 \n",
            "Regression loss: 0.05874443054199219\n",
            "----------------\n",
            "Loss:  0.9005395509302616 \n",
            "Classification loss: 1.728251338005066 \n",
            "Regression loss: 0.03641388192772865\n",
            "----------------\n",
            "Loss:  0.850407537072897 \n",
            "Classification loss: 1.601109266281128 \n",
            "Regression loss: 0.04985290393233299\n",
            "----------------\n",
            "Loss:  0.8801169991493225 \n",
            "Classification loss: 1.6703572273254395 \n",
            "Regression loss: 0.04493838548660278\n",
            "----------------\n",
            "Loss:  0.8749477937817574 \n",
            "Classification loss: 1.624974250793457 \n",
            "Regression loss: 0.06246066838502884\n",
            "----------------\n",
            "Loss:  0.9299744311720133 \n",
            "Classification loss: 1.7991828918457031 \n",
            "Regression loss: 0.03038298524916172\n",
            "----------------\n",
            "Loss:  1.0230120122432709 \n",
            "Classification loss: 1.9471375942230225 \n",
            "Regression loss: 0.049443215131759644\n",
            "----------------\n",
            "Loss:  0.9279172942042351 \n",
            "Classification loss: 1.7535920143127441 \n",
            "Regression loss: 0.05112128704786301\n",
            "----------------\n",
            "Loss:  0.9089880064129829 \n",
            "Classification loss: 1.7154654264450073 \n",
            "Regression loss: 0.05125529319047928\n",
            "----------------\n",
            "Loss:  0.9527266100049019 \n",
            "Classification loss: 1.7499315738677979 \n",
            "Regression loss: 0.07776082307100296\n",
            "----------------\n",
            "Loss:  0.9491457343101501 \n",
            "Classification loss: 1.7992278337478638 \n",
            "Regression loss: 0.04953181743621826\n",
            "----------------\n",
            "Loss:  0.9021611213684082 \n",
            "Classification loss: 1.7186001539230347 \n",
            "Regression loss: 0.04286104440689087\n",
            "----------------\n",
            "Loss:  1.0302799567580223 \n",
            "Classification loss: 1.9201257228851318 \n",
            "Regression loss: 0.07021709531545639\n",
            "----------------\n",
            "Loss:  0.9627127796411514 \n",
            "Classification loss: 1.821906328201294 \n",
            "Regression loss: 0.051759615540504456\n",
            "----------------\n",
            "Loss:  0.8806376941502094 \n",
            "Classification loss: 1.6621999740600586 \n",
            "Regression loss: 0.04953770712018013\n",
            "----------------\n",
            "Loss:  0.9461673349142075 \n",
            "Classification loss: 1.7712616920471191 \n",
            "Regression loss: 0.06053648889064789\n",
            "----------------\n",
            "Loss:  0.8398055955767632 \n",
            "Classification loss: 1.5890319347381592 \n",
            "Regression loss: 0.04528962820768356\n",
            "----------------\n",
            "Loss:  0.9395699873566628 \n",
            "Classification loss: 1.7712066173553467 \n",
            "Regression loss: 0.05396667867898941\n",
            "----------------\n",
            "Loss:  0.9243351221084595 \n",
            "Classification loss: 1.684777855873108 \n",
            "Regression loss: 0.08194619417190552\n",
            "----------------\n",
            "Loss:  0.8850975334644318 \n",
            "Classification loss: 1.6617636680603027 \n",
            "Regression loss: 0.054215699434280396\n",
            "----------------\n",
            "Loss:  0.8239071629941463 \n",
            "Classification loss: 1.5599431991577148 \n",
            "Regression loss: 0.043935563415288925\n",
            "----------------\n",
            "Loss:  0.8900174871087074 \n",
            "Classification loss: 1.6953401565551758 \n",
            "Regression loss: 0.04234740883111954\n",
            "----------------\n",
            "Loss:  0.8747123405337334 \n",
            "Classification loss: 1.6963928937911987 \n",
            "Regression loss: 0.026515893638134003\n",
            "----------------\n",
            "Loss:  0.9085165038704872 \n",
            "Classification loss: 1.7308324575424194 \n",
            "Regression loss: 0.043100275099277496\n",
            "----------------\n",
            "Loss:  0.9354790262877941 \n",
            "Classification loss: 1.7489291429519653 \n",
            "Regression loss: 0.06101445481181145\n",
            "----------------\n",
            "Loss:  0.8335514329373837 \n",
            "Classification loss: 1.5803278684616089 \n",
            "Regression loss: 0.04338749870657921\n",
            "----------------\n",
            "Loss:  0.8130133002996445 \n",
            "Classification loss: 1.5239975452423096 \n",
            "Regression loss: 0.051014527678489685\n",
            "----------------\n",
            "Loss:  0.781467892229557 \n",
            "Classification loss: 1.4424796104431152 \n",
            "Regression loss: 0.06022808700799942\n",
            "----------------\n",
            "Loss:  0.8261159956455231 \n",
            "Classification loss: 1.5883781909942627 \n",
            "Regression loss: 0.031926900148391724\n",
            "----------------\n",
            "Loss:  0.9184795916080475 \n",
            "Classification loss: 1.7550029754638672 \n",
            "Regression loss: 0.04097810387611389\n",
            "----------------\n",
            "Loss:  0.8305744789540768 \n",
            "Classification loss: 1.615088939666748 \n",
            "Regression loss: 0.023030009120702744\n",
            "----------------\n",
            "Loss:  0.8843629732728004 \n",
            "Classification loss: 1.643725037574768 \n",
            "Regression loss: 0.06250045448541641\n",
            "----------------\n",
            "Loss:  0.9001390412449837 \n",
            "Classification loss: 1.725427269935608 \n",
            "Regression loss: 0.03742540627717972\n",
            "----------------\n",
            "Loss:  0.8535737469792366 \n",
            "Classification loss: 1.6178603172302246 \n",
            "Regression loss: 0.0446435883641243\n",
            "----------------\n",
            "Loss:  0.8647036999464035 \n",
            "Classification loss: 1.622037649154663 \n",
            "Regression loss: 0.05368487536907196\n",
            "----------------\n",
            "Loss:  0.8795297704637051 \n",
            "Classification loss: 1.6815192699432373 \n",
            "Regression loss: 0.03877013549208641\n",
            "----------------\n",
            "Loss:  0.9203806146979332 \n",
            "Classification loss: 1.72585129737854 \n",
            "Regression loss: 0.05745496600866318\n",
            "----------------\n",
            "Loss:  0.8088452070951462 \n",
            "Classification loss: 1.553426742553711 \n",
            "Regression loss: 0.03213183581829071\n",
            "----------------\n",
            "Loss:  0.8770828396081924 \n",
            "Classification loss: 1.600637435913086 \n",
            "Regression loss: 0.07676412165164948\n",
            "----------------\n",
            "Loss:  0.9792255982756615 \n",
            "Classification loss: 1.7872092723846436 \n",
            "Regression loss: 0.08562096208333969\n",
            "----------------\n",
            "Loss:  0.7803304009139538 \n",
            "Classification loss: 1.4491302967071533 \n",
            "Regression loss: 0.05576525256037712\n",
            "----------------\n",
            "Loss:  0.9623543322086334 \n",
            "Classification loss: 1.7668805122375488 \n",
            "Regression loss: 0.07891407608985901\n",
            "----------------\n",
            "Loss:  0.9400098845362663 \n",
            "Classification loss: 1.7097870111465454 \n",
            "Regression loss: 0.08511637896299362\n",
            "----------------\n",
            "Loss:  0.8803059607744217 \n",
            "Classification loss: 1.5814388990402222 \n",
            "Regression loss: 0.08958651125431061\n",
            "----------------\n",
            "Loss:  0.7787922248244286 \n",
            "Classification loss: 1.478134274482727 \n",
            "Regression loss: 0.03972508758306503\n",
            "----------------\n",
            "Loss:  0.9439169764518738 \n",
            "Classification loss: 1.7683080434799194 \n",
            "Regression loss: 0.05976295471191406\n",
            "----------------\n",
            "Loss:  0.755328182131052 \n",
            "Classification loss: 1.4396672248840332 \n",
            "Regression loss: 0.035494569689035416\n",
            "----------------\n",
            "Loss:  0.7998744286596775 \n",
            "Classification loss: 1.5328274965286255 \n",
            "Regression loss: 0.03346068039536476\n",
            "----------------\n",
            "Loss:  0.944775678217411 \n",
            "Classification loss: 1.762681484222412 \n",
            "Regression loss: 0.06343493610620499\n",
            "----------------\n",
            "Loss:  0.9795504435896873 \n",
            "Classification loss: 1.8492906093597412 \n",
            "Regression loss: 0.05490513890981674\n",
            "----------------\n",
            "Loss:  0.8987855166196823 \n",
            "Classification loss: 1.7224297523498535 \n",
            "Regression loss: 0.037570640444755554\n",
            "----------------\n",
            "Loss:  0.9252106621861458 \n",
            "Classification loss: 1.7598992586135864 \n",
            "Regression loss: 0.04526103287935257\n",
            "----------------\n",
            "Loss:  0.8869236409664154 \n",
            "Classification loss: 1.632123351097107 \n",
            "Regression loss: 0.07086196541786194\n",
            "----------------\n",
            "Loss:  0.8736916966736317 \n",
            "Classification loss: 1.6681431531906128 \n",
            "Regression loss: 0.03962012007832527\n",
            "----------------\n",
            "Loss:  0.9495988227427006 \n",
            "Classification loss: 1.779845118522644 \n",
            "Regression loss: 0.059676263481378555\n",
            "----------------\n",
            "Loss:  0.8800099641084671 \n",
            "Classification loss: 1.6424177885055542 \n",
            "Regression loss: 0.05880106985569\n",
            "----------------\n",
            "Loss:  0.9072565138339996 \n",
            "Classification loss: 1.6713054180145264 \n",
            "Regression loss: 0.07160380482673645\n",
            "----------------\n",
            "Loss:  0.8339792788028717 \n",
            "Classification loss: 1.5757440328598022 \n",
            "Regression loss: 0.04610726237297058\n",
            "----------------\n",
            "Loss:  0.8817784748971462 \n",
            "Classification loss: 1.644370675086975 \n",
            "Regression loss: 0.059593137353658676\n",
            "----------------\n",
            "Loss:  0.9321732483804226 \n",
            "Classification loss: 1.7525920867919922 \n",
            "Regression loss: 0.0558772049844265\n",
            "----------------\n",
            "Loss:  0.9179273918271065 \n",
            "Classification loss: 1.7042585611343384 \n",
            "Regression loss: 0.06579811125993729\n",
            "----------------\n",
            "Loss:  0.9021425917744637 \n",
            "Classification loss: 1.7026406526565552 \n",
            "Regression loss: 0.050822265446186066\n",
            "----------------\n",
            "Loss:  0.878060407936573 \n",
            "Classification loss: 1.7038109302520752 \n",
            "Regression loss: 0.02615494281053543\n",
            "----------------\n",
            "Loss:  0.9627181515097618 \n",
            "Classification loss: 1.7931482791900635 \n",
            "Regression loss: 0.06614401191473007\n",
            "----------------\n",
            "Loss:  0.8689634725451469 \n",
            "Classification loss: 1.6212049722671509 \n",
            "Regression loss: 0.0583609864115715\n",
            "----------------\n",
            "Loss:  0.9221442826092243 \n",
            "Classification loss: 1.7650482654571533 \n",
            "Regression loss: 0.03962014988064766\n",
            "----------------\n",
            "Loss:  1.008798636496067 \n",
            "Classification loss: 1.8902102708816528 \n",
            "Regression loss: 0.06369350105524063\n",
            "----------------\n",
            "Loss:  0.9492290541529655 \n",
            "Classification loss: 1.7282445430755615 \n",
            "Regression loss: 0.08510678261518478\n",
            "----------------\n",
            "Loss:  0.89718172326684 \n",
            "Classification loss: 1.708968162536621 \n",
            "Regression loss: 0.042697641998529434\n",
            "----------------\n",
            "Loss:  0.9555452913045883 \n",
            "Classification loss: 1.7753626108169556 \n",
            "Regression loss: 0.06786398589611053\n",
            "----------------\n",
            "Loss:  0.8266548402607441 \n",
            "Classification loss: 1.6093615293502808 \n",
            "Regression loss: 0.021974075585603714\n",
            "----------------\n",
            "Loss:  0.9757314138114452 \n",
            "Classification loss: 1.877886414527893 \n",
            "Regression loss: 0.0367882065474987\n",
            "----------------\n",
            "Loss:  0.8863519839942455 \n",
            "Classification loss: 1.7014235258102417 \n",
            "Regression loss: 0.03564022108912468\n",
            "----------------\n",
            "Loss:  0.7905082032084465 \n",
            "Classification loss: 1.5108575820922852 \n",
            "Regression loss: 0.035079412162303925\n",
            "----------------\n",
            "Loss:  0.7782559096813202 \n",
            "Classification loss: 1.4564012289047241 \n",
            "Regression loss: 0.05005529522895813\n",
            "----------------\n",
            "Loss:  0.9855809025466442 \n",
            "Classification loss: 1.8759716749191284 \n",
            "Regression loss: 0.04759506508708\n",
            "----------------\n",
            "Loss:  0.8635120056569576 \n",
            "Classification loss: 1.6117020845413208 \n",
            "Regression loss: 0.057660963386297226\n",
            "----------------\n",
            "Loss:  0.883445717394352 \n",
            "Classification loss: 1.6375831365585327 \n",
            "Regression loss: 0.0646541491150856\n",
            "----------------\n",
            "Loss:  0.8130836859345436 \n",
            "Classification loss: 1.567396640777588 \n",
            "Regression loss: 0.029385365545749664\n",
            "----------------\n",
            "Loss:  0.8511113002896309 \n",
            "Classification loss: 1.6333364248275757 \n",
            "Regression loss: 0.03444308787584305\n",
            "----------------\n",
            "Loss:  0.8822528049349785 \n",
            "Classification loss: 1.6696293354034424 \n",
            "Regression loss: 0.047438137233257294\n",
            "----------------\n",
            "Loss:  0.8317570090293884 \n",
            "Classification loss: 1.5852911472320557 \n",
            "Regression loss: 0.039111435413360596\n",
            "----------------\n",
            "Loss:  0.8126599863171577 \n",
            "Classification loss: 1.543111801147461 \n",
            "Regression loss: 0.04110408574342728\n",
            "----------------\n",
            "Loss:  0.8131387531757355 \n",
            "Classification loss: 1.5340957641601562 \n",
            "Regression loss: 0.04609087109565735\n",
            "----------------\n",
            "Loss:  0.8930891491472721 \n",
            "Classification loss: 1.688136339187622 \n",
            "Regression loss: 0.049020979553461075\n",
            "----------------\n",
            "Loss:  0.8620521258562803 \n",
            "Classification loss: 1.6763956546783447 \n",
            "Regression loss: 0.023854298517107964\n",
            "----------------\n",
            "Loss:  0.9646913968026638 \n",
            "Classification loss: 1.8105275630950928 \n",
            "Regression loss: 0.059427615255117416\n",
            "----------------\n",
            "Loss:  0.8605141416192055 \n",
            "Classification loss: 1.5831317901611328 \n",
            "Regression loss: 0.06894824653863907\n",
            "----------------\n",
            "Loss:  0.8694574981927872 \n",
            "Classification loss: 1.6439595222473145 \n",
            "Regression loss: 0.047477737069129944\n",
            "----------------\n",
            "Loss:  0.8631909936666489 \n",
            "Classification loss: 1.6268916130065918 \n",
            "Regression loss: 0.049745187163352966\n",
            "----------------\n",
            "Loss:  0.8346629031002522 \n",
            "Classification loss: 1.6060082912445068 \n",
            "Regression loss: 0.031658757477998734\n",
            "----------------\n",
            "Loss:  0.8028603605926037 \n",
            "Classification loss: 1.555307149887085 \n",
            "Regression loss: 0.025206785649061203\n",
            "----------------\n",
            "Loss:  0.8080267459154129 \n",
            "Classification loss: 1.5158244371414185 \n",
            "Regression loss: 0.050114527344703674\n",
            "----------------\n",
            "Loss:  0.7519937939941883 \n",
            "Classification loss: 1.412414312362671 \n",
            "Regression loss: 0.04578663781285286\n",
            "----------------\n",
            "Loss:  0.8782981410622597 \n",
            "Classification loss: 1.633545160293579 \n",
            "Regression loss: 0.06152556091547012\n",
            "----------------\n",
            "Loss:  0.8573788702487946 \n",
            "Classification loss: 1.578467845916748 \n",
            "Regression loss: 0.06814494729042053\n",
            "----------------\n",
            "Loss:  0.8244839906692505 \n",
            "Classification loss: 1.5639870166778564 \n",
            "Regression loss: 0.042490482330322266\n",
            "----------------\n",
            "Loss:  1.0584328211843967 \n",
            "Classification loss: 2.0030813217163086 \n",
            "Regression loss: 0.05689216032624245\n",
            "----------------\n",
            "Loss:  0.9542991537600756 \n",
            "Classification loss: 1.8508696556091309 \n",
            "Regression loss: 0.02886432595551014\n",
            "----------------\n",
            "Loss:  0.8857919536530972 \n",
            "Classification loss: 1.6612226963043213 \n",
            "Regression loss: 0.05518060550093651\n",
            "----------------\n",
            "Loss:  0.9087168350815773 \n",
            "Classification loss: 1.7142422199249268 \n",
            "Regression loss: 0.05159572511911392\n",
            "----------------\n",
            "Loss:  0.9133064188063145 \n",
            "Classification loss: 1.7237380743026733 \n",
            "Regression loss: 0.0514373816549778\n",
            "----------------\n",
            "Loss:  0.81158547103405 \n",
            "Classification loss: 1.4990180730819702 \n",
            "Regression loss: 0.06207643449306488\n",
            "----------------\n",
            "Loss:  0.9286184012889862 \n",
            "Classification loss: 1.7652236223220825 \n",
            "Regression loss: 0.046006590127944946\n",
            "----------------\n",
            "Loss:  0.8887819610536098 \n",
            "Classification loss: 1.654585599899292 \n",
            "Regression loss: 0.06148916110396385\n",
            "----------------\n",
            "Loss:  0.792698822915554 \n",
            "Classification loss: 1.4825327396392822 \n",
            "Regression loss: 0.05143245309591293\n",
            "----------------\n",
            "Loss:  0.8527160733938217 \n",
            "Classification loss: 1.599741816520691 \n",
            "Regression loss: 0.05284516513347626\n",
            "----------------\n",
            "Loss:  0.7995125576853752 \n",
            "Classification loss: 1.50211501121521 \n",
            "Regression loss: 0.04845505207777023\n",
            "----------------\n",
            "Loss:  0.7735732048749924 \n",
            "Classification loss: 1.463114857673645 \n",
            "Regression loss: 0.04201577603816986\n",
            "----------------\n",
            "Loss:  0.8441946282982826 \n",
            "Classification loss: 1.6007354259490967 \n",
            "Regression loss: 0.04382691532373428\n",
            "----------------\n",
            "Loss:  0.8444764129817486 \n",
            "Classification loss: 1.6100482940673828 \n",
            "Regression loss: 0.039452265948057175\n",
            "----------------\n",
            "Loss:  0.9593607485294342 \n",
            "Classification loss: 1.781633734703064 \n",
            "Regression loss: 0.06854388117790222\n",
            "----------------\n",
            "Loss:  0.8845375441014767 \n",
            "Classification loss: 1.6575734615325928 \n",
            "Regression loss: 0.05575081333518028\n",
            "----------------\n",
            "Loss:  0.9541444964706898 \n",
            "Classification loss: 1.7852728366851807 \n",
            "Regression loss: 0.06150807812809944\n",
            "----------------\n",
            "Loss:  0.9766485802829266 \n",
            "Classification loss: 1.8599642515182495 \n",
            "Regression loss: 0.046666454523801804\n",
            "----------------\n",
            "Loss:  0.8207327164709568 \n",
            "Classification loss: 1.5248796939849854 \n",
            "Regression loss: 0.05829286947846413\n",
            "----------------\n",
            "Loss:  0.8165304511785507 \n",
            "Classification loss: 1.5319184064865112 \n",
            "Regression loss: 0.050571247935295105\n",
            "----------------\n",
            "Loss:  0.8313808366656303 \n",
            "Classification loss: 1.54563570022583 \n",
            "Regression loss: 0.0585629865527153\n",
            "----------------\n",
            "Loss:  0.8550447262823582 \n",
            "Classification loss: 1.594970464706421 \n",
            "Regression loss: 0.05755949392914772\n",
            "----------------\n",
            "Loss:  0.7734038829803467 \n",
            "Classification loss: 1.4621319770812988 \n",
            "Regression loss: 0.042337894439697266\n",
            "----------------\n",
            "Loss:  0.8023693263530731 \n",
            "Classification loss: 1.5374292135238647 \n",
            "Regression loss: 0.03365471959114075\n",
            "----------------\n",
            "Loss:  0.8712076619267464 \n",
            "Classification loss: 1.6457555294036865 \n",
            "Regression loss: 0.04832989722490311\n",
            "----------------\n",
            "Loss:  0.8664909191429615 \n",
            "Classification loss: 1.6567238569259644 \n",
            "Regression loss: 0.038128990679979324\n",
            "----------------\n",
            "Loss:  1.004082903265953 \n",
            "Classification loss: 1.891094446182251 \n",
            "Regression loss: 0.058535680174827576\n",
            "----------------\n",
            "Loss:  0.8595454767346382 \n",
            "Classification loss: 1.6327030658721924 \n",
            "Regression loss: 0.04319394379854202\n",
            "----------------\n",
            "Loss:  0.879006277769804 \n",
            "Classification loss: 1.6515949964523315 \n",
            "Regression loss: 0.05320877954363823\n",
            "----------------\n",
            "Loss:  0.8632076308131218 \n",
            "Classification loss: 1.6407967805862427 \n",
            "Regression loss: 0.04280924052000046\n",
            "----------------\n",
            "Loss:  0.8393339812755585 \n",
            "Classification loss: 1.552772045135498 \n",
            "Regression loss: 0.06294795870780945\n",
            "----------------\n",
            "Loss:  0.8418967239558697 \n",
            "Classification loss: 1.5996904373168945 \n",
            "Regression loss: 0.04205150529742241\n",
            "----------------\n",
            "Loss:  0.9786802679300308 \n",
            "Classification loss: 1.8303958177566528 \n",
            "Regression loss: 0.0634823590517044\n",
            "----------------\n",
            "Loss:  0.8042021878063679 \n",
            "Classification loss: 1.5287528038024902 \n",
            "Regression loss: 0.03982578590512276\n",
            "----------------\n",
            "Loss:  0.8297120407223701 \n",
            "Classification loss: 1.5404173135757446 \n",
            "Regression loss: 0.05950338393449783\n",
            "----------------\n",
            "Loss:  1.0066417008638382 \n",
            "Classification loss: 1.8558714389801025 \n",
            "Regression loss: 0.07870598137378693\n",
            "----------------\n",
            "Loss:  0.8713535852730274 \n",
            "Classification loss: 1.6261500120162964 \n",
            "Regression loss: 0.05827857926487923\n",
            "----------------\n",
            "Loss:  0.8691263906657696 \n",
            "Classification loss: 1.6237001419067383 \n",
            "Regression loss: 0.057276319712400436\n",
            "----------------\n",
            "Loss:  0.935702495276928 \n",
            "Classification loss: 1.7622424364089966 \n",
            "Regression loss: 0.05458127707242966\n",
            "----------------\n",
            "Loss:  0.8404507674276829 \n",
            "Classification loss: 1.6051338911056519 \n",
            "Regression loss: 0.03788382187485695\n",
            "----------------\n",
            "Loss:  1.0282342284917831 \n",
            "Classification loss: 1.9599175453186035 \n",
            "Regression loss: 0.048275455832481384\n",
            "----------------\n",
            "Loss:  0.8843653425574303 \n",
            "Classification loss: 1.649673581123352 \n",
            "Regression loss: 0.05952855199575424\n",
            "----------------\n",
            "Loss:  0.8554099649190903 \n",
            "Classification loss: 1.57663893699646 \n",
            "Regression loss: 0.06709049642086029\n",
            "----------------\n",
            "Loss:  0.8411743491888046 \n",
            "Classification loss: 1.5366414785385132 \n",
            "Regression loss: 0.07285360991954803\n",
            "----------------\n",
            "Loss:  0.8352570198476315 \n",
            "Classification loss: 1.567190408706665 \n",
            "Regression loss: 0.051661815494298935\n",
            "----------------\n",
            "Loss:  0.8821896910667419 \n",
            "Classification loss: 1.636399745941162 \n",
            "Regression loss: 0.06398981809616089\n",
            "----------------\n",
            "Loss:  0.8234167117625475 \n",
            "Classification loss: 1.588922381401062 \n",
            "Regression loss: 0.028955521062016487\n",
            "----------------\n",
            "Loss:  0.8268333487212658 \n",
            "Classification loss: 1.5656743049621582 \n",
            "Regression loss: 0.04399619624018669\n",
            "----------------\n",
            "Loss:  0.912641417235136 \n",
            "Classification loss: 1.7198526859283447 \n",
            "Regression loss: 0.05271507427096367\n",
            "----------------\n",
            "Loss:  0.9224627315998077 \n",
            "Classification loss: 1.7527742385864258 \n",
            "Regression loss: 0.04607561230659485\n",
            "----------------\n",
            "Loss:  0.829881027340889 \n",
            "Classification loss: 1.5879318714141846 \n",
            "Regression loss: 0.03591509163379669\n",
            "----------------\n",
            "Loss:  0.9225055947899818 \n",
            "Classification loss: 1.7305530309677124 \n",
            "Regression loss: 0.05722907930612564\n",
            "----------------\n",
            "Loss:  0.7799137905240059 \n",
            "Classification loss: 1.4781732559204102 \n",
            "Regression loss: 0.04082716256380081\n",
            "----------------\n",
            "Loss:  0.8660908937454224 \n",
            "Classification loss: 1.5982717275619507 \n",
            "Regression loss: 0.06695502996444702\n",
            "----------------\n",
            "Loss:  0.8752081245183945 \n",
            "Classification loss: 1.6452159881591797 \n",
            "Regression loss: 0.052600130438804626\n",
            "----------------\n",
            "Loss:  0.8248203322291374 \n",
            "Classification loss: 1.5476148128509521 \n",
            "Regression loss: 0.051012925803661346\n",
            "----------------\n",
            "Loss:  0.8424007818102837 \n",
            "Classification loss: 1.609764814376831 \n",
            "Regression loss: 0.037518374621868134\n",
            "----------------\n",
            "Loss:  0.9628533646464348 \n",
            "Classification loss: 1.8151596784591675 \n",
            "Regression loss: 0.055273525416851044\n",
            "----------------\n",
            "Loss:  0.8713167607784271 \n",
            "Classification loss: 1.6611865758895874 \n",
            "Regression loss: 0.04072347283363342\n",
            "----------------\n",
            "Loss:  0.7673681862652302 \n",
            "Classification loss: 1.455477237701416 \n",
            "Regression loss: 0.03962956741452217\n",
            "----------------\n",
            "Loss:  0.7994303926825523 \n",
            "Classification loss: 1.5065735578536987 \n",
            "Regression loss: 0.04614361375570297\n",
            "----------------\n",
            "Loss:  0.7597071565687656 \n",
            "Classification loss: 1.45999014377594 \n",
            "Regression loss: 0.02971208468079567\n",
            "----------------\n",
            "Loss:  0.9187120124697685 \n",
            "Classification loss: 1.7521462440490723 \n",
            "Regression loss: 0.04263889044523239\n",
            "----------------\n",
            "Loss:  0.8904812783002853 \n",
            "Classification loss: 1.703657627105713 \n",
            "Regression loss: 0.038652464747428894\n",
            "----------------\n",
            "Loss:  0.7997153997421265 \n",
            "Classification loss: 1.5225944519042969 \n",
            "Regression loss: 0.03841817378997803\n",
            "----------------\n",
            "Loss:  0.9323900640010834 \n",
            "Classification loss: 1.7229098081588745 \n",
            "Regression loss: 0.07093515992164612\n",
            "----------------\n",
            "Loss:  0.8788846731185913 \n",
            "Classification loss: 1.648330807685852 \n",
            "Regression loss: 0.05471926927566528\n",
            "----------------\n",
            "Loss:  0.8407121822237968 \n",
            "Classification loss: 1.593988299369812 \n",
            "Regression loss: 0.04371803253889084\n",
            "----------------\n",
            "Loss:  0.9759991094470024 \n",
            "Classification loss: 1.7798974514007568 \n",
            "Regression loss: 0.08605038374662399\n",
            "----------------\n",
            "Loss:  0.8551551587879658 \n",
            "Classification loss: 1.6392790079116821 \n",
            "Regression loss: 0.03551565483212471\n",
            "----------------\n",
            "Loss:  0.9229992404580116 \n",
            "Classification loss: 1.6762959957122803 \n",
            "Regression loss: 0.08485124260187149\n",
            "----------------\n",
            "Loss:  0.9187013655900955 \n",
            "Classification loss: 1.7229478359222412 \n",
            "Regression loss: 0.057227447628974915\n",
            "----------------\n",
            "Loss:  0.8653359413146973 \n",
            "Classification loss: 1.6321239471435547 \n",
            "Regression loss: 0.04927396774291992\n",
            "----------------\n",
            "Loss:  0.7886283621191978 \n",
            "Classification loss: 1.5009633302688599 \n",
            "Regression loss: 0.038146696984767914\n",
            "----------------\n",
            "Loss:  0.8691050820052624 \n",
            "Classification loss: 1.6595178842544556 \n",
            "Regression loss: 0.03934613987803459\n",
            "----------------\n",
            "Loss:  0.9145533740520477 \n",
            "Classification loss: 1.689717411994934 \n",
            "Regression loss: 0.06969466805458069\n",
            "----------------\n",
            "Loss:  0.8108215294778347 \n",
            "Classification loss: 1.522437334060669 \n",
            "Regression loss: 0.04960286244750023\n",
            "----------------\n",
            "Loss:  0.8957570418715477 \n",
            "Classification loss: 1.6910381317138672 \n",
            "Regression loss: 0.050237976014614105\n",
            "----------------\n",
            "Loss:  0.7945564836263657 \n",
            "Classification loss: 1.4620041847229004 \n",
            "Regression loss: 0.06355439126491547\n",
            "----------------\n",
            "Loss:  0.7995526343584061 \n",
            "Classification loss: 1.4718718528747559 \n",
            "Regression loss: 0.06361670792102814\n",
            "----------------\n",
            "Loss:  0.9332648329436779 \n",
            "Classification loss: 1.7512718439102173 \n",
            "Regression loss: 0.05762891098856926\n",
            "----------------\n",
            "Loss:  0.8789536133408546 \n",
            "Classification loss: 1.661175012588501 \n",
            "Regression loss: 0.048366107046604156\n",
            "----------------\n",
            "Loss:  0.8298525772988796 \n",
            "Classification loss: 1.5552458763122559 \n",
            "Regression loss: 0.052229639142751694\n",
            "----------------\n",
            "Loss:  0.8431795164942741 \n",
            "Classification loss: 1.598602294921875 \n",
            "Regression loss: 0.04387836903333664\n",
            "----------------\n",
            "Loss:  0.829370204359293 \n",
            "Classification loss: 1.5784344673156738 \n",
            "Regression loss: 0.04015297070145607\n",
            "----------------\n",
            "Loss:  0.8606901802122593 \n",
            "Classification loss: 1.6550202369689941 \n",
            "Regression loss: 0.03318006172776222\n",
            "----------------\n",
            "Loss:  0.8710102774202824 \n",
            "Classification loss: 1.6663382053375244 \n",
            "Regression loss: 0.03784117475152016\n",
            "----------------\n",
            "Loss:  0.776227917522192 \n",
            "Classification loss: 1.461904525756836 \n",
            "Regression loss: 0.04527565464377403\n",
            "----------------\n",
            "Loss:  0.8678671419620514 \n",
            "Classification loss: 1.6087157726287842 \n",
            "Regression loss: 0.0635092556476593\n",
            "----------------\n",
            "Loss:  0.8522146418690681 \n",
            "Classification loss: 1.5449624061584473 \n",
            "Regression loss: 0.07973343878984451\n",
            "----------------\n",
            "Loss:  0.8074637465178967 \n",
            "Classification loss: 1.5338870286941528 \n",
            "Regression loss: 0.040520232170820236\n",
            "----------------\n",
            "Loss:  0.8139205202460289 \n",
            "Classification loss: 1.494368553161621 \n",
            "Regression loss: 0.06673624366521835\n",
            "----------------\n",
            "Loss:  0.8869025409221649 \n",
            "Classification loss: 1.6644837856292725 \n",
            "Regression loss: 0.054660648107528687\n",
            "----------------\n",
            "Loss:  0.9462807141244411 \n",
            "Classification loss: 1.7948682308197021 \n",
            "Regression loss: 0.04884659871459007\n",
            "----------------\n",
            "Loss:  0.8298941850662231 \n",
            "Classification loss: 1.528367042541504 \n",
            "Regression loss: 0.06571066379547119\n",
            "----------------\n",
            "Loss:  0.8834950476884842 \n",
            "Classification loss: 1.6605867147445679 \n",
            "Regression loss: 0.053201690316200256\n",
            "----------------\n",
            "Loss:  0.7990987934172153 \n",
            "Classification loss: 1.5175005197525024 \n",
            "Regression loss: 0.04034853354096413\n",
            "----------------\n",
            "Loss:  0.8501513302326202 \n",
            "Classification loss: 1.5739095211029053 \n",
            "Regression loss: 0.0631965696811676\n",
            "----------------\n",
            "Loss:  0.9475754424929619 \n",
            "Classification loss: 1.8052679300308228 \n",
            "Regression loss: 0.04494147747755051\n",
            "----------------\n",
            "Loss:  0.8370886147022247 \n",
            "Classification loss: 1.5350086688995361 \n",
            "Regression loss: 0.06958428025245667\n",
            "----------------\n",
            "Loss:  0.9540834128856659 \n",
            "Classification loss: 1.7705566883087158 \n",
            "Regression loss: 0.06880506873130798\n",
            "----------------\n",
            "Loss:  0.8862805217504501 \n",
            "Classification loss: 1.6989386081695557 \n",
            "Regression loss: 0.0368112176656723\n",
            "----------------\n",
            "Loss:  0.8641318902373314 \n",
            "Classification loss: 1.6384718418121338 \n",
            "Regression loss: 0.044895969331264496\n",
            "----------------\n",
            "Loss:  0.8389571309089661 \n",
            "Classification loss: 1.5436877012252808 \n",
            "Regression loss: 0.06711328029632568\n",
            "----------------\n",
            "Loss:  0.8642867095768452 \n",
            "Classification loss: 1.6062253713607788 \n",
            "Regression loss: 0.061174023896455765\n",
            "----------------\n",
            "Loss:  0.8332309611141682 \n",
            "Classification loss: 1.6000993251800537 \n",
            "Regression loss: 0.03318129852414131\n",
            "----------------\n",
            "Loss:  0.8651075139641762 \n",
            "Classification loss: 1.6007846593856812 \n",
            "Regression loss: 0.0647151842713356\n",
            "----------------\n",
            "Loss:  0.8467283733189106 \n",
            "Classification loss: 1.5821914672851562 \n",
            "Regression loss: 0.055632639676332474\n",
            "----------------\n",
            "Loss:  0.9056724049150944 \n",
            "Classification loss: 1.6924841403961182 \n",
            "Regression loss: 0.059430334717035294\n",
            "----------------\n",
            "Loss:  0.816044133156538 \n",
            "Classification loss: 1.5382614135742188 \n",
            "Regression loss: 0.046913426369428635\n",
            "----------------\n",
            "Loss:  0.9748534522950649 \n",
            "Classification loss: 1.8671869039535522 \n",
            "Regression loss: 0.0412600003182888\n",
            "----------------\n",
            "Loss:  0.7868557386100292 \n",
            "Classification loss: 1.4883129596710205 \n",
            "Regression loss: 0.04269925877451897\n",
            "----------------\n",
            "Loss:  1.0117272213101387 \n",
            "Classification loss: 1.8636178970336914 \n",
            "Regression loss: 0.079918272793293\n",
            "----------------\n",
            "Loss:  0.9302408695220947 \n",
            "Classification loss: 1.7318317890167236 \n",
            "Regression loss: 0.06432497501373291\n",
            "----------------\n",
            "Loss:  0.81842240691185 \n",
            "Classification loss: 1.5532329082489014 \n",
            "Regression loss: 0.04180595278739929\n",
            "----------------\n",
            "Loss:  0.8612353950738907 \n",
            "Classification loss: 1.6049364805221558 \n",
            "Regression loss: 0.058767154812812805\n",
            "----------------\n",
            "Loss:  0.7576233707368374 \n",
            "Classification loss: 1.3958804607391357 \n",
            "Regression loss: 0.059683140367269516\n",
            "----------------\n",
            "Loss:  0.8252253793179989 \n",
            "Classification loss: 1.5827383995056152 \n",
            "Regression loss: 0.03385617956519127\n",
            "----------------\n",
            "Loss:  0.8418010920286179 \n",
            "Classification loss: 1.5817365646362305 \n",
            "Regression loss: 0.050932809710502625\n",
            "----------------\n",
            "Loss:  0.9169821105897427 \n",
            "Classification loss: 1.7517566680908203 \n",
            "Regression loss: 0.041103776544332504\n",
            "----------------\n",
            "Loss:  0.8409751057624817 \n",
            "Classification loss: 1.5821499824523926 \n",
            "Regression loss: 0.0499001145362854\n",
            "----------------\n",
            "Loss:  0.9671248756349087 \n",
            "Classification loss: 1.8169002532958984 \n",
            "Regression loss: 0.05867474898695946\n",
            "----------------\n",
            "Loss:  0.7776264622807503 \n",
            "Classification loss: 1.4413708448410034 \n",
            "Regression loss: 0.056941039860248566\n",
            "----------------\n",
            "Loss:  0.9707237407565117 \n",
            "Classification loss: 1.8083683252334595 \n",
            "Regression loss: 0.06653957813978195\n",
            "----------------\n",
            "Loss:  0.8121292926371098 \n",
            "Classification loss: 1.5234620571136475 \n",
            "Regression loss: 0.050398264080286026\n",
            "----------------\n",
            "Loss:  0.8511320203542709 \n",
            "Classification loss: 1.5547599792480469 \n",
            "Regression loss: 0.0737520307302475\n",
            "----------------\n",
            "Loss:  0.8918776400387287 \n",
            "Classification loss: 1.6912062168121338 \n",
            "Regression loss: 0.04627453163266182\n",
            "----------------\n",
            "Loss:  0.8433459717780352 \n",
            "Classification loss: 1.6286498308181763 \n",
            "Regression loss: 0.02902105636894703\n",
            "----------------\n",
            "Loss:  0.9083067625761032 \n",
            "Classification loss: 1.7086434364318848 \n",
            "Regression loss: 0.05398504436016083\n",
            "----------------\n",
            "Loss:  0.7802436277270317 \n",
            "Classification loss: 1.4565067291259766 \n",
            "Regression loss: 0.051990263164043427\n",
            "----------------\n",
            "Loss:  0.8074557930231094 \n",
            "Classification loss: 1.514513373374939 \n",
            "Regression loss: 0.050199106335639954\n",
            "----------------\n",
            "Loss:  0.8547978363931179 \n",
            "Classification loss: 1.5977035760879517 \n",
            "Regression loss: 0.055946048349142075\n",
            "----------------\n",
            "Loss:  0.8842274658381939 \n",
            "Classification loss: 1.6607487201690674 \n",
            "Regression loss: 0.0538531057536602\n",
            "----------------\n",
            "Loss:  0.8239476792514324 \n",
            "Classification loss: 1.5444955825805664 \n",
            "Regression loss: 0.051699887961149216\n",
            "----------------\n",
            "Loss:  0.8350068777799606 \n",
            "Classification loss: 1.5643329620361328 \n",
            "Regression loss: 0.052840396761894226\n",
            "----------------\n",
            "Loss:  0.8572199866175652 \n",
            "Classification loss: 1.5820674896240234 \n",
            "Regression loss: 0.06618624180555344\n",
            "----------------\n",
            "Loss:  0.9101431965827942 \n",
            "Classification loss: 1.713388442993164 \n",
            "Regression loss: 0.05344897508621216\n",
            "----------------\n",
            "Loss:  0.7814649157226086 \n",
            "Classification loss: 1.4816653728485107 \n",
            "Regression loss: 0.040632229298353195\n",
            "----------------\n",
            "Loss:  0.9226645305752754 \n",
            "Classification loss: 1.729048490524292 \n",
            "Regression loss: 0.058140285313129425\n",
            "----------------\n",
            "Loss:  0.8730409294366837 \n",
            "Classification loss: 1.6642498970031738 \n",
            "Regression loss: 0.04091598093509674\n",
            "----------------\n",
            "Loss:  0.8253304287791252 \n",
            "Classification loss: 1.5525609254837036 \n",
            "Regression loss: 0.04904996603727341\n",
            "----------------\n",
            "Loss:  0.849322222173214 \n",
            "Classification loss: 1.6289311647415161 \n",
            "Regression loss: 0.0348566398024559\n",
            "----------------\n",
            "Loss:  0.7829019203782082 \n",
            "Classification loss: 1.432145595550537 \n",
            "Regression loss: 0.0668291226029396\n",
            "----------------\n",
            "Loss:  0.8586983643472195 \n",
            "Classification loss: 1.6063730716705322 \n",
            "Regression loss: 0.055511828511953354\n",
            "----------------\n",
            "Loss:  0.9029166921973228 \n",
            "Classification loss: 1.6790030002593994 \n",
            "Regression loss: 0.06341519206762314\n",
            "----------------\n",
            "Loss:  0.9102152995765209 \n",
            "Classification loss: 1.714322805404663 \n",
            "Regression loss: 0.05305389687418938\n",
            "----------------\n",
            "Loss:  0.7226691097021103 \n",
            "Classification loss: 1.360255479812622 \n",
            "Regression loss: 0.042541369795799255\n",
            "----------------\n",
            "Loss:  0.8641338869929314 \n",
            "Classification loss: 1.5810494422912598 \n",
            "Regression loss: 0.07360916584730148\n",
            "----------------\n",
            "Loss:  0.8171965107321739 \n",
            "Classification loss: 1.5461211204528809 \n",
            "Regression loss: 0.04413595050573349\n",
            "----------------\n",
            "Loss:  0.934155248105526 \n",
            "Classification loss: 1.724219799041748 \n",
            "Regression loss: 0.07204534858465195\n",
            "----------------\n",
            "Loss:  0.924842469394207 \n",
            "Classification loss: 1.7321758270263672 \n",
            "Regression loss: 0.05875455588102341\n",
            "----------------\n",
            "Loss:  0.8017709627747536 \n",
            "Classification loss: 1.525832176208496 \n",
            "Regression loss: 0.038854874670505524\n",
            "----------------\n",
            "Loss:  0.9045461900532246 \n",
            "Classification loss: 1.701831340789795 \n",
            "Regression loss: 0.0536305196583271\n",
            "----------------\n",
            "Loss:  0.932064987719059 \n",
            "Classification loss: 1.7401739358901978 \n",
            "Regression loss: 0.061978019773960114\n",
            "----------------\n",
            "Loss:  0.9068847708404064 \n",
            "Classification loss: 1.705108880996704 \n",
            "Regression loss: 0.05433033034205437\n",
            "----------------\n",
            "Loss:  0.8478867933154106 \n",
            "Classification loss: 1.587586760520935 \n",
            "Regression loss: 0.054093413054943085\n",
            "----------------\n",
            "Loss:  0.902759350836277 \n",
            "Classification loss: 1.7152557373046875 \n",
            "Regression loss: 0.04513148218393326\n",
            "----------------\n",
            "Loss:  0.9474116750061512 \n",
            "Classification loss: 1.7780170440673828 \n",
            "Regression loss: 0.05840315297245979\n",
            "----------------\n",
            "Loss:  0.74048225954175 \n",
            "Classification loss: 1.4290721416473389 \n",
            "Regression loss: 0.02594618871808052\n",
            "----------------\n",
            "Loss:  0.8749707825481892 \n",
            "Classification loss: 1.6252162456512451 \n",
            "Regression loss: 0.062362659722566605\n",
            "----------------\n",
            "Loss:  0.711640976369381 \n",
            "Classification loss: 1.3055846691131592 \n",
            "Regression loss: 0.05884864181280136\n",
            "----------------\n",
            "Loss:  0.8329367488622665 \n",
            "Classification loss: 1.5479753017425537 \n",
            "Regression loss: 0.058949097990989685\n",
            "----------------\n",
            "Loss:  0.8459918946027756 \n",
            "Classification loss: 1.582751989364624 \n",
            "Regression loss: 0.05461589992046356\n",
            "----------------\n",
            "Loss:  0.8764260336756706 \n",
            "Classification loss: 1.6228175163269043 \n",
            "Regression loss: 0.06501727551221848\n",
            "----------------\n",
            "Loss:  0.7898863926529884 \n",
            "Classification loss: 1.4472421407699585 \n",
            "Regression loss: 0.06626532226800919\n",
            "----------------\n",
            "Loss:  0.8056025970727205 \n",
            "Classification loss: 1.5520418882369995 \n",
            "Regression loss: 0.029581652954220772\n",
            "----------------\n",
            "Loss:  0.8397430777549744 \n",
            "Classification loss: 1.5338389873504639 \n",
            "Regression loss: 0.07282358407974243\n",
            "----------------\n",
            "Loss:  0.7970679104328156 \n",
            "Classification loss: 1.509049892425537 \n",
            "Regression loss: 0.042542964220047\n",
            "----------------\n",
            "Loss:  0.9111664816737175 \n",
            "Classification loss: 1.6996351480484009 \n",
            "Regression loss: 0.06134890764951706\n",
            "----------------\n",
            "Loss:  0.934025302529335 \n",
            "Classification loss: 1.7130504846572876 \n",
            "Regression loss: 0.07750006020069122\n",
            "----------------\n",
            "Loss:  0.8366701900959015 \n",
            "Classification loss: 1.5845900774002075 \n",
            "Regression loss: 0.04437515139579773\n",
            "----------------\n",
            "Loss:  0.8422246612608433 \n",
            "Classification loss: 1.593489408493042 \n",
            "Regression loss: 0.04547995701432228\n",
            "----------------\n",
            "Loss:  0.8511048182845116 \n",
            "Classification loss: 1.5721091032028198 \n",
            "Regression loss: 0.06505026668310165\n",
            "----------------\n",
            "Loss:  0.8956307917833328 \n",
            "Classification loss: 1.6760385036468506 \n",
            "Regression loss: 0.05761153995990753\n",
            "----------------\n",
            "Loss:  0.7914057821035385 \n",
            "Classification loss: 1.5380353927612305 \n",
            "Regression loss: 0.02238808572292328\n",
            "----------------\n",
            "Loss:  0.8912846706807613 \n",
            "Classification loss: 1.6742703914642334 \n",
            "Regression loss: 0.05414947494864464\n",
            "----------------\n",
            "Loss:  0.8643028140068054 \n",
            "Classification loss: 1.5799838304519653 \n",
            "Regression loss: 0.07431089878082275\n",
            "----------------\n",
            "Loss:  0.8267101496458054 \n",
            "Classification loss: 1.524715542793274 \n",
            "Regression loss: 0.0643523782491684\n",
            "----------------\n",
            "Loss:  1.077224776148796 \n",
            "Classification loss: 2.010044813156128 \n",
            "Regression loss: 0.07220236957073212\n",
            "----------------\n",
            "Loss:  0.7968309931457043 \n",
            "Classification loss: 1.5204036235809326 \n",
            "Regression loss: 0.03662918135523796\n",
            "----------------\n",
            "Loss:  0.8709788396954536 \n",
            "Classification loss: 1.6277834177017212 \n",
            "Regression loss: 0.05708713084459305\n",
            "----------------\n",
            "Loss:  0.8662262186408043 \n",
            "Classification loss: 1.6069023609161377 \n",
            "Regression loss: 0.06277503818273544\n",
            "----------------\n",
            "Loss:  0.8053542487323284 \n",
            "Classification loss: 1.5345768928527832 \n",
            "Regression loss: 0.03806580230593681\n",
            "----------------\n",
            "Loss:  0.8596857115626335 \n",
            "Classification loss: 1.6026172637939453 \n",
            "Regression loss: 0.05837707966566086\n",
            "----------------\n",
            "Loss:  0.8793399184942245 \n",
            "Classification loss: 1.6515979766845703 \n",
            "Regression loss: 0.05354093015193939\n",
            "----------------\n",
            "Loss:  0.8083777837455273 \n",
            "Classification loss: 1.503928780555725 \n",
            "Regression loss: 0.05641339346766472\n",
            "----------------\n",
            "Loss:  0.8144371826201677 \n",
            "Classification loss: 1.575827717781067 \n",
            "Regression loss: 0.026523323729634285\n",
            "----------------\n",
            "Loss:  0.7619494497776031 \n",
            "Classification loss: 1.4332613945007324 \n",
            "Regression loss: 0.04531875252723694\n",
            "----------------\n",
            "Loss:  0.8525392636656761 \n",
            "Classification loss: 1.5769662857055664 \n",
            "Regression loss: 0.06405612081289291\n",
            "----------------\n",
            "Loss:  0.8272517211735249 \n",
            "Classification loss: 1.577630639076233 \n",
            "Regression loss: 0.0384364016354084\n",
            "----------------\n",
            "Loss:  0.9302317053079605 \n",
            "Classification loss: 1.7809128761291504 \n",
            "Regression loss: 0.039775267243385315\n",
            "----------------\n",
            "Loss:  0.9311387240886688 \n",
            "Classification loss: 1.6970988512039185 \n",
            "Regression loss: 0.0825892984867096\n",
            "----------------\n",
            "Loss:  0.8903862908482552 \n",
            "Classification loss: 1.6689205169677734 \n",
            "Regression loss: 0.05592603236436844\n",
            "----------------\n",
            "Loss:  0.9022113978862762 \n",
            "Classification loss: 1.718268871307373 \n",
            "Regression loss: 0.04307696223258972\n",
            "----------------\n",
            "Loss:  0.8839695304632187 \n",
            "Classification loss: 1.6701512336730957 \n",
            "Regression loss: 0.04889391362667084\n",
            "----------------\n",
            "Loss:  0.823168721050024 \n",
            "Classification loss: 1.5604451894760132 \n",
            "Regression loss: 0.04294612631201744\n",
            "----------------\n",
            "Loss:  0.8736085034906864 \n",
            "Classification loss: 1.6610240936279297 \n",
            "Regression loss: 0.04309645667672157\n",
            "----------------\n",
            "Loss:  0.7328598238527775 \n",
            "Classification loss: 1.3915183544158936 \n",
            "Regression loss: 0.037100646644830704\n",
            "----------------\n",
            "Loss:  0.8873537331819534 \n",
            "Classification loss: 1.644531488418579 \n",
            "Regression loss: 0.06508798897266388\n",
            "----------------\n",
            "Loss:  0.8353407252579927 \n",
            "Classification loss: 1.6086082458496094 \n",
            "Regression loss: 0.031036602333188057\n",
            "----------------\n",
            "Loss:  0.7993383482098579 \n",
            "Classification loss: 1.539949893951416 \n",
            "Regression loss: 0.029363401234149933\n",
            "----------------\n",
            "Loss:  0.7552015110850334 \n",
            "Classification loss: 1.4315540790557861 \n",
            "Regression loss: 0.03942447155714035\n",
            "----------------\n",
            "Loss:  0.8484103940427303 \n",
            "Classification loss: 1.6266878843307495 \n",
            "Regression loss: 0.035066451877355576\n",
            "----------------\n",
            "Loss:  0.8332746475934982 \n",
            "Classification loss: 1.5660808086395264 \n",
            "Regression loss: 0.050234243273735046\n",
            "----------------\n",
            "Loss:  0.8587435781955719 \n",
            "Classification loss: 1.634958028793335 \n",
            "Regression loss: 0.04126456379890442\n",
            "----------------\n",
            "Loss:  0.7999017499387264 \n",
            "Classification loss: 1.4989690780639648 \n",
            "Regression loss: 0.050417210906744\n",
            "----------------\n",
            "Loss:  0.8377870097756386 \n",
            "Classification loss: 1.5570933818817139 \n",
            "Regression loss: 0.05924031883478165\n",
            "----------------\n",
            "Loss:  0.8604730442166328 \n",
            "Classification loss: 1.6253911256790161 \n",
            "Regression loss: 0.047777481377124786\n",
            "----------------\n",
            "Loss:  0.8429225534200668 \n",
            "Classification loss: 1.5992534160614014 \n",
            "Regression loss: 0.04329584538936615\n",
            "----------------\n",
            "Loss:  0.8172494098544121 \n",
            "Classification loss: 1.5323604345321655 \n",
            "Regression loss: 0.051069192588329315\n",
            "----------------\n",
            "Loss:  0.9622779525816441 \n",
            "Classification loss: 1.8356560468673706 \n",
            "Regression loss: 0.044449929147958755\n",
            "----------------\n",
            "Loss:  0.9167615622282028 \n",
            "Classification loss: 1.7044618129730225 \n",
            "Regression loss: 0.06453065574169159\n",
            "----------------\n",
            "Loss:  0.8847133480012417 \n",
            "Classification loss: 1.6666454076766968 \n",
            "Regression loss: 0.051390644162893295\n",
            "----------------\n",
            "Loss:  0.8801475018262863 \n",
            "Classification loss: 1.6260125637054443 \n",
            "Regression loss: 0.06714121997356415\n",
            "----------------\n",
            "Loss:  0.8629729188978672 \n",
            "Classification loss: 1.637550711631775 \n",
            "Regression loss: 0.04419756308197975\n",
            "----------------\n",
            "Loss:  0.837408684194088 \n",
            "Classification loss: 1.586830973625183 \n",
            "Regression loss: 0.04399319738149643\n",
            "----------------\n",
            "Loss:  0.7636071182787418 \n",
            "Classification loss: 1.436354398727417 \n",
            "Regression loss: 0.04542991891503334\n",
            "----------------\n",
            "Loss:  0.8775436729192734 \n",
            "Classification loss: 1.6110663414001465 \n",
            "Regression loss: 0.07201050221920013\n",
            "----------------\n",
            "Loss:  0.855828158557415 \n",
            "Classification loss: 1.5966181755065918 \n",
            "Regression loss: 0.05751907080411911\n",
            "----------------\n",
            "Loss:  0.7700077034533024 \n",
            "Classification loss: 1.4681718349456787 \n",
            "Regression loss: 0.03592178598046303\n",
            "----------------\n",
            "Loss:  0.8611987344920635 \n",
            "Classification loss: 1.6148536205291748 \n",
            "Regression loss: 0.05377192422747612\n",
            "----------------\n",
            "Loss:  0.8217745013535023 \n",
            "Classification loss: 1.5356382131576538 \n",
            "Regression loss: 0.05395539477467537\n",
            "----------------\n",
            "Loss:  0.9798652939498425 \n",
            "Classification loss: 1.8770337104797363 \n",
            "Regression loss: 0.04134843870997429\n",
            "----------------\n",
            "Loss:  0.813987672328949 \n",
            "Classification loss: 1.4808943271636963 \n",
            "Regression loss: 0.07354050874710083\n",
            "----------------\n",
            "Loss:  0.9096643812954426 \n",
            "Classification loss: 1.7014074325561523 \n",
            "Regression loss: 0.05896066501736641\n",
            "----------------\n",
            "Loss:  0.825465977191925 \n",
            "Classification loss: 1.5658226013183594 \n",
            "Regression loss: 0.04255467653274536\n",
            "----------------\n",
            "Loss:  0.8925227411091328 \n",
            "Classification loss: 1.6604149341583252 \n",
            "Regression loss: 0.06231527402997017\n",
            "----------------\n",
            "Loss:  0.8538539037108421 \n",
            "Classification loss: 1.6158959865570068 \n",
            "Regression loss: 0.045905910432338715\n",
            "----------------\n",
            "Loss:  0.8477694950997829 \n",
            "Classification loss: 1.6364463567733765 \n",
            "Regression loss: 0.02954631671309471\n",
            "----------------\n",
            "Loss:  0.7571585066616535 \n",
            "Classification loss: 1.4036459922790527 \n",
            "Regression loss: 0.05533551052212715\n",
            "----------------\n",
            "Loss:  0.9085578322410583 \n",
            "Classification loss: 1.7169766426086426 \n",
            "Regression loss: 0.05006951093673706\n",
            "----------------\n",
            "Loss:  0.7634083405137062 \n",
            "Classification loss: 1.4169812202453613 \n",
            "Regression loss: 0.05491773039102554\n",
            "----------------\n",
            "Loss:  0.8420994430780411 \n",
            "Classification loss: 1.609727144241333 \n",
            "Regression loss: 0.03723587095737457\n",
            "----------------\n",
            "Loss:  0.8625275567173958 \n",
            "Classification loss: 1.600097894668579 \n",
            "Regression loss: 0.06247860938310623\n",
            "----------------\n",
            "Loss:  0.9059860333800316 \n",
            "Classification loss: 1.7334201335906982 \n",
            "Regression loss: 0.039275966584682465\n",
            "----------------\n",
            "Loss:  0.8976715207099915 \n",
            "Classification loss: 1.6797685623168945 \n",
            "Regression loss: 0.05778723955154419\n",
            "----------------\n",
            "Loss:  0.8869830965995789 \n",
            "Classification loss: 1.6907603740692139 \n",
            "Regression loss: 0.041602909564971924\n",
            "----------------\n",
            "Loss:  0.8053699284791946 \n",
            "Classification loss: 1.5055286884307861 \n",
            "Regression loss: 0.052605584263801575\n",
            "----------------\n",
            "Loss:  0.7878032065927982 \n",
            "Classification loss: 1.4705264568328857 \n",
            "Regression loss: 0.05253997817635536\n",
            "----------------\n",
            "Loss:  0.8433323502540588 \n",
            "Classification loss: 1.578352689743042 \n",
            "Regression loss: 0.05415600538253784\n",
            "----------------\n",
            "Loss:  0.7953864522278309 \n",
            "Classification loss: 1.4981367588043213 \n",
            "Regression loss: 0.04631807282567024\n",
            "----------------\n",
            "Loss:  0.8389948308467865 \n",
            "Classification loss: 1.583989143371582 \n",
            "Regression loss: 0.04700025916099548\n",
            "----------------\n",
            "Loss:  0.7989148125052452 \n",
            "Classification loss: 1.511075735092163 \n",
            "Regression loss: 0.043376944959163666\n",
            "----------------\n",
            "Loss:  0.8996272273361683 \n",
            "Classification loss: 1.7005207538604736 \n",
            "Regression loss: 0.04936685040593147\n",
            "----------------\n",
            "Loss:  0.8125270269811153 \n",
            "Classification loss: 1.5367199182510376 \n",
            "Regression loss: 0.04416706785559654\n",
            "----------------\n",
            "Loss:  0.79209054261446 \n",
            "Classification loss: 1.491358757019043 \n",
            "Regression loss: 0.04641116410493851\n",
            "----------------\n",
            "Loss:  0.894273430109024 \n",
            "Classification loss: 1.6289173364639282 \n",
            "Regression loss: 0.07981476187705994\n",
            "----------------\n",
            "Loss:  0.9377007186412811 \n",
            "Classification loss: 1.7565088272094727 \n",
            "Regression loss: 0.0594463050365448\n",
            "----------------\n",
            "Loss:  0.854728564620018 \n",
            "Classification loss: 1.6338512897491455 \n",
            "Regression loss: 0.03780291974544525\n",
            "----------------\n",
            "Loss:  0.8512232564389706 \n",
            "Classification loss: 1.6195590496063232 \n",
            "Regression loss: 0.041443731635808945\n",
            "----------------\n",
            "Loss:  0.8719524070620537 \n",
            "Classification loss: 1.6247222423553467 \n",
            "Regression loss: 0.05959128588438034\n",
            "----------------\n",
            "Loss:  0.9060087688267231 \n",
            "Classification loss: 1.706301212310791 \n",
            "Regression loss: 0.05285816267132759\n",
            "----------------\n",
            "Loss:  0.9334083423018456 \n",
            "Classification loss: 1.6868746280670166 \n",
            "Regression loss: 0.08997102826833725\n",
            "----------------\n",
            "Loss:  0.8170178160071373 \n",
            "Classification loss: 1.550392985343933 \n",
            "Regression loss: 0.041821323335170746\n",
            "----------------\n",
            "Loss:  0.8340872302651405 \n",
            "Classification loss: 1.5576732158660889 \n",
            "Regression loss: 0.0552506223320961\n",
            "----------------\n",
            "Loss:  0.8466460630297661 \n",
            "Classification loss: 1.602211356163025 \n",
            "Regression loss: 0.04554038494825363\n",
            "----------------\n",
            "Loss:  0.7875442355871201 \n",
            "Classification loss: 1.4532839059829712 \n",
            "Regression loss: 0.06090228259563446\n",
            "----------------\n",
            "Loss:  0.890118807554245 \n",
            "Classification loss: 1.6469390392303467 \n",
            "Regression loss: 0.06664928793907166\n",
            "----------------\n",
            "Loss:  0.8554623425006866 \n",
            "Classification loss: 1.616943597793579 \n",
            "Regression loss: 0.046990543603897095\n",
            "----------------\n",
            "Loss:  0.7375009208917618 \n",
            "Classification loss: 1.4016834497451782 \n",
            "Regression loss: 0.03665919601917267\n",
            "----------------\n",
            "Loss:  0.763301320374012 \n",
            "Classification loss: 1.447448492050171 \n",
            "Regression loss: 0.039577074348926544\n",
            "----------------\n",
            "Loss:  0.8017097115516663 \n",
            "Classification loss: 1.5383580923080444 \n",
            "Regression loss: 0.03253066539764404\n",
            "----------------\n",
            "Loss:  0.8444312661886215 \n",
            "Classification loss: 1.599839448928833 \n",
            "Regression loss: 0.04451154172420502\n",
            "----------------\n",
            "Loss:  0.8062743693590164 \n",
            "Classification loss: 1.4914172887802124 \n",
            "Regression loss: 0.06056572496891022\n",
            "----------------\n",
            "Loss:  0.8952190056443214 \n",
            "Classification loss: 1.6647107601165771 \n",
            "Regression loss: 0.06286362558603287\n",
            "----------------\n",
            "Loss:  0.8678437918424606 \n",
            "Classification loss: 1.6095718145370483 \n",
            "Regression loss: 0.06305788457393646\n",
            "----------------\n",
            "Loss:  0.9103401303291321 \n",
            "Classification loss: 1.7296017408370972 \n",
            "Regression loss: 0.045539259910583496\n",
            "----------------\n",
            "Loss:  0.8747462444007397 \n",
            "Classification loss: 1.6915318965911865 \n",
            "Regression loss: 0.028980296105146408\n",
            "----------------\n",
            "Loss:  0.7894888818264008 \n",
            "Classification loss: 1.47371244430542 \n",
            "Regression loss: 0.052632659673690796\n",
            "----------------\n",
            "Loss:  0.8140853010118008 \n",
            "Classification loss: 1.556516408920288 \n",
            "Regression loss: 0.03582709655165672\n",
            "----------------\n",
            "Loss:  0.9545246213674545 \n",
            "Classification loss: 1.777151346206665 \n",
            "Regression loss: 0.06594894826412201\n",
            "----------------\n",
            "Loss:  0.8853113055229187 \n",
            "Classification loss: 1.6390546560287476 \n",
            "Regression loss: 0.06578397750854492\n",
            "----------------\n",
            "Loss:  0.9079030938446522 \n",
            "Classification loss: 1.722447156906128 \n",
            "Regression loss: 0.04667951539158821\n",
            "----------------\n",
            "Loss:  0.9002161473035812 \n",
            "Classification loss: 1.7238609790802002 \n",
            "Regression loss: 0.03828565776348114\n",
            "----------------\n",
            "Loss:  0.8010939210653305 \n",
            "Classification loss: 1.4581483602523804 \n",
            "Regression loss: 0.07201974093914032\n",
            "----------------\n",
            "Loss:  0.8963909670710564 \n",
            "Classification loss: 1.7181615829467773 \n",
            "Regression loss: 0.037310175597667694\n",
            "----------------\n",
            "Loss:  0.8857202529907227 \n",
            "Classification loss: 1.6725010871887207 \n",
            "Regression loss: 0.049469709396362305\n",
            "----------------\n",
            "Loss:  0.8541425280272961 \n",
            "Classification loss: 1.5930813550949097 \n",
            "Regression loss: 0.05760185047984123\n",
            "----------------\n",
            "Loss:  0.7629593871533871 \n",
            "Classification loss: 1.411801815032959 \n",
            "Regression loss: 0.05705847963690758\n",
            "----------------\n",
            "Loss:  0.7650640197098255 \n",
            "Classification loss: 1.444460391998291 \n",
            "Regression loss: 0.04283382371068001\n",
            "----------------\n",
            "Loss:  0.8240726739168167 \n",
            "Classification loss: 1.527733564376831 \n",
            "Regression loss: 0.060205891728401184\n",
            "----------------\n",
            "Loss:  0.782847948372364 \n",
            "Classification loss: 1.478661298751831 \n",
            "Regression loss: 0.04351729899644852\n",
            "----------------\n",
            "Loss:  0.9214113615453243 \n",
            "Classification loss: 1.7380856275558472 \n",
            "Regression loss: 0.05236854776740074\n",
            "----------------\n",
            "Loss:  0.7905361875891685 \n",
            "Classification loss: 1.4856233596801758 \n",
            "Regression loss: 0.04772450774908066\n",
            "----------------\n",
            "Loss:  0.9715688452124596 \n",
            "Classification loss: 1.79219651222229 \n",
            "Regression loss: 0.07547058910131454\n",
            "----------------\n",
            "Loss:  0.9260420724749565 \n",
            "Classification loss: 1.7637803554534912 \n",
            "Regression loss: 0.04415189474821091\n",
            "----------------\n",
            "Loss:  0.8135874159634113 \n",
            "Classification loss: 1.5075995922088623 \n",
            "Regression loss: 0.05978761985898018\n",
            "----------------\n",
            "Loss:  0.9007036406546831 \n",
            "Classification loss: 1.7506909370422363 \n",
            "Regression loss: 0.02535817213356495\n",
            "----------------\n",
            "Loss:  0.9383188169449568 \n",
            "Classification loss: 1.8211588859558105 \n",
            "Regression loss: 0.027739373967051506\n",
            "----------------\n",
            "Loss:  0.9784192964434624 \n",
            "Classification loss: 1.8193978071212769 \n",
            "Regression loss: 0.06872039288282394\n",
            "----------------\n",
            "Loss:  0.8336063586175442 \n",
            "Classification loss: 1.5925918817520142 \n",
            "Regression loss: 0.037310417741537094\n",
            "----------------\n",
            "Loss:  0.8729973584413528 \n",
            "Classification loss: 1.5903995037078857 \n",
            "Regression loss: 0.07779760658740997\n",
            "----------------\n",
            "Loss:  0.8649913147091866 \n",
            "Classification loss: 1.5926655530929565 \n",
            "Regression loss: 0.06865853816270828\n",
            "----------------\n",
            "Loss:  0.7746737226843834 \n",
            "Classification loss: 1.4547724723815918 \n",
            "Regression loss: 0.047287486493587494\n",
            "----------------\n",
            "Loss:  0.9104892760515213 \n",
            "Classification loss: 1.68131422996521 \n",
            "Regression loss: 0.06983216106891632\n",
            "----------------\n",
            "Loss:  0.8331335857510567 \n",
            "Classification loss: 1.5634695291519165 \n",
            "Regression loss: 0.05139882117509842\n",
            "----------------\n",
            "Loss:  0.9391277059912682 \n",
            "Classification loss: 1.7896316051483154 \n",
            "Regression loss: 0.04431190341711044\n",
            "----------------\n",
            "Loss:  1.0703815966844559 \n",
            "Classification loss: 1.9916454553604126 \n",
            "Regression loss: 0.07455886900424957\n",
            "----------------\n",
            "Loss:  0.8365213200449944 \n",
            "Classification loss: 1.5630533695220947 \n",
            "Regression loss: 0.05499463528394699\n",
            "----------------\n",
            "Loss:  0.7947247698903084 \n",
            "Classification loss: 1.4843018054962158 \n",
            "Regression loss: 0.05257386714220047\n",
            "----------------\n",
            "Loss:  0.6855591386556625 \n",
            "Classification loss: 1.2948118448257446 \n",
            "Regression loss: 0.03815321624279022\n",
            "----------------\n",
            "Loss:  0.8349042609333992 \n",
            "Classification loss: 1.5214699506759644 \n",
            "Regression loss: 0.07416928559541702\n",
            "----------------\n",
            "Loss:  0.853717528283596 \n",
            "Classification loss: 1.6058681011199951 \n",
            "Regression loss: 0.05078347772359848\n",
            "----------------\n",
            "Loss:  0.8413202911615372 \n",
            "Classification loss: 1.6276915073394775 \n",
            "Regression loss: 0.0274745374917984\n",
            "----------------\n",
            "Loss:  0.8356122523546219 \n",
            "Classification loss: 1.599137306213379 \n",
            "Regression loss: 0.036043599247932434\n",
            "----------------\n",
            "Loss:  0.7852779272943735 \n",
            "Classification loss: 1.5238065719604492 \n",
            "Regression loss: 0.023374641314148903\n",
            "----------------\n",
            "Loss:  0.8879644647240639 \n",
            "Classification loss: 1.6592923402786255 \n",
            "Regression loss: 0.05831829458475113\n",
            "----------------\n",
            "Loss:  0.8699535317718983 \n",
            "Classification loss: 1.65079665184021 \n",
            "Regression loss: 0.04455520585179329\n",
            "----------------\n",
            "Loss:  0.8193334676325321 \n",
            "Classification loss: 1.5409619808197021 \n",
            "Regression loss: 0.048852477222681046\n",
            "----------------\n",
            "Loss:  0.8054291531443596 \n",
            "Classification loss: 1.5068578720092773 \n",
            "Regression loss: 0.05200021713972092\n",
            "----------------\n",
            "Loss:  0.8693060949444771 \n",
            "Classification loss: 1.6314244270324707 \n",
            "Regression loss: 0.05359388142824173\n",
            "----------------\n",
            "Loss:  0.8082630001008511 \n",
            "Classification loss: 1.520829439163208 \n",
            "Regression loss: 0.047848280519247055\n",
            "----------------\n",
            "Loss:  0.7953422479331493 \n",
            "Classification loss: 1.4802367687225342 \n",
            "Regression loss: 0.05522386357188225\n",
            "----------------\n",
            "Loss:  0.8412304259836674 \n",
            "Classification loss: 1.5893691778182983 \n",
            "Regression loss: 0.046545837074518204\n",
            "----------------\n",
            "Loss:  0.8479648157954216 \n",
            "Classification loss: 1.5711557865142822 \n",
            "Regression loss: 0.06238692253828049\n",
            "----------------\n",
            "Loss:  0.8465762548148632 \n",
            "Classification loss: 1.5914885997772217 \n",
            "Regression loss: 0.050831954926252365\n",
            "----------------\n",
            "Loss:  0.8742977529764175 \n",
            "Classification loss: 1.6231498718261719 \n",
            "Regression loss: 0.0627228170633316\n",
            "----------------\n",
            "Loss:  0.9056957215070724 \n",
            "Classification loss: 1.7061176300048828 \n",
            "Regression loss: 0.05263690650463104\n",
            "----------------\n",
            "Loss:  0.779717706143856 \n",
            "Classification loss: 1.4513139724731445 \n",
            "Regression loss: 0.05406071990728378\n",
            "----------------\n",
            "Loss:  0.8417749106884003 \n",
            "Classification loss: 1.5855751037597656 \n",
            "Regression loss: 0.048987358808517456\n",
            "----------------\n",
            "Loss:  0.883251179009676 \n",
            "Classification loss: 1.7079261541366577 \n",
            "Regression loss: 0.029288101941347122\n",
            "----------------\n",
            "Loss:  0.8838288225233555 \n",
            "Classification loss: 1.6760001182556152 \n",
            "Regression loss: 0.04582876339554787\n",
            "----------------\n",
            "Loss:  0.8723296970129013 \n",
            "Classification loss: 1.6186439990997314 \n",
            "Regression loss: 0.06300769746303558\n",
            "----------------\n",
            "Loss:  0.8679396323859692 \n",
            "Classification loss: 1.646537184715271 \n",
            "Regression loss: 0.044671040028333664\n",
            "----------------\n",
            "Loss:  0.8679541274905205 \n",
            "Classification loss: 1.6275005340576172 \n",
            "Regression loss: 0.054203860461711884\n",
            "----------------\n",
            "Loss:  0.8963962942361832 \n",
            "Classification loss: 1.7161564826965332 \n",
            "Regression loss: 0.038318052887916565\n",
            "----------------\n",
            "Loss:  0.914581835269928 \n",
            "Classification loss: 1.708091139793396 \n",
            "Regression loss: 0.06053626537322998\n",
            "----------------\n",
            "Loss:  0.8696446120738983 \n",
            "Classification loss: 1.6208398342132568 \n",
            "Regression loss: 0.0592246949672699\n",
            "----------------\n",
            "Loss:  0.8623376414179802 \n",
            "Classification loss: 1.5984338521957397 \n",
            "Regression loss: 0.06312071532011032\n",
            "----------------\n",
            "Loss:  0.8667798303067684 \n",
            "Classification loss: 1.634816288948059 \n",
            "Regression loss: 0.049371685832738876\n",
            "----------------\n",
            "Loss:  0.7732478100806475 \n",
            "Classification loss: 1.489922285079956 \n",
            "Regression loss: 0.02828666754066944\n",
            "----------------\n",
            "Loss:  0.903557538986206 \n",
            "Classification loss: 1.6985225677490234 \n",
            "Regression loss: 0.054296255111694336\n",
            "----------------\n",
            "Loss:  0.9568205922842026 \n",
            "Classification loss: 1.772420883178711 \n",
            "Regression loss: 0.0706101506948471\n",
            "----------------\n",
            "Loss:  0.844149649143219 \n",
            "Classification loss: 1.6107683181762695 \n",
            "Regression loss: 0.03876549005508423\n",
            "----------------\n",
            "Loss:  0.8322455435991287 \n",
            "Classification loss: 1.503867268562317 \n",
            "Regression loss: 0.08031190931797028\n",
            "----------------\n",
            "Loss:  0.7291505225002766 \n",
            "Classification loss: 1.3909388780593872 \n",
            "Regression loss: 0.03368108347058296\n",
            "----------------\n",
            "Loss:  0.7253443244844675 \n",
            "Classification loss: 1.4025450944900513 \n",
            "Regression loss: 0.02407177723944187\n",
            "----------------\n",
            "Loss:  0.9076326340436935 \n",
            "Classification loss: 1.6438078880310059 \n",
            "Regression loss: 0.08572869002819061\n",
            "----------------\n",
            "Loss:  0.7855307161808014 \n",
            "Classification loss: 1.4791760444641113 \n",
            "Regression loss: 0.04594269394874573\n",
            "----------------\n",
            "Loss:  0.8050671964883804 \n",
            "Classification loss: 1.4965128898620605 \n",
            "Regression loss: 0.05681075155735016\n",
            "----------------\n",
            "Loss:  0.9209141582250595 \n",
            "Classification loss: 1.7585855722427368 \n",
            "Regression loss: 0.0416213721036911\n",
            "----------------\n",
            "Loss:  0.8750133961439133 \n",
            "Classification loss: 1.6265194416046143 \n",
            "Regression loss: 0.06175367534160614\n",
            "----------------\n",
            "Loss:  0.7789460197091103 \n",
            "Classification loss: 1.4736270904541016 \n",
            "Regression loss: 0.04213247448205948\n",
            "----------------\n",
            "Loss:  0.8116952963173389 \n",
            "Classification loss: 1.5019420385360718 \n",
            "Regression loss: 0.060724277049303055\n",
            "----------------\n",
            "Loss:  0.9312478750944138 \n",
            "Classification loss: 1.6839978694915771 \n",
            "Regression loss: 0.08924894034862518\n",
            "----------------\n",
            "Loss:  0.7760673873126507 \n",
            "Classification loss: 1.4277620315551758 \n",
            "Regression loss: 0.06218637153506279\n",
            "----------------\n",
            "Loss:  0.7225529737770557 \n",
            "Classification loss: 1.3897876739501953 \n",
            "Regression loss: 0.027659136801958084\n",
            "----------------\n",
            "Loss:  0.8358853086829185 \n",
            "Classification loss: 1.6042011976242065 \n",
            "Regression loss: 0.03378470987081528\n",
            "----------------\n",
            "Loss:  0.811741977930069 \n",
            "Classification loss: 1.489815592765808 \n",
            "Regression loss: 0.06683418154716492\n",
            "----------------\n",
            "Loss:  0.760234659537673 \n",
            "Classification loss: 1.4741945266723633 \n",
            "Regression loss: 0.023137396201491356\n",
            "----------------\n",
            "Loss:  0.8071951791644096 \n",
            "Classification loss: 1.512789011001587 \n",
            "Regression loss: 0.05080067366361618\n",
            "----------------\n",
            "Loss:  0.7698247842490673 \n",
            "Classification loss: 1.4583337306976318 \n",
            "Regression loss: 0.04065791890025139\n",
            "----------------\n",
            "Loss:  0.8450340777635574 \n",
            "Classification loss: 1.619709849357605 \n",
            "Regression loss: 0.035179153084754944\n",
            "----------------\n",
            "Loss:  0.8377361111342907 \n",
            "Classification loss: 1.5951807498931885 \n",
            "Regression loss: 0.04014573618769646\n",
            "----------------\n",
            "Loss:  0.7649465389549732 \n",
            "Classification loss: 1.4529192447662354 \n",
            "Regression loss: 0.038486916571855545\n",
            "----------------\n",
            "Loss:  0.7504669055342674 \n",
            "Classification loss: 1.418570876121521 \n",
            "Regression loss: 0.04118146747350693\n",
            "----------------\n",
            "Loss:  0.9469708949327469 \n",
            "Classification loss: 1.7811273336410522 \n",
            "Regression loss: 0.056407228112220764\n",
            "----------------\n",
            "Loss:  0.7265850640833378 \n",
            "Classification loss: 1.3769104480743408 \n",
            "Regression loss: 0.038129840046167374\n",
            "----------------\n",
            "Loss:  0.788065679371357 \n",
            "Classification loss: 1.4727650880813599 \n",
            "Regression loss: 0.05168313533067703\n",
            "----------------\n",
            "Loss:  0.9324532374739647 \n",
            "Classification loss: 1.7606701850891113 \n",
            "Regression loss: 0.05211814492940903\n",
            "----------------\n",
            "Loss:  0.6951499283313751 \n",
            "Classification loss: 1.3361241817474365 \n",
            "Regression loss: 0.02708783745765686\n",
            "----------------\n",
            "Loss:  0.9177748635411263 \n",
            "Classification loss: 1.7203917503356934 \n",
            "Regression loss: 0.05757898837327957\n",
            "----------------\n",
            "Loss:  0.8396663293242455 \n",
            "Classification loss: 1.5509599447250366 \n",
            "Regression loss: 0.06418635696172714\n",
            "----------------\n",
            "Loss:  0.8386327475309372 \n",
            "Classification loss: 1.623160481452942 \n",
            "Regression loss: 0.027052506804466248\n",
            "----------------\n",
            "Loss:  0.8971193768084049 \n",
            "Classification loss: 1.6824531555175781 \n",
            "Regression loss: 0.05589279904961586\n",
            "----------------\n",
            "Loss:  0.8025281578302383 \n",
            "Classification loss: 1.5211122035980225 \n",
            "Regression loss: 0.04197205603122711\n",
            "----------------\n",
            "Loss:  0.9309019185602665 \n",
            "Classification loss: 1.7813177108764648 \n",
            "Regression loss: 0.04024306312203407\n",
            "----------------\n",
            "Loss:  0.7380150184035301 \n",
            "Classification loss: 1.4076533317565918 \n",
            "Regression loss: 0.03418835252523422\n",
            "----------------\n",
            "Loss:  0.8264803066849709 \n",
            "Classification loss: 1.5956909656524658 \n",
            "Regression loss: 0.028634823858737946\n",
            "----------------\n",
            "Loss:  0.9724702090024948 \n",
            "Classification loss: 1.8329761028289795 \n",
            "Regression loss: 0.055982157588005066\n",
            "----------------\n",
            "Loss:  0.9572080746293068 \n",
            "Classification loss: 1.8113411664962769 \n",
            "Regression loss: 0.051537491381168365\n",
            "----------------\n",
            "Loss:  0.8516469821333885 \n",
            "Classification loss: 1.5770219564437866 \n",
            "Regression loss: 0.06313600391149521\n",
            "----------------\n",
            "Loss:  0.9017459154129028 \n",
            "Classification loss: 1.6808397769927979 \n",
            "Regression loss: 0.061326026916503906\n",
            "----------------\n",
            "Loss:  0.8117673993110657 \n",
            "Classification loss: 1.4830498695373535 \n",
            "Regression loss: 0.07024246454238892\n",
            "----------------\n",
            "Loss:  0.7581406012177467 \n",
            "Classification loss: 1.4246795177459717 \n",
            "Regression loss: 0.045800842344760895\n",
            "----------------\n",
            "Loss:  0.9300662465393543 \n",
            "Classification loss: 1.772652268409729 \n",
            "Regression loss: 0.04374011233448982\n",
            "----------------\n",
            "Loss:  0.7980426698923111 \n",
            "Classification loss: 1.4544780254364014 \n",
            "Regression loss: 0.07080365717411041\n",
            "----------------\n",
            "Loss:  0.8440482690930367 \n",
            "Classification loss: 1.561911940574646 \n",
            "Regression loss: 0.06309229880571365\n",
            "----------------\n",
            "Loss:  0.9289108067750931 \n",
            "Classification loss: 1.7058035135269165 \n",
            "Regression loss: 0.07600905001163483\n",
            "----------------\n",
            "Loss:  0.8291050717234612 \n",
            "Classification loss: 1.493695616722107 \n",
            "Regression loss: 0.08225726336240768\n",
            "----------------\n",
            "Loss:  0.8195467889308929 \n",
            "Classification loss: 1.5096254348754883 \n",
            "Regression loss: 0.0647340714931488\n",
            "----------------\n",
            "Loss:  0.8867452368140221 \n",
            "Classification loss: 1.6769739389419556 \n",
            "Regression loss: 0.04825826734304428\n",
            "----------------\n",
            "Loss:  0.8241047747433186 \n",
            "Classification loss: 1.542952537536621 \n",
            "Regression loss: 0.05262850597500801\n",
            "----------------\n",
            "Loss:  0.8575474321842194 \n",
            "Classification loss: 1.621999979019165 \n",
            "Regression loss: 0.04654744267463684\n",
            "----------------\n",
            "Loss:  0.8157244920730591 \n",
            "Classification loss: 1.5205191373825073 \n",
            "Regression loss: 0.05546492338180542\n",
            "----------------\n",
            "Loss:  0.7969996407628059 \n",
            "Classification loss: 1.465569257736206 \n",
            "Regression loss: 0.06421501189470291\n",
            "----------------\n",
            "Loss:  0.72646514326334 \n",
            "Classification loss: 1.3690003156661987 \n",
            "Regression loss: 0.04196498543024063\n",
            "----------------\n",
            "Loss:  0.8566807247698307 \n",
            "Classification loss: 1.6492246389389038 \n",
            "Regression loss: 0.0320684053003788\n",
            "----------------\n",
            "Loss:  0.8418470285832882 \n",
            "Classification loss: 1.5875245332717896 \n",
            "Regression loss: 0.04808476194739342\n",
            "----------------\n",
            "Loss:  0.858697384595871 \n",
            "Classification loss: 1.6091749668121338 \n",
            "Regression loss: 0.05410990118980408\n",
            "----------------\n",
            "Loss:  0.7944103069603443 \n",
            "Classification loss: 1.5067675113677979 \n",
            "Regression loss: 0.04102655127644539\n",
            "----------------\n",
            "Loss:  0.7607198320329189 \n",
            "Classification loss: 1.430397391319275 \n",
            "Regression loss: 0.04552113637328148\n",
            "----------------\n",
            "Loss:  0.8654474057257175 \n",
            "Classification loss: 1.6432691812515259 \n",
            "Regression loss: 0.043812815099954605\n",
            "----------------\n",
            "Loss:  0.7679628543555737 \n",
            "Classification loss: 1.435081958770752 \n",
            "Regression loss: 0.05042187497019768\n",
            "----------------\n",
            "Loss:  0.8060931712388992 \n",
            "Classification loss: 1.5293214321136475 \n",
            "Regression loss: 0.0414324551820755\n",
            "----------------\n",
            "Loss:  0.8605076931416988 \n",
            "Classification loss: 1.6014655828475952 \n",
            "Regression loss: 0.05977490171790123\n",
            "----------------\n",
            "Loss:  0.8801408782601357 \n",
            "Classification loss: 1.6408120393753052 \n",
            "Regression loss: 0.05973485857248306\n",
            "----------------\n",
            "Loss:  0.8645532876253128 \n",
            "Classification loss: 1.6392381191253662 \n",
            "Regression loss: 0.0449342280626297\n",
            "----------------\n",
            "Loss:  0.8716680482029915 \n",
            "Classification loss: 1.614393711090088 \n",
            "Regression loss: 0.06447119265794754\n",
            "----------------\n",
            "Loss:  0.8007520437240601 \n",
            "Classification loss: 1.5158908367156982 \n",
            "Regression loss: 0.04280662536621094\n",
            "----------------\n",
            "Loss:  0.8203284963965416 \n",
            "Classification loss: 1.5291905403137207 \n",
            "Regression loss: 0.055733226239681244\n",
            "----------------\n",
            "Loss:  0.8316349871456623 \n",
            "Classification loss: 1.5785560607910156 \n",
            "Regression loss: 0.042356956750154495\n",
            "----------------\n",
            "Loss:  0.8182349726557732 \n",
            "Classification loss: 1.5601308345794678 \n",
            "Regression loss: 0.038169555366039276\n",
            "----------------\n",
            "Loss:  0.7879117354750633 \n",
            "Classification loss: 1.4941105842590332 \n",
            "Regression loss: 0.04085644334554672\n",
            "----------------\n",
            "Loss:  0.736089687794447 \n",
            "Classification loss: 1.3480334281921387 \n",
            "Regression loss: 0.06207297369837761\n",
            "----------------\n",
            "Loss:  0.8772174790501595 \n",
            "Classification loss: 1.6554824113845825 \n",
            "Regression loss: 0.049476273357868195\n",
            "----------------\n",
            "Loss:  0.8812366910278797 \n",
            "Classification loss: 1.690418004989624 \n",
            "Regression loss: 0.0360276885330677\n",
            "----------------\n",
            "Loss:  0.880574643611908 \n",
            "Classification loss: 1.607722520828247 \n",
            "Regression loss: 0.07671338319778442\n",
            "----------------\n",
            "Loss:  0.8719564154744148 \n",
            "Classification loss: 1.6469286680221558 \n",
            "Regression loss: 0.048492081463336945\n",
            "----------------\n",
            "Loss:  0.8078235499560833 \n",
            "Classification loss: 1.5424970388412476 \n",
            "Regression loss: 0.03657503053545952\n",
            "----------------\n",
            "Loss:  0.7823162861168385 \n",
            "Classification loss: 1.4559500217437744 \n",
            "Regression loss: 0.05434127524495125\n",
            "----------------\n",
            "Loss:  0.9863790571689606 \n",
            "Classification loss: 1.8937361240386963 \n",
            "Regression loss: 0.03951099514961243\n",
            "----------------\n",
            "Loss:  0.8033042550086975 \n",
            "Classification loss: 1.527701497077942 \n",
            "Regression loss: 0.03945350646972656\n",
            "----------------\n",
            "Loss:  0.8922538720071316 \n",
            "Classification loss: 1.7143534421920776 \n",
            "Regression loss: 0.03507715091109276\n",
            "----------------\n",
            "Loss:  0.7970471158623695 \n",
            "Classification loss: 1.493384599685669 \n",
            "Regression loss: 0.050354816019535065\n",
            "----------------\n",
            "Loss:  0.7902254443615675 \n",
            "Classification loss: 1.5215932130813599 \n",
            "Regression loss: 0.029428837820887566\n",
            "----------------\n",
            "Loss:  0.7963262759149075 \n",
            "Classification loss: 1.510995864868164 \n",
            "Regression loss: 0.040828343480825424\n",
            "----------------\n",
            "Loss:  1.0005119815468788 \n",
            "Classification loss: 1.827714204788208 \n",
            "Regression loss: 0.08665487915277481\n",
            "----------------\n",
            "Loss:  0.8642371445894241 \n",
            "Classification loss: 1.597671389579773 \n",
            "Regression loss: 0.06540144979953766\n",
            "----------------\n",
            "Loss:  0.8560389280319214 \n",
            "Classification loss: 1.631062626838684 \n",
            "Regression loss: 0.040507614612579346\n",
            "----------------\n",
            "Loss:  0.7808424308896065 \n",
            "Classification loss: 1.403498888015747 \n",
            "Regression loss: 0.07909298688173294\n",
            "----------------\n",
            "Loss:  0.8452604860067368 \n",
            "Classification loss: 1.5664998292922974 \n",
            "Regression loss: 0.062010571360588074\n",
            "----------------\n",
            "Loss:  0.8128050938248634 \n",
            "Classification loss: 1.5083979368209839 \n",
            "Regression loss: 0.05860612541437149\n",
            "----------------\n",
            "Loss:  0.7941726855933666 \n",
            "Classification loss: 1.4659209251403809 \n",
            "Regression loss: 0.06121222302317619\n",
            "----------------\n",
            "Loss:  0.8034466803073883 \n",
            "Classification loss: 1.5114061832427979 \n",
            "Regression loss: 0.04774358868598938\n",
            "----------------\n",
            "Loss:  0.8003855347633362 \n",
            "Classification loss: 1.4843952655792236 \n",
            "Regression loss: 0.058187901973724365\n",
            "----------------\n",
            "Loss:  0.7792613953351974 \n",
            "Classification loss: 1.4444785118103027 \n",
            "Regression loss: 0.05702213943004608\n",
            "----------------\n",
            "Loss:  0.8514932245016098 \n",
            "Classification loss: 1.6195244789123535 \n",
            "Regression loss: 0.041730985045433044\n",
            "----------------\n",
            "Loss:  0.806792326271534 \n",
            "Classification loss: 1.5221161842346191 \n",
            "Regression loss: 0.045734234154224396\n",
            "----------------\n",
            "Loss:  0.8442648574709892 \n",
            "Classification loss: 1.6029797792434692 \n",
            "Regression loss: 0.04277496784925461\n",
            "----------------\n",
            "Loss:  0.8020788542926311 \n",
            "Classification loss: 1.5435261726379395 \n",
            "Regression loss: 0.030315767973661423\n",
            "----------------\n",
            "Loss:  0.7432654537260532 \n",
            "Classification loss: 1.418412446975708 \n",
            "Regression loss: 0.034059230238199234\n",
            "----------------\n",
            "Loss:  0.7551193758845329 \n",
            "Classification loss: 1.4151530265808105 \n",
            "Regression loss: 0.047542862594127655\n",
            "----------------\n",
            "Loss:  0.8476309180259705 \n",
            "Classification loss: 1.609788179397583 \n",
            "Regression loss: 0.042736828327178955\n",
            "----------------\n",
            "Loss:  0.8366526961326599 \n",
            "Classification loss: 1.5806785821914673 \n",
            "Regression loss: 0.04631340503692627\n",
            "----------------\n",
            "Loss:  0.9038202576339245 \n",
            "Classification loss: 1.7407042980194092 \n",
            "Regression loss: 0.033468108624219894\n",
            "----------------\n",
            "Loss:  0.8218831121921539 \n",
            "Classification loss: 1.5170071125030518 \n",
            "Regression loss: 0.06337955594062805\n",
            "----------------\n",
            "Loss:  0.8765822350978851 \n",
            "Classification loss: 1.6826707124710083 \n",
            "Regression loss: 0.03524687886238098\n",
            "----------------\n",
            "Loss:  0.7740819603204727 \n",
            "Classification loss: 1.468959927558899 \n",
            "Regression loss: 0.039601996541023254\n",
            "----------------\n",
            "Loss:  0.8431260511279106 \n",
            "Classification loss: 1.5876178741455078 \n",
            "Regression loss: 0.04931711405515671\n",
            "----------------\n",
            "Loss:  0.8054975606501102 \n",
            "Classification loss: 1.4895548820495605 \n",
            "Regression loss: 0.06072011962532997\n",
            "----------------\n",
            "Loss:  0.8858079388737679 \n",
            "Classification loss: 1.6846449375152588 \n",
            "Regression loss: 0.04348547011613846\n",
            "----------------\n",
            "Loss:  0.7897818647325039 \n",
            "Classification loss: 1.485426425933838 \n",
            "Regression loss: 0.047068651765584946\n",
            "----------------\n",
            "Loss:  0.8760966286063194 \n",
            "Classification loss: 1.6745526790618896 \n",
            "Regression loss: 0.0388202890753746\n",
            "----------------\n",
            "Loss:  0.8473997786641121 \n",
            "Classification loss: 1.5655157566070557 \n",
            "Regression loss: 0.06464190036058426\n",
            "----------------\n",
            "Loss:  0.8610449731349945 \n",
            "Classification loss: 1.585360050201416 \n",
            "Regression loss: 0.0683649480342865\n",
            "----------------\n",
            "Loss:  0.7488785907626152 \n",
            "Classification loss: 1.4541682004928589 \n",
            "Regression loss: 0.02179449051618576\n",
            "----------------\n",
            "Loss:  0.7922420427203178 \n",
            "Classification loss: 1.4715166091918945 \n",
            "Regression loss: 0.056483738124370575\n",
            "----------------\n",
            "Loss:  0.8175012171268463 \n",
            "Classification loss: 1.5147486925125122 \n",
            "Regression loss: 0.06012687087059021\n",
            "----------------\n",
            "Loss:  0.8827242329716682 \n",
            "Classification loss: 1.6365469694137573 \n",
            "Regression loss: 0.06445074826478958\n",
            "----------------\n",
            "Loss:  0.7427179180085659 \n",
            "Classification loss: 1.3997141122817993 \n",
            "Regression loss: 0.042860861867666245\n",
            "----------------\n",
            "Loss:  0.7840931862592697 \n",
            "Classification loss: 1.4869199991226196 \n",
            "Regression loss: 0.0406331866979599\n",
            "----------------\n",
            "Loss:  0.693893700838089 \n",
            "Classification loss: 1.3242658376693726 \n",
            "Regression loss: 0.03176078200340271\n",
            "----------------\n",
            "Loss:  0.8174989111721516 \n",
            "Classification loss: 1.5103600025177002 \n",
            "Regression loss: 0.06231890991330147\n",
            "----------------\n",
            "Loss:  0.7027248553931713 \n",
            "Classification loss: 1.3510382175445557 \n",
            "Regression loss: 0.02720574662089348\n",
            "----------------\n",
            "Loss:  0.8686533719301224 \n",
            "Classification loss: 1.6486155986785889 \n",
            "Regression loss: 0.04434557259082794\n",
            "----------------\n",
            "Loss:  0.7959916666150093 \n",
            "Classification loss: 1.535045862197876 \n",
            "Regression loss: 0.02846873551607132\n",
            "----------------\n",
            "Loss:  0.7848172932863235 \n",
            "Classification loss: 1.507441520690918 \n",
            "Regression loss: 0.031096532940864563\n",
            "----------------\n",
            "Loss:  0.8479168266057968 \n",
            "Classification loss: 1.6185455322265625 \n",
            "Regression loss: 0.038644060492515564\n",
            "----------------\n",
            "Loss:  0.8552311509847641 \n",
            "Classification loss: 1.5796908140182495 \n",
            "Regression loss: 0.06538574397563934\n",
            "----------------\n",
            "Loss:  0.838706236332655 \n",
            "Classification loss: 1.5546741485595703 \n",
            "Regression loss: 0.0613691620528698\n",
            "----------------\n",
            "Loss:  0.875940628349781 \n",
            "Classification loss: 1.6371562480926514 \n",
            "Regression loss: 0.05736250430345535\n",
            "----------------\n",
            "Loss:  0.8138425145298243 \n",
            "Classification loss: 1.568099856376648 \n",
            "Regression loss: 0.029792586341500282\n",
            "----------------\n",
            "Loss:  0.7603271491825581 \n",
            "Classification loss: 1.43435800075531 \n",
            "Regression loss: 0.04314814880490303\n",
            "----------------\n",
            "Loss:  0.8765263482928276 \n",
            "Classification loss: 1.6043758392333984 \n",
            "Regression loss: 0.07433842867612839\n",
            "----------------\n",
            "Loss:  0.7854927033185959 \n",
            "Classification loss: 1.5071899890899658 \n",
            "Regression loss: 0.031897708773612976\n",
            "----------------\n",
            "Loss:  0.8384193405508995 \n",
            "Classification loss: 1.5873327255249023 \n",
            "Regression loss: 0.044752977788448334\n",
            "----------------\n",
            "Loss:  0.8352361917495728 \n",
            "Classification loss: 1.5637882947921753 \n",
            "Regression loss: 0.05334204435348511\n",
            "----------------\n",
            "Loss:  0.8553734235465527 \n",
            "Classification loss: 1.6278176307678223 \n",
            "Regression loss: 0.041464608162641525\n",
            "----------------\n",
            "Loss:  0.933464527130127 \n",
            "Classification loss: 1.7313228845596313 \n",
            "Regression loss: 0.06780308485031128\n",
            "----------------\n",
            "Loss:  0.8227698914706707 \n",
            "Classification loss: 1.5625218152999878 \n",
            "Regression loss: 0.041508983820676804\n",
            "----------------\n",
            "Loss:  0.9248770698904991 \n",
            "Classification loss: 1.7297298908233643 \n",
            "Regression loss: 0.060012124478816986\n",
            "----------------\n",
            "Loss:  0.8528686314821243 \n",
            "Classification loss: 1.6046171188354492 \n",
            "Regression loss: 0.05056007206439972\n",
            "----------------\n",
            "Loss:  0.7581110075116158 \n",
            "Classification loss: 1.4099011421203613 \n",
            "Regression loss: 0.05316043645143509\n",
            "----------------\n",
            "Loss:  0.733105257153511 \n",
            "Classification loss: 1.3835490942001343 \n",
            "Regression loss: 0.04133071005344391\n",
            "----------------\n",
            "Loss:  0.8808121345937252 \n",
            "Classification loss: 1.6600819826126099 \n",
            "Regression loss: 0.05077114328742027\n",
            "----------------\n",
            "Loss:  0.7842256687581539 \n",
            "Classification loss: 1.5024844408035278 \n",
            "Regression loss: 0.03298344835639\n",
            "----------------\n",
            "Loss:  0.9320176988840103 \n",
            "Classification loss: 1.7453029155731201 \n",
            "Regression loss: 0.059366241097450256\n",
            "----------------\n",
            "Loss:  0.7774936258792877 \n",
            "Classification loss: 1.4600614309310913 \n",
            "Regression loss: 0.047462910413742065\n",
            "----------------\n",
            "Loss:  0.7837719991803169 \n",
            "Classification loss: 1.4431164264678955 \n",
            "Regression loss: 0.06221378594636917\n",
            "----------------\n",
            "Loss:  0.825145997107029 \n",
            "Classification loss: 1.559998869895935 \n",
            "Regression loss: 0.04514656215906143\n",
            "----------------\n",
            "Loss:  0.8358847200870514 \n",
            "Classification loss: 1.5225515365600586 \n",
            "Regression loss: 0.0746089518070221\n",
            "----------------\n",
            "Loss:  0.7968230806291103 \n",
            "Classification loss: 1.494023084640503 \n",
            "Regression loss: 0.04981153830885887\n",
            "----------------\n",
            "Loss:  0.8376937620341778 \n",
            "Classification loss: 1.5863041877746582 \n",
            "Regression loss: 0.04454166814684868\n",
            "----------------\n",
            "Loss:  0.8366325385868549 \n",
            "Classification loss: 1.560262680053711 \n",
            "Regression loss: 0.056501198559999466\n",
            "----------------\n",
            "Loss:  0.9939795359969139 \n",
            "Classification loss: 1.8628175258636475 \n",
            "Regression loss: 0.06257077306509018\n",
            "----------------\n",
            "Loss:  0.900360744446516 \n",
            "Classification loss: 1.699615716934204 \n",
            "Regression loss: 0.050552885979413986\n",
            "----------------\n",
            "Loss:  0.8379037976264954 \n",
            "Classification loss: 1.5292651653289795 \n",
            "Regression loss: 0.07327121496200562\n",
            "----------------\n",
            "Loss:  0.8445234224200249 \n",
            "Classification loss: 1.5539047718048096 \n",
            "Regression loss: 0.06757103651762009\n",
            "----------------\n",
            "Loss:  0.8986883275210857 \n",
            "Classification loss: 1.6784303188323975 \n",
            "Regression loss: 0.05947316810488701\n",
            "----------------\n",
            "Loss:  0.848337285220623 \n",
            "Classification loss: 1.577329158782959 \n",
            "Regression loss: 0.059672705829143524\n",
            "----------------\n",
            "Loss:  0.921757698059082 \n",
            "Classification loss: 1.7340644598007202 \n",
            "Regression loss: 0.054725468158721924\n",
            "----------------\n",
            "Loss:  0.8701602667570114 \n",
            "Classification loss: 1.6519551277160645 \n",
            "Regression loss: 0.04418270289897919\n",
            "----------------\n",
            "Loss:  0.8610860258340836 \n",
            "Classification loss: 1.60825777053833 \n",
            "Regression loss: 0.05695714056491852\n",
            "----------------\n",
            "Loss:  0.8648298196494579 \n",
            "Classification loss: 1.6443665027618408 \n",
            "Regression loss: 0.04264656826853752\n",
            "----------------\n",
            "Loss:  0.8916733860969543 \n",
            "Classification loss: 1.6907870769500732 \n",
            "Regression loss: 0.046279847621917725\n",
            "----------------\n",
            "Loss:  0.9013965129852295 \n",
            "Classification loss: 1.6757841110229492 \n",
            "Regression loss: 0.06350445747375488\n",
            "----------------\n",
            "Loss:  0.8595283403992653 \n",
            "Classification loss: 1.6325761079788208 \n",
            "Regression loss: 0.04324028640985489\n",
            "----------------\n",
            "Loss:  0.7145907431840897 \n",
            "Classification loss: 1.364954948425293 \n",
            "Regression loss: 0.032113268971443176\n",
            "----------------\n",
            "Loss:  0.7853326350450516 \n",
            "Classification loss: 1.445397138595581 \n",
            "Regression loss: 0.06263406574726105\n",
            "----------------\n",
            "Loss:  0.8830622956156731 \n",
            "Classification loss: 1.6707689762115479 \n",
            "Regression loss: 0.04767780750989914\n",
            "----------------\n",
            "Loss:  0.8984347730875015 \n",
            "Classification loss: 1.6894056797027588 \n",
            "Regression loss: 0.05373193323612213\n",
            "----------------\n",
            "Loss:  0.864157147705555 \n",
            "Classification loss: 1.6104497909545898 \n",
            "Regression loss: 0.05893225222826004\n",
            "----------------\n",
            "Loss:  0.9433386959135532 \n",
            "Classification loss: 1.7833197116851807 \n",
            "Regression loss: 0.051678840070962906\n",
            "----------------\n",
            "Loss:  0.9215465672314167 \n",
            "Classification loss: 1.75332772731781 \n",
            "Regression loss: 0.04488270357251167\n",
            "----------------\n",
            "Loss:  0.8583058901131153 \n",
            "Classification loss: 1.6191092729568481 \n",
            "Regression loss: 0.04875125363469124\n",
            "----------------\n",
            "Loss:  0.7630762755870819 \n",
            "Classification loss: 1.41819429397583 \n",
            "Regression loss: 0.05397912859916687\n",
            "----------------\n",
            "Loss:  0.8412148579955101 \n",
            "Classification loss: 1.577144980430603 \n",
            "Regression loss: 0.05264236778020859\n",
            "----------------\n",
            "Loss:  0.8139951080083847 \n",
            "Classification loss: 1.5125421285629272 \n",
            "Regression loss: 0.05772404372692108\n",
            "----------------\n",
            "Loss:  0.8215683847665787 \n",
            "Classification loss: 1.540510892868042 \n",
            "Regression loss: 0.05131293833255768\n",
            "----------------\n",
            "Loss:  0.963308684527874 \n",
            "Classification loss: 1.8061739206314087 \n",
            "Regression loss: 0.06022172421216965\n",
            "----------------\n",
            "Loss:  0.863326720893383 \n",
            "Classification loss: 1.62615966796875 \n",
            "Regression loss: 0.050246886909008026\n",
            "----------------\n",
            "Loss:  0.979715570807457 \n",
            "Classification loss: 1.8554463386535645 \n",
            "Regression loss: 0.051992401480674744\n",
            "----------------\n",
            "Loss:  0.8023577854037285 \n",
            "Classification loss: 1.5040109157562256 \n",
            "Regression loss: 0.05035232752561569\n",
            "----------------\n",
            "Loss:  0.9636887013912201 \n",
            "Classification loss: 1.7595205307006836 \n",
            "Regression loss: 0.0839284360408783\n",
            "----------------\n",
            "Loss:  0.7746947854757309 \n",
            "Classification loss: 1.4528498649597168 \n",
            "Regression loss: 0.0482698529958725\n",
            "----------------\n",
            "Loss:  0.8673446141183376 \n",
            "Classification loss: 1.6570392847061157 \n",
            "Regression loss: 0.03882497176527977\n",
            "----------------\n",
            "Loss:  0.8535966202616692 \n",
            "Classification loss: 1.6256858110427856 \n",
            "Regression loss: 0.04075371474027634\n",
            "----------------\n",
            "Loss:  0.8117839619517326 \n",
            "Classification loss: 1.5093493461608887 \n",
            "Regression loss: 0.0571092888712883\n",
            "----------------\n",
            "Loss:  0.8712505884468555 \n",
            "Classification loss: 1.6399672031402588 \n",
            "Regression loss: 0.05126698687672615\n",
            "----------------\n",
            "Loss:  0.8223616555333138 \n",
            "Classification loss: 1.5286409854888916 \n",
            "Regression loss: 0.05804116278886795\n",
            "----------------\n",
            "Loss:  0.9184501692652702 \n",
            "Classification loss: 1.7402416467666626 \n",
            "Regression loss: 0.048329345881938934\n",
            "----------------\n",
            "Loss:  0.8695860709995031 \n",
            "Classification loss: 1.6810309886932373 \n",
            "Regression loss: 0.029070576652884483\n",
            "----------------\n",
            "Loss:  0.8010761216282845 \n",
            "Classification loss: 1.4867596626281738 \n",
            "Regression loss: 0.05769629031419754\n",
            "----------------\n",
            "Loss:  0.9134355708956718 \n",
            "Classification loss: 1.7233896255493164 \n",
            "Regression loss: 0.05174075812101364\n",
            "----------------\n",
            "Loss:  0.8647500872612 \n",
            "Classification loss: 1.6575652360916138 \n",
            "Regression loss: 0.035967469215393066\n",
            "----------------\n",
            "Loss:  0.9339717105031013 \n",
            "Classification loss: 1.7380062341690063 \n",
            "Regression loss: 0.06496859341859818\n",
            "----------------\n",
            "Loss:  0.9066590406000614 \n",
            "Classification loss: 1.6914594173431396 \n",
            "Regression loss: 0.06092933192849159\n",
            "----------------\n",
            "Loss:  0.8215663358569145 \n",
            "Classification loss: 1.5087599754333496 \n",
            "Regression loss: 0.06718634814023972\n",
            "----------------\n",
            "Loss:  0.9114100597798824 \n",
            "Classification loss: 1.7197191715240479 \n",
            "Regression loss: 0.051550474017858505\n",
            "----------------\n",
            "Loss:  0.8835812360048294 \n",
            "Classification loss: 1.5811680555343628 \n",
            "Regression loss: 0.09299720823764801\n",
            "----------------\n",
            "Loss:  0.831116184592247 \n",
            "Classification loss: 1.5116080045700073 \n",
            "Regression loss: 0.07531218230724335\n",
            "----------------\n",
            "Loss:  0.7141530960798264 \n",
            "Classification loss: 1.3658612966537476 \n",
            "Regression loss: 0.031222447752952576\n",
            "----------------\n",
            "Loss:  0.8909921571612358 \n",
            "Classification loss: 1.654532790184021 \n",
            "Regression loss: 0.06372576206922531\n",
            "----------------\n",
            "Loss:  0.8715397715568542 \n",
            "Classification loss: 1.6077665090560913 \n",
            "Regression loss: 0.0676565170288086\n",
            "----------------\n",
            "Loss:  0.9134458303451538 \n",
            "Classification loss: 1.6591529846191406 \n",
            "Regression loss: 0.0838693380355835\n",
            "----------------\n",
            "Loss:  0.800695825368166 \n",
            "Classification loss: 1.4910295009613037 \n",
            "Regression loss: 0.055181074887514114\n",
            "----------------\n",
            "Loss:  0.8695172891020775 \n",
            "Classification loss: 1.620424509048462 \n",
            "Regression loss: 0.05930503457784653\n",
            "----------------\n",
            "Loss:  0.7317016758024693 \n",
            "Classification loss: 1.366133213043213 \n",
            "Regression loss: 0.04863506928086281\n",
            "----------------\n",
            "Loss:  0.8858475014567375 \n",
            "Classification loss: 1.6657297611236572 \n",
            "Regression loss: 0.052982620894908905\n",
            "----------------\n",
            "Loss:  0.7616177871823311 \n",
            "Classification loss: 1.3999639749526978 \n",
            "Regression loss: 0.06163579970598221\n",
            "----------------\n",
            "Loss:  0.8226802200078964 \n",
            "Classification loss: 1.5378601551055908 \n",
            "Regression loss: 0.05375014245510101\n",
            "----------------\n",
            "Loss:  0.8937163054943085 \n",
            "Classification loss: 1.7235448360443115 \n",
            "Regression loss: 0.03194388747215271\n",
            "----------------\n",
            "Loss:  0.8246830776333809 \n",
            "Classification loss: 1.5513705015182495 \n",
            "Regression loss: 0.048997826874256134\n",
            "----------------\n",
            "Loss:  0.8430776484310627 \n",
            "Classification loss: 1.6209657192230225 \n",
            "Regression loss: 0.03259478881955147\n",
            "----------------\n",
            "Loss:  0.7817732281982899 \n",
            "Classification loss: 1.465559959411621 \n",
            "Regression loss: 0.048993248492479324\n",
            "----------------\n",
            "Loss:  0.9200138710439205 \n",
            "Classification loss: 1.7246665954589844 \n",
            "Regression loss: 0.05768057331442833\n",
            "----------------\n",
            "Loss:  0.8035083524882793 \n",
            "Classification loss: 1.5052859783172607 \n",
            "Regression loss: 0.05086536332964897\n",
            "----------------\n",
            "Loss:  0.8475842960178852 \n",
            "Classification loss: 1.628570795059204 \n",
            "Regression loss: 0.03329889848828316\n",
            "----------------\n",
            "Loss:  0.8902379125356674 \n",
            "Classification loss: 1.6592376232147217 \n",
            "Regression loss: 0.06061910092830658\n",
            "----------------\n",
            "Loss:  0.8202065005898476 \n",
            "Classification loss: 1.5296940803527832 \n",
            "Regression loss: 0.05535946041345596\n",
            "----------------\n",
            "Loss:  0.8107129260897636 \n",
            "Classification loss: 1.5472004413604736 \n",
            "Regression loss: 0.037112705409526825\n",
            "----------------\n",
            "Loss:  0.812841983512044 \n",
            "Classification loss: 1.5686500072479248 \n",
            "Regression loss: 0.02851697988808155\n",
            "----------------\n",
            "Loss:  0.6835947819054127 \n",
            "Classification loss: 1.3093321323394775 \n",
            "Regression loss: 0.028928715735673904\n",
            "----------------\n",
            "Loss:  0.9780969470739365 \n",
            "Classification loss: 1.817137598991394 \n",
            "Regression loss: 0.06952814757823944\n",
            "----------------\n",
            "Loss:  0.7551956474781036 \n",
            "Classification loss: 1.4383153915405273 \n",
            "Regression loss: 0.036037951707839966\n",
            "----------------\n",
            "Loss:  0.8961853235960007 \n",
            "Classification loss: 1.6779881715774536 \n",
            "Regression loss: 0.057191237807273865\n",
            "----------------\n",
            "Loss:  0.8318466022610664 \n",
            "Classification loss: 1.533310055732727 \n",
            "Regression loss: 0.06519157439470291\n",
            "----------------\n",
            "Loss:  0.8731608167290688 \n",
            "Classification loss: 1.6023069620132446 \n",
            "Regression loss: 0.07200733572244644\n",
            "----------------\n",
            "Loss:  0.8302024155855179 \n",
            "Classification loss: 1.5646753311157227 \n",
            "Regression loss: 0.047864750027656555\n",
            "----------------\n",
            "Loss:  0.8485112637281418 \n",
            "Classification loss: 1.5812379121780396 \n",
            "Regression loss: 0.05789230763912201\n",
            "----------------\n",
            "Loss:  0.8434765040874481 \n",
            "Classification loss: 1.5981570482254028 \n",
            "Regression loss: 0.044397979974746704\n",
            "----------------\n",
            "Loss:  0.8904134035110474 \n",
            "Classification loss: 1.700347661972046 \n",
            "Regression loss: 0.040239572525024414\n",
            "----------------\n",
            "Loss:  0.9040047600865364 \n",
            "Classification loss: 1.702152967453003 \n",
            "Regression loss: 0.05292827636003494\n",
            "----------------\n",
            "Loss:  0.7674841955304146 \n",
            "Classification loss: 1.4044897556304932 \n",
            "Regression loss: 0.065239317715168\n",
            "----------------\n",
            "Loss:  0.9085027649998665 \n",
            "Classification loss: 1.6777070760726929 \n",
            "Regression loss: 0.06964922696352005\n",
            "----------------\n",
            "Loss:  0.8074045814573765 \n",
            "Classification loss: 1.5477304458618164 \n",
            "Regression loss: 0.03353935852646828\n",
            "----------------\n",
            "Loss:  0.8742281384766102 \n",
            "Classification loss: 1.6830847263336182 \n",
            "Regression loss: 0.0326857753098011\n",
            "----------------\n",
            "Loss:  0.8689043186604977 \n",
            "Classification loss: 1.6402348279953003 \n",
            "Regression loss: 0.04878690466284752\n",
            "----------------\n",
            "Loss:  0.8202223852276802 \n",
            "Classification loss: 1.5258400440216064 \n",
            "Regression loss: 0.057302363216876984\n",
            "----------------\n",
            "Loss:  0.8308814242482185 \n",
            "Classification loss: 1.5379953384399414 \n",
            "Regression loss: 0.06188375502824783\n",
            "----------------\n",
            "Loss:  0.7713249921798706 \n",
            "Classification loss: 1.4394404888153076 \n",
            "Regression loss: 0.0516047477722168\n",
            "----------------\n",
            "Loss:  0.9155405908823013 \n",
            "Classification loss: 1.6482574939727783 \n",
            "Regression loss: 0.09141184389591217\n",
            "----------------\n",
            "Loss:  0.8487714491784573 \n",
            "Classification loss: 1.6003458499908447 \n",
            "Regression loss: 0.0485985241830349\n",
            "----------------\n",
            "Loss:  0.8040689490735531 \n",
            "Classification loss: 1.4851405620574951 \n",
            "Regression loss: 0.06149866804480553\n",
            "----------------\n",
            "Loss:  0.8141862824559212 \n",
            "Classification loss: 1.5370289087295532 \n",
            "Regression loss: 0.04567182809114456\n",
            "----------------\n",
            "Loss:  0.7459744401276112 \n",
            "Classification loss: 1.4057546854019165 \n",
            "Regression loss: 0.04309709742665291\n",
            "----------------\n",
            "Loss:  0.7862555384635925 \n",
            "Classification loss: 1.4861853122711182 \n",
            "Regression loss: 0.04316288232803345\n",
            "----------------\n",
            "Loss:  0.8669097758829594 \n",
            "Classification loss: 1.6329736709594727 \n",
            "Regression loss: 0.05042294040322304\n",
            "----------------\n",
            "Loss:  0.8589537739753723 \n",
            "Classification loss: 1.5696773529052734 \n",
            "Regression loss: 0.0741150975227356\n",
            "----------------\n",
            "Loss:  0.7726642936468124 \n",
            "Classification loss: 1.4509565830230713 \n",
            "Regression loss: 0.047186002135276794\n",
            "----------------\n",
            "Loss:  0.823976494371891 \n",
            "Classification loss: 1.5474379062652588 \n",
            "Regression loss: 0.05025754123926163\n",
            "----------------\n",
            "Loss:  0.821287490427494 \n",
            "Classification loss: 1.5235234498977661 \n",
            "Regression loss: 0.05952576547861099\n",
            "----------------\n",
            "Loss:  0.8977733142673969 \n",
            "Classification loss: 1.701403021812439 \n",
            "Regression loss: 0.047071803361177444\n",
            "----------------\n",
            "Loss:  0.8636564314365387 \n",
            "Classification loss: 1.5772581100463867 \n",
            "Regression loss: 0.07502737641334534\n",
            "----------------\n",
            "Loss:  0.8660545647144318 \n",
            "Classification loss: 1.6310224533081055 \n",
            "Regression loss: 0.05054333806037903\n",
            "----------------\n",
            "Loss:  0.8268065415322781 \n",
            "Classification loss: 1.5754413604736328 \n",
            "Regression loss: 0.039085861295461655\n",
            "----------------\n",
            "Loss:  0.7836184799671173 \n",
            "Classification loss: 1.4606937170028687 \n",
            "Regression loss: 0.05327162146568298\n",
            "----------------\n",
            "Loss:  0.909689374268055 \n",
            "Classification loss: 1.6893295049667358 \n",
            "Regression loss: 0.06502462178468704\n",
            "----------------\n",
            "Loss:  0.8735550493001938 \n",
            "Classification loss: 1.6041256189346313 \n",
            "Regression loss: 0.07149223983287811\n",
            "----------------\n",
            "Loss:  0.756548285484314 \n",
            "Classification loss: 1.3827283382415771 \n",
            "Regression loss: 0.06518411636352539\n",
            "----------------\n",
            "Loss:  0.7997386902570724 \n",
            "Classification loss: 1.5149469375610352 \n",
            "Regression loss: 0.04226522147655487\n",
            "----------------\n",
            "Loss:  0.7717405073344707 \n",
            "Classification loss: 1.4615920782089233 \n",
            "Regression loss: 0.04094446823000908\n",
            "----------------\n",
            "Loss:  0.7913875542581081 \n",
            "Classification loss: 1.4986381530761719 \n",
            "Regression loss: 0.0420684777200222\n",
            "----------------\n",
            "Loss:  0.7773684784770012 \n",
            "Classification loss: 1.4296104907989502 \n",
            "Regression loss: 0.06256323307752609\n",
            "----------------\n",
            "Loss:  0.7614704221487045 \n",
            "Classification loss: 1.4040050506591797 \n",
            "Regression loss: 0.059467896819114685\n",
            "----------------\n",
            "Loss:  0.9284515306353569 \n",
            "Classification loss: 1.7674165964126587 \n",
            "Regression loss: 0.04474323242902756\n",
            "----------------\n",
            "Loss:  0.8948735557496548 \n",
            "Classification loss: 1.6765103340148926 \n",
            "Regression loss: 0.05661838874220848\n",
            "----------------\n",
            "Loss:  0.7995279058814049 \n",
            "Classification loss: 1.5312771797180176 \n",
            "Regression loss: 0.03388931602239609\n",
            "----------------\n",
            "Loss:  0.789934579282999 \n",
            "Classification loss: 1.4639770984649658 \n",
            "Regression loss: 0.05794603005051613\n",
            "----------------\n",
            "Loss:  0.8306428529322147 \n",
            "Classification loss: 1.5793235301971436 \n",
            "Regression loss: 0.04098108783364296\n",
            "----------------\n",
            "Loss:  0.748709823936224 \n",
            "Classification loss: 1.4438600540161133 \n",
            "Regression loss: 0.026779796928167343\n",
            "----------------\n",
            "Loss:  0.7918643467128277 \n",
            "Classification loss: 1.5144850015640259 \n",
            "Regression loss: 0.03462184593081474\n",
            "----------------\n",
            "Loss:  0.793588537722826 \n",
            "Classification loss: 1.5023953914642334 \n",
            "Regression loss: 0.042390841990709305\n",
            "----------------\n",
            "Loss:  0.6958327554166317 \n",
            "Classification loss: 1.2705470323562622 \n",
            "Regression loss: 0.060559239238500595\n",
            "----------------\n",
            "Loss:  0.8545860759913921 \n",
            "Classification loss: 1.621734619140625 \n",
            "Regression loss: 0.043718766421079636\n",
            "----------------\n",
            "Loss:  0.9623755663633347 \n",
            "Classification loss: 1.7952243089675903 \n",
            "Regression loss: 0.06476341187953949\n",
            "----------------\n",
            "Loss:  0.693975742906332 \n",
            "Classification loss: 1.3218066692352295 \n",
            "Regression loss: 0.03307240828871727\n",
            "----------------\n",
            "Loss:  0.8409995883703232 \n",
            "Classification loss: 1.5753246545791626 \n",
            "Regression loss: 0.05333726108074188\n",
            "----------------\n",
            "Loss:  0.7885692864656448 \n",
            "Classification loss: 1.5013935565948486 \n",
            "Regression loss: 0.03787250816822052\n",
            "----------------\n",
            "Loss:  0.89194555580616 \n",
            "Classification loss: 1.6020660400390625 \n",
            "Regression loss: 0.09091253578662872\n",
            "----------------\n",
            "Loss:  0.8456170707941055 \n",
            "Classification loss: 1.5556831359863281 \n",
            "Regression loss: 0.06777550280094147\n",
            "----------------\n",
            "Loss:  0.8900457471609116 \n",
            "Classification loss: 1.6470948457717896 \n",
            "Regression loss: 0.06649832427501678\n",
            "----------------\n",
            "Loss:  0.788827620446682 \n",
            "Classification loss: 1.4753401279449463 \n",
            "Regression loss: 0.05115755647420883\n",
            "----------------\n",
            "Loss:  0.7362229973077774 \n",
            "Classification loss: 1.3617396354675293 \n",
            "Regression loss: 0.055353179574012756\n",
            "----------------\n",
            "Loss:  0.8035523295402527 \n",
            "Classification loss: 1.5154623985290527 \n",
            "Regression loss: 0.04582113027572632\n",
            "----------------\n",
            "Loss:  0.9129875600337982 \n",
            "Classification loss: 1.7148656845092773 \n",
            "Regression loss: 0.055554717779159546\n",
            "----------------\n",
            "Loss:  0.7815594002604485 \n",
            "Classification loss: 1.467138648033142 \n",
            "Regression loss: 0.04799007624387741\n",
            "----------------\n",
            "Loss:  0.9479676708579063 \n",
            "Classification loss: 1.7612415552139282 \n",
            "Regression loss: 0.06734689325094223\n",
            "----------------\n",
            "Loss:  0.8001417592167854 \n",
            "Classification loss: 1.4929255247116089 \n",
            "Regression loss: 0.05367899686098099\n",
            "----------------\n",
            "Loss:  0.9183709025382996 \n",
            "Classification loss: 1.7109732627868652 \n",
            "Regression loss: 0.06288427114486694\n",
            "----------------\n",
            "Loss:  0.8685230426490307 \n",
            "Classification loss: 1.6350347995758057 \n",
            "Regression loss: 0.05100564286112785\n",
            "----------------\n",
            "Loss:  0.8057634308934212 \n",
            "Classification loss: 1.5164446830749512 \n",
            "Regression loss: 0.04754108935594559\n",
            "----------------\n",
            "Loss:  0.842204712331295 \n",
            "Classification loss: 1.5250884294509888 \n",
            "Regression loss: 0.07966049760580063\n",
            "----------------\n",
            "Loss:  0.893054224550724 \n",
            "Classification loss: 1.6859108209609985 \n",
            "Regression loss: 0.05009881407022476\n",
            "----------------\n",
            "Loss:  0.8435563519597054 \n",
            "Classification loss: 1.58012056350708 \n",
            "Regression loss: 0.053496070206165314\n",
            "----------------\n",
            "Loss:  0.8928291238844395 \n",
            "Classification loss: 1.6952500343322754 \n",
            "Regression loss: 0.04520410671830177\n",
            "----------------\n",
            "Loss:  0.8064876645803452 \n",
            "Classification loss: 1.5346258878707886 \n",
            "Regression loss: 0.03917472064495087\n",
            "----------------\n",
            "Loss:  0.8951567262411118 \n",
            "Classification loss: 1.6575195789337158 \n",
            "Regression loss: 0.06639693677425385\n",
            "----------------\n",
            "Loss:  0.692236952483654 \n",
            "Classification loss: 1.2711002826690674 \n",
            "Regression loss: 0.05668681114912033\n",
            "----------------\n",
            "Loss:  0.8253279663622379 \n",
            "Classification loss: 1.5392488241195679 \n",
            "Regression loss: 0.055703554302453995\n",
            "----------------\n",
            "Loss:  0.7872884087264538 \n",
            "Classification loss: 1.474430799484253 \n",
            "Regression loss: 0.050073008984327316\n",
            "----------------\n",
            "Loss:  0.9078905880451202 \n",
            "Classification loss: 1.6740355491638184 \n",
            "Regression loss: 0.07087281346321106\n",
            "----------------\n",
            "Loss:  0.7223201841115952 \n",
            "Classification loss: 1.345733642578125 \n",
            "Regression loss: 0.049453362822532654\n",
            "----------------\n",
            "Loss:  0.7628343626856804 \n",
            "Classification loss: 1.4210541248321533 \n",
            "Regression loss: 0.05230730026960373\n",
            "----------------\n",
            "Loss:  0.7987948022782803 \n",
            "Classification loss: 1.528923511505127 \n",
            "Regression loss: 0.03433304652571678\n",
            "----------------\n",
            "Loss:  0.7370730265974998 \n",
            "Classification loss: 1.4093964099884033 \n",
            "Regression loss: 0.03237482160329819\n",
            "----------------\n",
            "Loss:  0.7510116919875145 \n",
            "Classification loss: 1.4079420566558838 \n",
            "Regression loss: 0.0470406636595726\n",
            "----------------\n",
            "Loss:  0.9345496371388435 \n",
            "Classification loss: 1.7499631643295288 \n",
            "Regression loss: 0.05956805497407913\n",
            "----------------\n",
            "Loss:  0.8915869332849979 \n",
            "Classification loss: 1.6789820194244385 \n",
            "Regression loss: 0.0520959235727787\n",
            "----------------\n",
            "Loss:  0.8511287719011307 \n",
            "Classification loss: 1.625706672668457 \n",
            "Regression loss: 0.03827543556690216\n",
            "----------------\n",
            "Loss:  0.8166995532810688 \n",
            "Classification loss: 1.5259827375411987 \n",
            "Regression loss: 0.05370818451046944\n",
            "----------------\n",
            "Loss:  0.83200304210186 \n",
            "Classification loss: 1.5193370580673218 \n",
            "Regression loss: 0.07233451306819916\n",
            "----------------\n",
            "Loss:  0.7949703224003315 \n",
            "Classification loss: 1.5081279277801514 \n",
            "Regression loss: 0.040906358510255814\n",
            "----------------\n",
            "Loss:  0.8203449547290802 \n",
            "Classification loss: 1.556685209274292 \n",
            "Regression loss: 0.042002350091934204\n",
            "----------------\n",
            "Loss:  0.7641860842704773 \n",
            "Classification loss: 1.4446117877960205 \n",
            "Regression loss: 0.04188019037246704\n",
            "----------------\n",
            "Loss:  0.8067332282662392 \n",
            "Classification loss: 1.5172628164291382 \n",
            "Regression loss: 0.048101820051670074\n",
            "----------------\n",
            "Loss:  0.6754445992410183 \n",
            "Classification loss: 1.238615870475769 \n",
            "Regression loss: 0.056136664003133774\n",
            "----------------\n",
            "Loss:  0.8833247609436512 \n",
            "Classification loss: 1.7061421871185303 \n",
            "Regression loss: 0.030253667384386063\n",
            "----------------\n",
            "Loss:  0.8593207970261574 \n",
            "Classification loss: 1.6075975894927979 \n",
            "Regression loss: 0.05552200227975845\n",
            "----------------\n",
            "Loss:  0.7897249311208725 \n",
            "Classification loss: 1.4560524225234985 \n",
            "Regression loss: 0.06169871985912323\n",
            "----------------\n",
            "Loss:  0.8663166910409927 \n",
            "Classification loss: 1.628908634185791 \n",
            "Regression loss: 0.05186237394809723\n",
            "----------------\n",
            "Loss:  0.8732412233948708 \n",
            "Classification loss: 1.6721125841140747 \n",
            "Regression loss: 0.037184931337833405\n",
            "----------------\n",
            "Loss:  0.902021087706089 \n",
            "Classification loss: 1.6419901847839355 \n",
            "Regression loss: 0.08102599531412125\n",
            "----------------\n",
            "Loss:  0.8176730200648308 \n",
            "Classification loss: 1.4906789064407349 \n",
            "Regression loss: 0.07233356684446335\n",
            "----------------\n",
            "Loss:  0.8132685385644436 \n",
            "Classification loss: 1.5409598350524902 \n",
            "Regression loss: 0.04278862103819847\n",
            "----------------\n",
            "Loss:  0.8167005702853203 \n",
            "Classification loss: 1.540804147720337 \n",
            "Regression loss: 0.046298496425151825\n",
            "----------------\n",
            "Loss:  0.8128817155957222 \n",
            "Classification loss: 1.5556683540344238 \n",
            "Regression loss: 0.035047538578510284\n",
            "----------------\n",
            "Loss:  0.6635897755622864 \n",
            "Classification loss: 1.2497020959854126 \n",
            "Regression loss: 0.03873872756958008\n",
            "----------------\n",
            "Loss:  0.8508132472634315 \n",
            "Classification loss: 1.5883177518844604 \n",
            "Regression loss: 0.056654371321201324\n",
            "----------------\n",
            "Loss:  0.7844986990094185 \n",
            "Classification loss: 1.4365613460540771 \n",
            "Regression loss: 0.06621802598237991\n",
            "----------------\n",
            "Loss:  0.7947347685694695 \n",
            "Classification loss: 1.4606032371520996 \n",
            "Regression loss: 0.06443314999341965\n",
            "----------------\n",
            "Loss:  0.7650399561971426 \n",
            "Classification loss: 1.476443886756897 \n",
            "Regression loss: 0.026818012818694115\n",
            "----------------\n",
            "Loss:  0.8962679132819176 \n",
            "Classification loss: 1.6992119550704956 \n",
            "Regression loss: 0.04666193574666977\n",
            "----------------\n",
            "Loss:  0.8795036226511002 \n",
            "Classification loss: 1.6784579753875732 \n",
            "Regression loss: 0.04027463495731354\n",
            "----------------\n",
            "Loss:  0.929825559258461 \n",
            "Classification loss: 1.7155888080596924 \n",
            "Regression loss: 0.07203115522861481\n",
            "----------------\n",
            "Loss:  0.7863280214369297 \n",
            "Classification loss: 1.4779390096664429 \n",
            "Regression loss: 0.04735851660370827\n",
            "----------------\n",
            "Loss:  0.8985705003142357 \n",
            "Classification loss: 1.653745174407959 \n",
            "Regression loss: 0.0716979131102562\n",
            "----------------\n",
            "Loss:  0.7996263951063156 \n",
            "Classification loss: 1.5089247226715088 \n",
            "Regression loss: 0.04516403377056122\n",
            "----------------\n",
            "Loss:  0.8252400457859039 \n",
            "Classification loss: 1.5225948095321655 \n",
            "Regression loss: 0.06394264101982117\n",
            "----------------\n",
            "Loss:  0.8570242524147034 \n",
            "Classification loss: 1.6375330686569214 \n",
            "Regression loss: 0.038257718086242676\n",
            "----------------\n",
            "Loss:  0.945218026638031 \n",
            "Classification loss: 1.7311124801635742 \n",
            "Regression loss: 0.0796617865562439\n",
            "----------------\n",
            "Loss:  0.8092519193887711 \n",
            "Classification loss: 1.5377111434936523 \n",
            "Regression loss: 0.040396347641944885\n",
            "----------------\n",
            "Loss:  0.8314488381147385 \n",
            "Classification loss: 1.57157564163208 \n",
            "Regression loss: 0.045661017298698425\n",
            "----------------\n",
            "Loss:  0.8333385735750198 \n",
            "Classification loss: 1.5251308679580688 \n",
            "Regression loss: 0.07077313959598541\n",
            "----------------\n",
            "Loss:  0.75023003667593 \n",
            "Classification loss: 1.4212234020233154 \n",
            "Regression loss: 0.03961833566427231\n",
            "----------------\n",
            "Loss:  0.80697051435709 \n",
            "Classification loss: 1.4880430698394775 \n",
            "Regression loss: 0.06294897943735123\n",
            "----------------\n",
            "Loss:  0.8182720802724361 \n",
            "Classification loss: 1.5430505275726318 \n",
            "Regression loss: 0.046746816486120224\n",
            "----------------\n",
            "Loss:  0.8520297072827816 \n",
            "Classification loss: 1.6193794012069702 \n",
            "Regression loss: 0.042340006679296494\n",
            "----------------\n",
            "Loss:  0.8372389003634453 \n",
            "Classification loss: 1.5956918001174927 \n",
            "Regression loss: 0.039393000304698944\n",
            "----------------\n",
            "Loss:  0.9571156725287437 \n",
            "Classification loss: 1.818727731704712 \n",
            "Regression loss: 0.04775180667638779\n",
            "----------------\n",
            "Loss:  0.8445881959050894 \n",
            "Classification loss: 1.6386606693267822 \n",
            "Regression loss: 0.025257861241698265\n",
            "----------------\n",
            "Loss:  0.8654916882514954 \n",
            "Classification loss: 1.6459451913833618 \n",
            "Regression loss: 0.04251909255981445\n",
            "----------------\n",
            "Loss:  0.7878857925534248 \n",
            "Classification loss: 1.4401777982711792 \n",
            "Regression loss: 0.06779689341783524\n",
            "----------------\n",
            "Loss:  0.8317825943231583 \n",
            "Classification loss: 1.5360870361328125 \n",
            "Regression loss: 0.06373907625675201\n",
            "----------------\n",
            "Loss:  0.9379445984959602 \n",
            "Classification loss: 1.7149338722229004 \n",
            "Regression loss: 0.08047766238451004\n",
            "----------------\n",
            "Loss:  0.8648923970758915 \n",
            "Classification loss: 1.6106865406036377 \n",
            "Regression loss: 0.05954912677407265\n",
            "----------------\n",
            "Loss:  0.7282650135457516 \n",
            "Classification loss: 1.4013166427612305 \n",
            "Regression loss: 0.027606692165136337\n",
            "----------------\n",
            "Loss:  0.8044631853699684 \n",
            "Classification loss: 1.4718353748321533 \n",
            "Regression loss: 0.06854549795389175\n",
            "----------------\n",
            "Loss:  0.8185837045311928 \n",
            "Classification loss: 1.5383460521697998 \n",
            "Regression loss: 0.04941067844629288\n",
            "----------------\n",
            "Loss:  0.8287688344717026 \n",
            "Classification loss: 1.566829800605774 \n",
            "Regression loss: 0.04535393416881561\n",
            "----------------\n",
            "Loss:  0.8770342171192169 \n",
            "Classification loss: 1.60051429271698 \n",
            "Regression loss: 0.07677707076072693\n",
            "----------------\n",
            "Loss:  0.8634335026144981 \n",
            "Classification loss: 1.623785138130188 \n",
            "Regression loss: 0.051540933549404144\n",
            "----------------\n",
            "Loss:  0.7162980400025845 \n",
            "Classification loss: 1.3718469142913818 \n",
            "Regression loss: 0.03037458285689354\n",
            "----------------\n",
            "Loss:  0.8527881950139999 \n",
            "Classification loss: 1.6212221384048462 \n",
            "Regression loss: 0.04217712581157684\n",
            "----------------\n",
            "Loss:  0.7425787709653378 \n",
            "Classification loss: 1.408874273300171 \n",
            "Regression loss: 0.038141634315252304\n",
            "----------------\n",
            "Loss:  0.8944912105798721 \n",
            "Classification loss: 1.6808993816375732 \n",
            "Regression loss: 0.05404151976108551\n",
            "----------------\n",
            "Loss:  0.9069700464606285 \n",
            "Classification loss: 1.6854019165039062 \n",
            "Regression loss: 0.06426908820867538\n",
            "----------------\n",
            "Loss:  0.8292149342596531 \n",
            "Classification loss: 1.5505328178405762 \n",
            "Regression loss: 0.053948525339365005\n",
            "----------------\n",
            "Loss:  0.8918604366481304 \n",
            "Classification loss: 1.677828073501587 \n",
            "Regression loss: 0.05294639989733696\n",
            "----------------\n",
            "Loss:  0.8463563919067383 \n",
            "Classification loss: 1.5522935390472412 \n",
            "Regression loss: 0.07020962238311768\n",
            "----------------\n",
            "Loss:  0.8060964476317167 \n",
            "Classification loss: 1.5605859756469727 \n",
            "Regression loss: 0.0258034598082304\n",
            "----------------\n",
            "Loss:  0.7345728985965252 \n",
            "Classification loss: 1.4221680164337158 \n",
            "Regression loss: 0.023488890379667282\n",
            "----------------\n",
            "Loss:  0.8895236030220985 \n",
            "Classification loss: 1.6834253072738647 \n",
            "Regression loss: 0.04781094938516617\n",
            "----------------\n",
            "Loss:  0.7099207658320665 \n",
            "Classification loss: 1.3628342151641846 \n",
            "Regression loss: 0.02850365824997425\n",
            "----------------\n",
            "Loss:  0.9480262920260429 \n",
            "Classification loss: 1.758095145225525 \n",
            "Regression loss: 0.06897871941328049\n",
            "----------------\n",
            "Loss:  0.6893790848553181 \n",
            "Classification loss: 1.3112167119979858 \n",
            "Regression loss: 0.03377072885632515\n",
            "----------------\n",
            "Loss:  0.7871371507644653 \n",
            "Classification loss: 1.4784117937088013 \n",
            "Regression loss: 0.0479312539100647\n",
            "----------------\n",
            "Loss:  0.8045040518045425 \n",
            "Classification loss: 1.4614616632461548 \n",
            "Regression loss: 0.07377322018146515\n",
            "----------------\n",
            "Loss:  0.816289696842432 \n",
            "Classification loss: 1.5265157222747803 \n",
            "Regression loss: 0.053031835705041885\n",
            "----------------\n",
            "Loss:  0.7980629801750183 \n",
            "Classification loss: 1.4556994438171387 \n",
            "Regression loss: 0.07021325826644897\n",
            "----------------\n",
            "Loss:  0.8439768515527248 \n",
            "Classification loss: 1.6123868227005005 \n",
            "Regression loss: 0.037783440202474594\n",
            "----------------\n",
            "Loss:  0.8362315781414509 \n",
            "Classification loss: 1.5616555213928223 \n",
            "Regression loss: 0.05540381744503975\n",
            "----------------\n",
            "Loss:  0.8411636464297771 \n",
            "Classification loss: 1.581532597541809 \n",
            "Regression loss: 0.050397347658872604\n",
            "----------------\n",
            "Loss:  0.8826071247458458 \n",
            "Classification loss: 1.6240118741989136 \n",
            "Regression loss: 0.07060118764638901\n",
            "----------------\n",
            "Loss:  0.881319172680378 \n",
            "Classification loss: 1.658268928527832 \n",
            "Regression loss: 0.052184708416461945\n",
            "----------------\n",
            "Loss:  0.6795971170067787 \n",
            "Classification loss: 1.271659255027771 \n",
            "Regression loss: 0.04376748949289322\n",
            "----------------\n",
            "Loss:  0.7439349088817835 \n",
            "Classification loss: 1.4572187662124634 \n",
            "Regression loss: 0.015325525775551796\n",
            "----------------\n",
            "Loss:  0.7799209281802177 \n",
            "Classification loss: 1.4937825202941895 \n",
            "Regression loss: 0.033029668033123016\n",
            "----------------\n",
            "Loss:  0.8439089469611645 \n",
            "Classification loss: 1.5787357091903687 \n",
            "Regression loss: 0.05454109236598015\n",
            "----------------\n",
            "Loss:  0.8060308061540127 \n",
            "Classification loss: 1.5034624338150024 \n",
            "Regression loss: 0.05429958924651146\n",
            "----------------\n",
            "Loss:  0.8099154159426689 \n",
            "Classification loss: 1.4878480434417725 \n",
            "Regression loss: 0.06599139422178268\n",
            "----------------\n",
            "Loss:  0.8535373136401176 \n",
            "Classification loss: 1.6433453559875488 \n",
            "Regression loss: 0.03186463564634323\n",
            "----------------\n",
            "Loss:  0.8728015571832657 \n",
            "Classification loss: 1.6269793510437012 \n",
            "Regression loss: 0.0593118816614151\n",
            "----------------\n",
            "Loss:  0.8447694852948189 \n",
            "Classification loss: 1.5421133041381836 \n",
            "Regression loss: 0.07371283322572708\n",
            "----------------\n",
            "Loss:  0.7911198288202286 \n",
            "Classification loss: 1.507398009300232 \n",
            "Regression loss: 0.03742082417011261\n",
            "----------------\n",
            "Loss:  0.8063179478049278 \n",
            "Classification loss: 1.4660265445709229 \n",
            "Regression loss: 0.0733046755194664\n",
            "----------------\n",
            "Loss:  0.8897687345743179 \n",
            "Classification loss: 1.6745171546936035 \n",
            "Regression loss: 0.052510157227516174\n",
            "----------------\n",
            "Loss:  0.7979830950498581 \n",
            "Classification loss: 1.467442274093628 \n",
            "Regression loss: 0.06426195800304413\n",
            "----------------\n",
            "Loss:  0.7680575475096703 \n",
            "Classification loss: 1.4631797075271606 \n",
            "Regression loss: 0.036467693746089935\n",
            "----------------\n",
            "Loss:  0.7658771052956581 \n",
            "Classification loss: 1.3832464218139648 \n",
            "Regression loss: 0.07425389438867569\n",
            "----------------\n",
            "Loss:  0.8119445741176605 \n",
            "Classification loss: 1.5180490016937256 \n",
            "Regression loss: 0.05292007327079773\n",
            "----------------\n",
            "Loss:  0.855121910572052 \n",
            "Classification loss: 1.5279945135116577 \n",
            "Regression loss: 0.09112465381622314\n",
            "----------------\n",
            "Loss:  0.8762437850236893 \n",
            "Classification loss: 1.642030119895935 \n",
            "Regression loss: 0.05522872507572174\n",
            "----------------\n",
            "Loss:  0.8540631085634232 \n",
            "Classification loss: 1.5640082359313965 \n",
            "Regression loss: 0.07205899059772491\n",
            "----------------\n",
            "Loss:  0.7591767907142639 \n",
            "Classification loss: 1.418846607208252 \n",
            "Regression loss: 0.04975348711013794\n",
            "----------------\n",
            "Loss:  0.8422667905688286 \n",
            "Classification loss: 1.5243762731552124 \n",
            "Regression loss: 0.08007865399122238\n",
            "----------------\n",
            "Loss:  0.7208860069513321 \n",
            "Classification loss: 1.3187276124954224 \n",
            "Regression loss: 0.06152220070362091\n",
            "----------------\n",
            "Loss:  0.7371468469500542 \n",
            "Classification loss: 1.3369905948638916 \n",
            "Regression loss: 0.06865154951810837\n",
            "----------------\n",
            "Loss:  0.7873241864144802 \n",
            "Classification loss: 1.512141227722168 \n",
            "Regression loss: 0.031253572553396225\n",
            "----------------\n",
            "Loss:  0.9028395414352417 \n",
            "Classification loss: 1.6794486045837402 \n",
            "Regression loss: 0.06311523914337158\n",
            "----------------\n",
            "Loss:  0.7868275120854378 \n",
            "Classification loss: 1.471642017364502 \n",
            "Regression loss: 0.0510065034031868\n",
            "----------------\n",
            "Loss:  0.7598019614815712 \n",
            "Classification loss: 1.4496922492980957 \n",
            "Regression loss: 0.034955836832523346\n",
            "----------------\n",
            "Loss:  0.8196137696504593 \n",
            "Classification loss: 1.522706151008606 \n",
            "Regression loss: 0.05826069414615631\n",
            "----------------\n",
            "Loss:  0.880903847515583 \n",
            "Classification loss: 1.6287152767181396 \n",
            "Regression loss: 0.06654620915651321\n",
            "----------------\n",
            "Loss:  0.8246747367084026 \n",
            "Classification loss: 1.5313458442687988 \n",
            "Regression loss: 0.05900181457400322\n",
            "----------------\n",
            "Loss:  0.827054925262928 \n",
            "Classification loss: 1.4978264570236206 \n",
            "Regression loss: 0.0781416967511177\n",
            "----------------\n",
            "Loss:  0.7780363410711288 \n",
            "Classification loss: 1.4473211765289307 \n",
            "Regression loss: 0.05437575280666351\n",
            "----------------\n",
            "Loss:  0.708151139318943 \n",
            "Classification loss: 1.3310039043426514 \n",
            "Regression loss: 0.04264918714761734\n",
            "----------------\n",
            "Loss:  0.7202402874827385 \n",
            "Classification loss: 1.351920485496521 \n",
            "Regression loss: 0.044280044734478\n",
            "----------------\n",
            "Loss:  0.8002466410398483 \n",
            "Classification loss: 1.5005548000335693 \n",
            "Regression loss: 0.04996924102306366\n",
            "----------------\n",
            "Loss:  0.8614264149218798 \n",
            "Classification loss: 1.6650021076202393 \n",
            "Regression loss: 0.02892536111176014\n",
            "----------------\n",
            "Loss:  0.8744003176689148 \n",
            "Classification loss: 1.6213499307632446 \n",
            "Regression loss: 0.06372535228729248\n",
            "----------------\n",
            "Loss:  0.6636173725128174 \n",
            "Classification loss: 1.2162742614746094 \n",
            "Regression loss: 0.055480241775512695\n",
            "----------------\n",
            "Loss:  0.705566518008709 \n",
            "Classification loss: 1.3256447315216064 \n",
            "Regression loss: 0.04274415224790573\n",
            "----------------\n",
            "Loss:  0.756123848259449 \n",
            "Classification loss: 1.4395028352737427 \n",
            "Regression loss: 0.03637243062257767\n",
            "----------------\n",
            "Loss:  0.7533250264823437 \n",
            "Classification loss: 1.3859633207321167 \n",
            "Regression loss: 0.060343366116285324\n",
            "----------------\n",
            "Loss:  0.8844634741544724 \n",
            "Classification loss: 1.64342200756073 \n",
            "Regression loss: 0.06275247037410736\n",
            "----------------\n",
            "Loss:  0.7108936496078968 \n",
            "Classification loss: 1.3315353393554688 \n",
            "Regression loss: 0.04512597993016243\n",
            "----------------\n",
            "Loss:  0.8479314483702183 \n",
            "Classification loss: 1.6180548667907715 \n",
            "Regression loss: 0.038904014974832535\n",
            "----------------\n",
            "Loss:  0.8440711069852114 \n",
            "Classification loss: 1.6287295818328857 \n",
            "Regression loss: 0.0297063160687685\n",
            "----------------\n",
            "Loss:  0.856573686003685 \n",
            "Classification loss: 1.621891975402832 \n",
            "Regression loss: 0.04562769830226898\n",
            "----------------\n",
            "Loss:  0.8022502511739731 \n",
            "Classification loss: 1.4359723329544067 \n",
            "Regression loss: 0.08426408469676971\n",
            "----------------\n",
            "Loss:  0.8157772421836853 \n",
            "Classification loss: 1.5599048137664795 \n",
            "Regression loss: 0.03582483530044556\n",
            "----------------\n",
            "Loss:  0.9073985740542412 \n",
            "Classification loss: 1.7255827188491821 \n",
            "Regression loss: 0.044607214629650116\n",
            "----------------\n",
            "Loss:  0.7089336775243282 \n",
            "Classification loss: 1.3428964614868164 \n",
            "Regression loss: 0.03748544678092003\n",
            "----------------\n",
            "Loss:  0.7138177640736103 \n",
            "Classification loss: 1.359708309173584 \n",
            "Regression loss: 0.033963609486818314\n",
            "----------------\n",
            "Loss:  0.8623447231948376 \n",
            "Classification loss: 1.6405904293060303 \n",
            "Regression loss: 0.042049508541822433\n",
            "----------------\n",
            "Loss:  0.7630820274353027 \n",
            "Classification loss: 1.4408502578735352 \n",
            "Regression loss: 0.042656898498535156\n",
            "----------------\n",
            "Loss:  0.7125896960496902 \n",
            "Classification loss: 1.316540002822876 \n",
            "Regression loss: 0.05431969463825226\n",
            "----------------\n",
            "Loss:  0.726709321141243 \n",
            "Classification loss: 1.3219025135040283 \n",
            "Regression loss: 0.06575806438922882\n",
            "----------------\n",
            "Loss:  0.7809733673930168 \n",
            "Classification loss: 1.4123682975769043 \n",
            "Regression loss: 0.07478921860456467\n",
            "----------------\n",
            "Loss:  0.8056805729866028 \n",
            "Classification loss: 1.4629783630371094 \n",
            "Regression loss: 0.0741913914680481\n",
            "----------------\n",
            "Loss:  0.7812836617231369 \n",
            "Classification loss: 1.493437647819519 \n",
            "Regression loss: 0.03456483781337738\n",
            "----------------\n",
            "Loss:  0.8908227756619453 \n",
            "Classification loss: 1.682584524154663 \n",
            "Regression loss: 0.0495305135846138\n",
            "----------------\n",
            "Loss:  0.7896930389106274 \n",
            "Classification loss: 1.484589695930481 \n",
            "Regression loss: 0.04739819094538689\n",
            "----------------\n",
            "Loss:  0.8589680269360542 \n",
            "Classification loss: 1.5930852890014648 \n",
            "Regression loss: 0.06242538243532181\n",
            "----------------\n",
            "Loss:  0.8812267743051052 \n",
            "Classification loss: 1.6520662307739258 \n",
            "Regression loss: 0.05519365891814232\n",
            "----------------\n",
            "Loss:  0.753497950732708 \n",
            "Classification loss: 1.3699357509613037 \n",
            "Regression loss: 0.06853007525205612\n",
            "----------------\n",
            "Loss:  0.7615597285330296 \n",
            "Classification loss: 1.4528661966323853 \n",
            "Regression loss: 0.03512663021683693\n",
            "----------------\n",
            "Loss:  0.6996882855892181 \n",
            "Classification loss: 1.3171184062957764 \n",
            "Regression loss: 0.041129082441329956\n",
            "----------------\n",
            "Loss:  0.7787888050079346 \n",
            "Classification loss: 1.430426836013794 \n",
            "Regression loss: 0.0635753870010376\n",
            "----------------\n",
            "Loss:  0.7912890575826168 \n",
            "Classification loss: 1.5047693252563477 \n",
            "Regression loss: 0.03890439495444298\n",
            "----------------\n",
            "Loss:  0.857231393456459 \n",
            "Classification loss: 1.636244773864746 \n",
            "Regression loss: 0.039109006524086\n",
            "----------------\n",
            "Loss:  0.7749007269740105 \n",
            "Classification loss: 1.442521095275879 \n",
            "Regression loss: 0.053640179336071014\n",
            "----------------\n",
            "Loss:  0.8131736218929291 \n",
            "Classification loss: 1.4849903583526611 \n",
            "Regression loss: 0.07067844271659851\n",
            "----------------\n",
            "Loss:  0.826326534152031 \n",
            "Classification loss: 1.5315240621566772 \n",
            "Regression loss: 0.06056450307369232\n",
            "----------------\n",
            "Loss:  0.8439708240330219 \n",
            "Classification loss: 1.6032383441925049 \n",
            "Regression loss: 0.042351651936769485\n",
            "----------------\n",
            "Loss:  0.8441950269043446 \n",
            "Classification loss: 1.5832395553588867 \n",
            "Regression loss: 0.0525752492249012\n",
            "----------------\n",
            "Loss:  0.8136071860790253 \n",
            "Classification loss: 1.4615668058395386 \n",
            "Regression loss: 0.08282378315925598\n",
            "----------------\n",
            "Loss:  0.7575123347342014 \n",
            "Classification loss: 1.426092505455017 \n",
            "Regression loss: 0.044466082006692886\n",
            "----------------\n",
            "Loss:  0.7954851165413857 \n",
            "Classification loss: 1.510066270828247 \n",
            "Regression loss: 0.040451981127262115\n",
            "----------------\n",
            "Loss:  0.7679342329502106 \n",
            "Classification loss: 1.437617301940918 \n",
            "Regression loss: 0.04912558197975159\n",
            "----------------\n",
            "Loss:  0.7435839511454105 \n",
            "Classification loss: 1.3953170776367188 \n",
            "Regression loss: 0.04592541232705116\n",
            "----------------\n",
            "Loss:  0.7691998817026615 \n",
            "Classification loss: 1.4190115928649902 \n",
            "Regression loss: 0.0596940852701664\n",
            "----------------\n",
            "Loss:  0.858705073595047 \n",
            "Classification loss: 1.593230128288269 \n",
            "Regression loss: 0.062090009450912476\n",
            "----------------\n",
            "Loss:  0.8304193615913391 \n",
            "Classification loss: 1.5349557399749756 \n",
            "Regression loss: 0.06294149160385132\n",
            "----------------\n",
            "Loss:  0.8565336614847183 \n",
            "Classification loss: 1.6105566024780273 \n",
            "Regression loss: 0.05125536024570465\n",
            "----------------\n",
            "Loss:  0.9253120981156826 \n",
            "Classification loss: 1.7586753368377686 \n",
            "Regression loss: 0.045974429696798325\n",
            "----------------\n",
            "Loss:  0.8271234445273876 \n",
            "Classification loss: 1.6047720909118652 \n",
            "Regression loss: 0.024737399071455002\n",
            "----------------\n",
            "Loss:  0.7957196310162544 \n",
            "Classification loss: 1.4801952838897705 \n",
            "Regression loss: 0.05562198907136917\n",
            "----------------\n",
            "Loss:  0.7762530222535133 \n",
            "Classification loss: 1.4291938543319702 \n",
            "Regression loss: 0.06165609508752823\n",
            "----------------\n",
            "Loss:  0.9338955208659172 \n",
            "Classification loss: 1.7159254550933838 \n",
            "Regression loss: 0.07593279331922531\n",
            "----------------\n",
            "Loss:  0.7168788313865662 \n",
            "Classification loss: 1.3395766019821167 \n",
            "Regression loss: 0.04709053039550781\n",
            "----------------\n",
            "Loss:  0.8178225755691528 \n",
            "Classification loss: 1.5449129343032837 \n",
            "Regression loss: 0.045366108417510986\n",
            "----------------\n",
            "Loss:  0.8714552447199821 \n",
            "Classification loss: 1.609632134437561 \n",
            "Regression loss: 0.06663917750120163\n",
            "----------------\n",
            "Loss:  0.7034835293889046 \n",
            "Classification loss: 1.2862935066223145 \n",
            "Regression loss: 0.060336776077747345\n",
            "----------------\n",
            "Loss:  0.8414278402924538 \n",
            "Classification loss: 1.611901044845581 \n",
            "Regression loss: 0.03547731786966324\n",
            "----------------\n",
            "Loss:  0.7501971609890461 \n",
            "Classification loss: 1.4355583190917969 \n",
            "Regression loss: 0.03241800144314766\n",
            "----------------\n",
            "Loss:  0.7907477989792824 \n",
            "Classification loss: 1.4623912572860718 \n",
            "Regression loss: 0.05955217033624649\n",
            "----------------\n",
            "Loss:  0.8183620050549507 \n",
            "Classification loss: 1.5488731861114502 \n",
            "Regression loss: 0.043925411999225616\n",
            "----------------\n",
            "Loss:  0.7991134822368622 \n",
            "Classification loss: 1.4503540992736816 \n",
            "Regression loss: 0.07393643260002136\n",
            "----------------\n",
            "Loss:  0.8606181964278221 \n",
            "Classification loss: 1.633833885192871 \n",
            "Regression loss: 0.043701253831386566\n",
            "----------------\n",
            "Loss:  0.8026730790734291 \n",
            "Classification loss: 1.491733431816101 \n",
            "Regression loss: 0.05680636316537857\n",
            "----------------\n",
            "Loss:  0.836299316957593 \n",
            "Classification loss: 1.626171588897705 \n",
            "Regression loss: 0.023213522508740425\n",
            "----------------\n",
            "Loss:  0.8054771646857262 \n",
            "Classification loss: 1.535172939300537 \n",
            "Regression loss: 0.03789069503545761\n",
            "----------------\n",
            "Loss:  0.7067513018846512 \n",
            "Classification loss: 1.3462929725646973 \n",
            "Regression loss: 0.03360481560230255\n",
            "----------------\n",
            "Loss:  0.8040051832795143 \n",
            "Classification loss: 1.546342134475708 \n",
            "Regression loss: 0.03083411604166031\n",
            "----------------\n",
            "Loss:  0.815118670463562 \n",
            "Classification loss: 1.5303303003311157 \n",
            "Regression loss: 0.04995352029800415\n",
            "----------------\n",
            "Loss:  0.7415708005428314 \n",
            "Classification loss: 1.3737318515777588 \n",
            "Regression loss: 0.054704874753952026\n",
            "----------------\n",
            "Loss:  0.8458561487495899 \n",
            "Classification loss: 1.619797706604004 \n",
            "Regression loss: 0.03595729544758797\n",
            "----------------\n",
            "Loss:  0.9680836945772171 \n",
            "Classification loss: 1.7922956943511963 \n",
            "Regression loss: 0.07193584740161896\n",
            "----------------\n",
            "Loss:  0.7980924565345049 \n",
            "Classification loss: 1.5354061126708984 \n",
            "Regression loss: 0.03038940019905567\n",
            "----------------\n",
            "Loss:  0.8348933309316635 \n",
            "Classification loss: 1.5266547203063965 \n",
            "Regression loss: 0.07156597077846527\n",
            "----------------\n",
            "Loss:  0.6896291747689247 \n",
            "Classification loss: 1.2703680992126465 \n",
            "Regression loss: 0.05444512516260147\n",
            "----------------\n",
            "Loss:  0.8621062412858009 \n",
            "Classification loss: 1.6045289039611816 \n",
            "Regression loss: 0.059841789305210114\n",
            "----------------\n",
            "Loss:  0.8458735384047031 \n",
            "Classification loss: 1.5745981931686401 \n",
            "Regression loss: 0.05857444182038307\n",
            "----------------\n",
            "Loss:  0.7953633815050125 \n",
            "Classification loss: 1.4961802959442139 \n",
            "Regression loss: 0.04727323353290558\n",
            "----------------\n",
            "Loss:  0.7007773742079735 \n",
            "Classification loss: 1.3306175470352173 \n",
            "Regression loss: 0.03546860069036484\n",
            "----------------\n",
            "Loss:  0.6883034836500883 \n",
            "Classification loss: 1.3174991607666016 \n",
            "Regression loss: 0.02955390326678753\n",
            "----------------\n",
            "Loss:  0.7430656664073467 \n",
            "Classification loss: 1.4198004007339478 \n",
            "Regression loss: 0.03316546604037285\n",
            "----------------\n",
            "Loss:  0.7582487761974335 \n",
            "Classification loss: 1.3541189432144165 \n",
            "Regression loss: 0.08118930459022522\n",
            "----------------\n",
            "Loss:  0.7700250297784805 \n",
            "Classification loss: 1.4342575073242188 \n",
            "Regression loss: 0.052896276116371155\n",
            "----------------\n",
            "Loss:  0.790745448321104 \n",
            "Classification loss: 1.5224617719650269 \n",
            "Regression loss: 0.029514562338590622\n",
            "----------------\n",
            "Loss:  0.7839365303516388 \n",
            "Classification loss: 1.4899300336837769 \n",
            "Regression loss: 0.038971513509750366\n",
            "----------------\n",
            "Loss:  0.8533139508217573 \n",
            "Classification loss: 1.6550040245056152 \n",
            "Regression loss: 0.0258119385689497\n",
            "----------------\n",
            "Loss:  0.8169383816421032 \n",
            "Classification loss: 1.5278445482254028 \n",
            "Regression loss: 0.05301610752940178\n",
            "----------------\n",
            "Loss:  0.7732406575232744 \n",
            "Classification loss: 1.5015019178390503 \n",
            "Regression loss: 0.022489698603749275\n",
            "----------------\n",
            "Loss:  0.8218251839280128 \n",
            "Classification loss: 1.5352883338928223 \n",
            "Regression loss: 0.054181016981601715\n",
            "----------------\n",
            "Loss:  0.8682540133595467 \n",
            "Classification loss: 1.6321111917495728 \n",
            "Regression loss: 0.052198417484760284\n",
            "----------------\n",
            "Loss:  0.8200245462357998 \n",
            "Classification loss: 1.5264923572540283 \n",
            "Regression loss: 0.05677836760878563\n",
            "----------------\n",
            "Loss:  0.7710383906960487 \n",
            "Classification loss: 1.4317047595977783 \n",
            "Regression loss: 0.055186010897159576\n",
            "----------------\n",
            "Loss:  0.891726441681385 \n",
            "Classification loss: 1.6509640216827393 \n",
            "Regression loss: 0.06624443084001541\n",
            "----------------\n",
            "Loss:  0.8018658235669136 \n",
            "Classification loss: 1.4784759283065796 \n",
            "Regression loss: 0.06262785941362381\n",
            "----------------\n",
            "Loss:  0.8449170291423798 \n",
            "Classification loss: 1.5569630861282349 \n",
            "Regression loss: 0.06643548607826233\n",
            "----------------\n",
            "Loss:  0.8236870542168617 \n",
            "Classification loss: 1.5577548742294312 \n",
            "Regression loss: 0.04480961710214615\n",
            "----------------\n",
            "Loss:  0.7918572574853897 \n",
            "Classification loss: 1.499565601348877 \n",
            "Regression loss: 0.04207445681095123\n",
            "----------------\n",
            "Loss:  0.7353005483746529 \n",
            "Classification loss: 1.3859496116638184 \n",
            "Regression loss: 0.04232574254274368\n",
            "----------------\n",
            "Loss:  0.7913402244448662 \n",
            "Classification loss: 1.4832814931869507 \n",
            "Regression loss: 0.04969947785139084\n",
            "----------------\n",
            "Loss:  0.8268486000597477 \n",
            "Classification loss: 1.5763351917266846 \n",
            "Regression loss: 0.03868100419640541\n",
            "----------------\n",
            "Loss:  0.8109678290784359 \n",
            "Classification loss: 1.5197687149047852 \n",
            "Regression loss: 0.05108347162604332\n",
            "----------------\n",
            "Loss:  0.763844933360815 \n",
            "Classification loss: 1.4311033487319946 \n",
            "Regression loss: 0.048293258994817734\n",
            "----------------\n",
            "Loss:  0.854474663734436 \n",
            "Classification loss: 1.5804344415664673 \n",
            "Regression loss: 0.06425744295120239\n",
            "----------------\n",
            "Loss:  0.7580338940024376 \n",
            "Classification loss: 1.4169026613235474 \n",
            "Regression loss: 0.04958256334066391\n",
            "----------------\n",
            "Loss:  0.7541026547551155 \n",
            "Classification loss: 1.3945188522338867 \n",
            "Regression loss: 0.05684322863817215\n",
            "----------------\n",
            "Loss:  0.8268494457006454 \n",
            "Classification loss: 1.5098533630371094 \n",
            "Regression loss: 0.07192276418209076\n",
            "----------------\n",
            "Loss:  0.8864252232015133 \n",
            "Classification loss: 1.6573948860168457 \n",
            "Regression loss: 0.05772778019309044\n",
            "----------------\n",
            "Loss:  0.9338287375867367 \n",
            "Classification loss: 1.7722251415252686 \n",
            "Regression loss: 0.0477161668241024\n",
            "----------------\n",
            "Loss:  0.8264002613723278 \n",
            "Classification loss: 1.57004976272583 \n",
            "Regression loss: 0.041375380009412766\n",
            "----------------\n",
            "Loss:  0.8380042426288128 \n",
            "Classification loss: 1.5858726501464844 \n",
            "Regression loss: 0.0450679175555706\n",
            "----------------\n",
            "Loss:  0.677216213196516 \n",
            "Classification loss: 1.2345082759857178 \n",
            "Regression loss: 0.05996207520365715\n",
            "----------------\n",
            "Loss:  0.9183005467057228 \n",
            "Classification loss: 1.719547152519226 \n",
            "Regression loss: 0.05852697044610977\n",
            "----------------\n",
            "Loss:  0.7716104276478291 \n",
            "Classification loss: 1.4397177696228027 \n",
            "Regression loss: 0.05175154283642769\n",
            "----------------\n",
            "Loss:  0.669973261654377 \n",
            "Classification loss: 1.248112678527832 \n",
            "Regression loss: 0.04591692239046097\n",
            "----------------\n",
            "Loss:  0.8097654357552528 \n",
            "Classification loss: 1.5079196691513062 \n",
            "Regression loss: 0.05580560117959976\n",
            "----------------\n",
            "Loss:  0.7126051336526871 \n",
            "Classification loss: 1.3599209785461426 \n",
            "Regression loss: 0.032644644379615784\n",
            "----------------\n",
            "Loss:  0.8841675966978073 \n",
            "Classification loss: 1.6399836540222168 \n",
            "Regression loss: 0.06417576968669891\n",
            "----------------\n",
            "Loss:  0.7540726363658905 \n",
            "Classification loss: 1.3304650783538818 \n",
            "Regression loss: 0.08884009718894958\n",
            "----------------\n",
            "Loss:  0.825365699827671 \n",
            "Classification loss: 1.5559841394424438 \n",
            "Regression loss: 0.04737363010644913\n",
            "----------------\n",
            "Loss:  0.8397035859525204 \n",
            "Classification loss: 1.604461908340454 \n",
            "Regression loss: 0.03747263178229332\n",
            "----------------\n",
            "Loss:  0.80347740650177 \n",
            "Classification loss: 1.484085202217102 \n",
            "Regression loss: 0.061434805393218994\n",
            "----------------\n",
            "Loss:  0.7982410155236721 \n",
            "Classification loss: 1.5221688747406006 \n",
            "Regression loss: 0.03715657815337181\n",
            "----------------\n",
            "Loss:  0.8310529664158821 \n",
            "Classification loss: 1.5602840185165405 \n",
            "Regression loss: 0.05091095715761185\n",
            "----------------\n",
            "Loss:  0.6347591727972031 \n",
            "Classification loss: 1.1918233633041382 \n",
            "Regression loss: 0.03884749114513397\n",
            "----------------\n",
            "Loss:  0.8720160759985447 \n",
            "Classification loss: 1.6434623003005981 \n",
            "Regression loss: 0.05028492584824562\n",
            "----------------\n",
            "Loss:  0.6904892697930336 \n",
            "Classification loss: 1.2897248268127441 \n",
            "Regression loss: 0.04562685638666153\n",
            "----------------\n",
            "Loss:  0.8104809075593948 \n",
            "Classification loss: 1.4738483428955078 \n",
            "Regression loss: 0.07355673611164093\n",
            "----------------\n",
            "Loss:  0.7060666847974062 \n",
            "Classification loss: 1.3640570640563965 \n",
            "Regression loss: 0.024038152769207954\n",
            "----------------\n",
            "Loss:  0.934416651725769 \n",
            "Classification loss: 1.7769607305526733 \n",
            "Regression loss: 0.04593628644943237\n",
            "----------------\n",
            "Loss:  0.7759231179952621 \n",
            "Classification loss: 1.3858630657196045 \n",
            "Regression loss: 0.0829915851354599\n",
            "----------------\n",
            "Loss:  0.8447378426790237 \n",
            "Classification loss: 1.5941171646118164 \n",
            "Regression loss: 0.04767926037311554\n",
            "----------------\n",
            "Loss:  0.7378696538507938 \n",
            "Classification loss: 1.4092600345611572 \n",
            "Regression loss: 0.033239636570215225\n",
            "----------------\n",
            "Loss:  0.7110896408557892 \n",
            "Classification loss: 1.326818585395813 \n",
            "Regression loss: 0.04768034815788269\n",
            "----------------\n",
            "Loss:  0.8053554072976112 \n",
            "Classification loss: 1.540313959121704 \n",
            "Regression loss: 0.035198427736759186\n",
            "----------------\n",
            "Loss:  0.7115216441452503 \n",
            "Classification loss: 1.3709272146224976 \n",
            "Regression loss: 0.02605803683400154\n",
            "----------------\n",
            "Loss:  0.7779421210289001 \n",
            "Classification loss: 1.481693983078003 \n",
            "Regression loss: 0.03709512948989868\n",
            "----------------\n",
            "Loss:  0.7954026609659195 \n",
            "Classification loss: 1.4439599514007568 \n",
            "Regression loss: 0.07342268526554108\n",
            "----------------\n",
            "Loss:  0.7194237187504768 \n",
            "Classification loss: 1.3173304796218872 \n",
            "Regression loss: 0.060758478939533234\n",
            "----------------\n",
            "Loss:  0.7112102434039116 \n",
            "Classification loss: 1.3278977870941162 \n",
            "Regression loss: 0.047261349856853485\n",
            "----------------\n",
            "Loss:  0.7025474198162556 \n",
            "Classification loss: 1.313971996307373 \n",
            "Regression loss: 0.045561421662569046\n",
            "----------------\n",
            "Loss:  0.7832176201045513 \n",
            "Classification loss: 1.507770299911499 \n",
            "Regression loss: 0.029332470148801804\n",
            "----------------\n",
            "Loss:  0.7828719168901443 \n",
            "Classification loss: 1.4746272563934326 \n",
            "Regression loss: 0.04555828869342804\n",
            "----------------\n",
            "Loss:  0.7986988201737404 \n",
            "Classification loss: 1.479833960533142 \n",
            "Regression loss: 0.05878183990716934\n",
            "----------------\n",
            "Loss:  0.7700661718845367 \n",
            "Classification loss: 1.4420777559280396 \n",
            "Regression loss: 0.04902729392051697\n",
            "----------------\n",
            "Loss:  0.8662863224744797 \n",
            "Classification loss: 1.6049696207046509 \n",
            "Regression loss: 0.06380151212215424\n",
            "----------------\n",
            "Loss:  0.7027352452278137 \n",
            "Classification loss: 1.3207906484603882 \n",
            "Regression loss: 0.04233992099761963\n",
            "----------------\n",
            "Loss:  0.8470759019255638 \n",
            "Classification loss: 1.590836524963379 \n",
            "Regression loss: 0.05165763944387436\n",
            "----------------\n",
            "Loss:  0.7772554308176041 \n",
            "Classification loss: 1.4280648231506348 \n",
            "Regression loss: 0.06322301924228668\n",
            "----------------\n",
            "Loss:  0.925808385014534 \n",
            "Classification loss: 1.7501535415649414 \n",
            "Regression loss: 0.05073161423206329\n",
            "----------------\n",
            "Loss:  0.7787832282483578 \n",
            "Classification loss: 1.462145447731018 \n",
            "Regression loss: 0.04771050438284874\n",
            "----------------\n",
            "Loss:  0.8160861283540726 \n",
            "Classification loss: 1.5399153232574463 \n",
            "Regression loss: 0.046128466725349426\n",
            "----------------\n",
            "Loss:  0.7187412083148956 \n",
            "Classification loss: 1.3063715696334839 \n",
            "Regression loss: 0.06555542349815369\n",
            "----------------\n",
            "Loss:  0.6754960902035236 \n",
            "Classification loss: 1.2436354160308838 \n",
            "Regression loss: 0.05367838218808174\n",
            "----------------\n",
            "Loss:  0.7770665548741817 \n",
            "Classification loss: 1.4634513854980469 \n",
            "Regression loss: 0.04534086212515831\n",
            "----------------\n",
            "Loss:  0.7350869216024876 \n",
            "Classification loss: 1.3631279468536377 \n",
            "Regression loss: 0.053522948175668716\n",
            "----------------\n",
            "Loss:  0.7890347614884377 \n",
            "Classification loss: 1.5070735216140747 \n",
            "Regression loss: 0.0354980006814003\n",
            "----------------\n",
            "Loss:  0.7812066078186035 \n",
            "Classification loss: 1.4119367599487305 \n",
            "Regression loss: 0.07523822784423828\n",
            "----------------\n",
            "Loss:  0.8094985038042068 \n",
            "Classification loss: 1.4931740760803223 \n",
            "Regression loss: 0.06291146576404572\n",
            "----------------\n",
            "Loss:  0.8177058771252632 \n",
            "Classification loss: 1.5335501432418823 \n",
            "Regression loss: 0.05093080550432205\n",
            "----------------\n",
            "Loss:  0.7934057414531708 \n",
            "Classification loss: 1.4647254943847656 \n",
            "Regression loss: 0.061042994260787964\n",
            "----------------\n",
            "Loss:  0.6998868770897388 \n",
            "Classification loss: 1.3244061470031738 \n",
            "Regression loss: 0.03768380358815193\n",
            "----------------\n",
            "Loss:  0.7793643958866596 \n",
            "Classification loss: 1.4820501804351807 \n",
            "Regression loss: 0.03833930566906929\n",
            "----------------\n",
            "Loss:  0.7486687079071999 \n",
            "Classification loss: 1.3994516134262085 \n",
            "Regression loss: 0.04894290119409561\n",
            "----------------\n",
            "Loss:  0.7834910526871681 \n",
            "Classification loss: 1.4521520137786865 \n",
            "Regression loss: 0.05741504579782486\n",
            "----------------\n",
            "Loss:  0.7413184717297554 \n",
            "Classification loss: 1.3292633295059204 \n",
            "Regression loss: 0.0766868069767952\n",
            "----------------\n",
            "Loss:  0.8037131018936634 \n",
            "Classification loss: 1.4827907085418701 \n",
            "Regression loss: 0.06231774762272835\n",
            "----------------\n",
            "Loss:  0.8166113570332527 \n",
            "Classification loss: 1.5245157480239868 \n",
            "Regression loss: 0.05435348302125931\n",
            "----------------\n",
            "Loss:  0.7087871544063091 \n",
            "Classification loss: 1.3508423566818237 \n",
            "Regression loss: 0.03336597606539726\n",
            "----------------\n",
            "Loss:  0.7414420247077942 \n",
            "Classification loss: 1.3911272287368774 \n",
            "Regression loss: 0.04587841033935547\n",
            "----------------\n",
            "Loss:  0.7619460709393024 \n",
            "Classification loss: 1.4496511220932007 \n",
            "Regression loss: 0.0371205098927021\n",
            "----------------\n",
            "Loss:  0.8899491541087627 \n",
            "Classification loss: 1.6673052310943604 \n",
            "Regression loss: 0.056296538561582565\n",
            "----------------\n",
            "Loss:  0.8405729606747627 \n",
            "Classification loss: 1.5505645275115967 \n",
            "Regression loss: 0.06529069691896439\n",
            "----------------\n",
            "Loss:  0.8176363855600357 \n",
            "Classification loss: 1.5065233707427979 \n",
            "Regression loss: 0.06437470018863678\n",
            "----------------\n",
            "Loss:  0.8567938059568405 \n",
            "Classification loss: 1.5754117965698242 \n",
            "Regression loss: 0.0690879076719284\n",
            "----------------\n",
            "Loss:  0.8647086396813393 \n",
            "Classification loss: 1.6194581985473633 \n",
            "Regression loss: 0.05497954040765762\n",
            "----------------\n",
            "Loss:  0.7254190966486931 \n",
            "Classification loss: 1.3427653312683105 \n",
            "Regression loss: 0.05403643101453781\n",
            "----------------\n",
            "Loss:  0.7385676093399525 \n",
            "Classification loss: 1.3776886463165283 \n",
            "Regression loss: 0.04972328618168831\n",
            "----------------\n",
            "Loss:  0.7821872755885124 \n",
            "Classification loss: 1.445749282836914 \n",
            "Regression loss: 0.05931263417005539\n",
            "----------------\n",
            "Loss:  0.8107595145702362 \n",
            "Classification loss: 1.5353648662567139 \n",
            "Regression loss: 0.04307708144187927\n",
            "----------------\n",
            "Loss:  0.805681049823761 \n",
            "Classification loss: 1.5259195566177368 \n",
            "Regression loss: 0.04272127151489258\n",
            "----------------\n",
            "Loss:  0.7629954107105732 \n",
            "Classification loss: 1.433182716369629 \n",
            "Regression loss: 0.04640405252575874\n",
            "----------------\n",
            "Loss:  0.7714705020189285 \n",
            "Classification loss: 1.4536467790603638 \n",
            "Regression loss: 0.04464711248874664\n",
            "----------------\n",
            "Loss:  0.8586554192006588 \n",
            "Classification loss: 1.609610676765442 \n",
            "Regression loss: 0.05385008081793785\n",
            "----------------\n",
            "Loss:  0.7547507062554359 \n",
            "Classification loss: 1.4001495838165283 \n",
            "Regression loss: 0.05467591434717178\n",
            "----------------\n",
            "Loss:  0.869190439581871 \n",
            "Classification loss: 1.6032317876815796 \n",
            "Regression loss: 0.06757454574108124\n",
            "----------------\n",
            "Loss:  0.8156396709382534 \n",
            "Classification loss: 1.542004108428955 \n",
            "Regression loss: 0.044637616723775864\n",
            "----------------\n",
            "Loss:  0.8173320852220058 \n",
            "Classification loss: 1.5219532251358032 \n",
            "Regression loss: 0.05635547265410423\n",
            "----------------\n",
            "Loss:  0.9434103444218636 \n",
            "Classification loss: 1.766330361366272 \n",
            "Regression loss: 0.06024516373872757\n",
            "----------------\n",
            "Loss:  0.854782797396183 \n",
            "Classification loss: 1.6085689067840576 \n",
            "Regression loss: 0.050498344004154205\n",
            "----------------\n",
            "Loss:  0.8227993510663509 \n",
            "Classification loss: 1.5819742679595947 \n",
            "Regression loss: 0.031812217086553574\n",
            "----------------\n",
            "Loss:  0.7461977265775204 \n",
            "Classification loss: 1.4104756116867065 \n",
            "Regression loss: 0.0409599207341671\n",
            "----------------\n",
            "Loss:  0.7901240773499012 \n",
            "Classification loss: 1.4823142290115356 \n",
            "Regression loss: 0.04896696284413338\n",
            "----------------\n",
            "Loss:  0.833773598074913 \n",
            "Classification loss: 1.541771411895752 \n",
            "Regression loss: 0.06288789212703705\n",
            "----------------\n",
            "Loss:  0.8423579037189484 \n",
            "Classification loss: 1.5525366067886353 \n",
            "Regression loss: 0.06608960032463074\n",
            "----------------\n",
            "Loss:  0.8477675840258598 \n",
            "Classification loss: 1.5776207447052002 \n",
            "Regression loss: 0.058957211673259735\n",
            "----------------\n",
            "Loss:  0.8987220078706741 \n",
            "Classification loss: 1.7438472509384155 \n",
            "Regression loss: 0.02679838240146637\n",
            "----------------\n",
            "Loss:  0.7912913225591183 \n",
            "Classification loss: 1.5003620386123657 \n",
            "Regression loss: 0.04111030325293541\n",
            "----------------\n",
            "Loss:  0.8259944692254066 \n",
            "Classification loss: 1.5074115991592407 \n",
            "Regression loss: 0.07228866964578629\n",
            "----------------\n",
            "Loss:  0.8970438465476036 \n",
            "Classification loss: 1.6740150451660156 \n",
            "Regression loss: 0.060036323964595795\n",
            "----------------\n",
            "Loss:  0.9042160175740719 \n",
            "Classification loss: 1.7053638696670532 \n",
            "Regression loss: 0.05153408274054527\n",
            "----------------\n",
            "Loss:  0.8377681523561478 \n",
            "Classification loss: 1.580338478088379 \n",
            "Regression loss: 0.04759891331195831\n",
            "----------------\n",
            "Loss:  0.824958860874176 \n",
            "Classification loss: 1.5177478790283203 \n",
            "Regression loss: 0.06608492136001587\n",
            "----------------\n",
            "Loss:  0.7141145616769791 \n",
            "Classification loss: 1.3560209274291992 \n",
            "Regression loss: 0.036104097962379456\n",
            "----------------\n",
            "Loss:  0.7666593715548515 \n",
            "Classification loss: 1.478775978088379 \n",
            "Regression loss: 0.02727138251066208\n",
            "----------------\n",
            "Loss:  0.7935779727995396 \n",
            "Classification loss: 1.5180189609527588 \n",
            "Regression loss: 0.03456849232316017\n",
            "----------------\n",
            "Loss:  0.8690016902983189 \n",
            "Classification loss: 1.6281870603561401 \n",
            "Regression loss: 0.054908160120248795\n",
            "----------------\n",
            "Loss:  0.8205864764750004 \n",
            "Classification loss: 1.556858777999878 \n",
            "Regression loss: 0.04215708747506142\n",
            "----------------\n",
            "Loss:  0.831409078091383 \n",
            "Classification loss: 1.5621318817138672 \n",
            "Regression loss: 0.05034313723444939\n",
            "----------------\n",
            "Loss:  0.9398284405469894 \n",
            "Classification loss: 1.7196290493011475 \n",
            "Regression loss: 0.08001391589641571\n",
            "----------------\n",
            "Loss:  0.6410247161984444 \n",
            "Classification loss: 1.217411756515503 \n",
            "Regression loss: 0.0323188379406929\n",
            "----------------\n",
            "Loss:  0.7746342606842518 \n",
            "Classification loss: 1.4514966011047363 \n",
            "Regression loss: 0.04888596013188362\n",
            "----------------\n",
            "Loss:  0.701599981635809 \n",
            "Classification loss: 1.3135323524475098 \n",
            "Regression loss: 0.04483380541205406\n",
            "----------------\n",
            "Loss:  0.8797137625515461 \n",
            "Classification loss: 1.6359769105911255 \n",
            "Regression loss: 0.06172530725598335\n",
            "----------------\n",
            "Loss:  0.7619804814457893 \n",
            "Classification loss: 1.3893064260482788 \n",
            "Regression loss: 0.06732726842164993\n",
            "----------------\n",
            "Loss:  0.7975318282842636 \n",
            "Classification loss: 1.4696416854858398 \n",
            "Regression loss: 0.06271098554134369\n",
            "----------------\n",
            "Loss:  0.781495738774538 \n",
            "Classification loss: 1.5053670406341553 \n",
            "Regression loss: 0.028812218457460403\n",
            "----------------\n",
            "Loss:  0.675666943192482 \n",
            "Classification loss: 1.2870911359786987 \n",
            "Regression loss: 0.03212137520313263\n",
            "----------------\n",
            "Loss:  0.7443960346281528 \n",
            "Classification loss: 1.39911949634552 \n",
            "Regression loss: 0.04483628645539284\n",
            "----------------\n",
            "Loss:  0.7404517196118832 \n",
            "Classification loss: 1.376072645187378 \n",
            "Regression loss: 0.0524153970181942\n",
            "----------------\n",
            "Loss:  0.710590660572052 \n",
            "Classification loss: 1.3048473596572876 \n",
            "Regression loss: 0.0581669807434082\n",
            "----------------\n",
            "Loss:  0.8170819133520126 \n",
            "Classification loss: 1.530266523361206 \n",
            "Regression loss: 0.05194865167140961\n",
            "----------------\n",
            "Loss:  0.7596666589379311 \n",
            "Classification loss: 1.4571870565414429 \n",
            "Regression loss: 0.031073130667209625\n",
            "----------------\n",
            "Loss:  0.706546638160944 \n",
            "Classification loss: 1.3443646430969238 \n",
            "Regression loss: 0.03436431661248207\n",
            "----------------\n",
            "Loss:  0.8768267519772053 \n",
            "Classification loss: 1.635667324066162 \n",
            "Regression loss: 0.05899308994412422\n",
            "----------------\n",
            "Loss:  0.7931346595287323 \n",
            "Classification loss: 1.47833251953125 \n",
            "Regression loss: 0.0539683997631073\n",
            "----------------\n",
            "Loss:  0.8952430039644241 \n",
            "Classification loss: 1.6659656763076782 \n",
            "Regression loss: 0.06226016581058502\n",
            "----------------\n",
            "Loss:  0.8533125445246696 \n",
            "Classification loss: 1.5916664600372314 \n",
            "Regression loss: 0.057479314506053925\n",
            "----------------\n",
            "Loss:  0.7333022058010101 \n",
            "Classification loss: 1.360501766204834 \n",
            "Regression loss: 0.05305132269859314\n",
            "----------------\n",
            "Loss:  0.8322171196341515 \n",
            "Classification loss: 1.5275654792785645 \n",
            "Regression loss: 0.06843437999486923\n",
            "----------------\n",
            "Loss:  0.8099303767085075 \n",
            "Classification loss: 1.5041613578796387 \n",
            "Regression loss: 0.0578496977686882\n",
            "----------------\n",
            "Loss:  0.8091598600149155 \n",
            "Classification loss: 1.468153953552246 \n",
            "Regression loss: 0.07508288323879242\n",
            "----------------\n",
            "Loss:  0.7432771399617195 \n",
            "Classification loss: 1.3687434196472168 \n",
            "Regression loss: 0.058905430138111115\n",
            "----------------\n",
            "Loss:  0.6385312266647816 \n",
            "Classification loss: 1.2152636051177979 \n",
            "Regression loss: 0.030899424105882645\n",
            "----------------\n",
            "Loss:  0.8060166165232658 \n",
            "Classification loss: 1.5039912462234497 \n",
            "Regression loss: 0.054020993411540985\n",
            "----------------\n",
            "Loss:  0.7694993987679482 \n",
            "Classification loss: 1.4612972736358643 \n",
            "Regression loss: 0.03885076195001602\n",
            "----------------\n",
            "Loss:  0.6576081365346909 \n",
            "Classification loss: 1.215117335319519 \n",
            "Regression loss: 0.050049468874931335\n",
            "----------------\n",
            "Loss:  0.8183810077607632 \n",
            "Classification loss: 1.559788465499878 \n",
            "Regression loss: 0.038486775010824203\n",
            "----------------\n",
            "Loss:  0.8722786903381348 \n",
            "Classification loss: 1.6344056129455566 \n",
            "Regression loss: 0.055075883865356445\n",
            "----------------\n",
            "Loss:  0.9233425185084343 \n",
            "Classification loss: 1.7214521169662476 \n",
            "Regression loss: 0.06261646002531052\n",
            "----------------\n",
            "Loss:  0.8638336174190044 \n",
            "Classification loss: 1.6131184101104736 \n",
            "Regression loss: 0.057274412363767624\n",
            "----------------\n",
            "Loss:  0.774042434990406 \n",
            "Classification loss: 1.4571847915649414 \n",
            "Regression loss: 0.04545003920793533\n",
            "----------------\n",
            "Loss:  0.7685917019844055 \n",
            "Classification loss: 1.4240281581878662 \n",
            "Regression loss: 0.05657762289047241\n",
            "----------------\n",
            "Loss:  0.9091037549078465 \n",
            "Classification loss: 1.7015687227249146 \n",
            "Regression loss: 0.058319393545389175\n",
            "----------------\n",
            "Loss:  0.8316811285912991 \n",
            "Classification loss: 1.572016716003418 \n",
            "Regression loss: 0.04567277058959007\n",
            "----------------\n",
            "Loss:  0.7452992461621761 \n",
            "Classification loss: 1.4088661670684814 \n",
            "Regression loss: 0.04086616262793541\n",
            "----------------\n",
            "Loss:  0.7114074975252151 \n",
            "Classification loss: 1.3425424098968506 \n",
            "Regression loss: 0.040136292576789856\n",
            "----------------\n",
            "Loss:  0.8243717439472675 \n",
            "Classification loss: 1.5411864519119263 \n",
            "Regression loss: 0.0537785179913044\n",
            "----------------\n",
            "Loss:  0.7628561034798622 \n",
            "Classification loss: 1.4552870988845825 \n",
            "Regression loss: 0.03521255403757095\n",
            "----------------\n",
            "Loss:  0.8342546746134758 \n",
            "Classification loss: 1.5233209133148193 \n",
            "Regression loss: 0.07259421795606613\n",
            "----------------\n",
            "Loss:  0.7676902413368225 \n",
            "Classification loss: 1.4655394554138184 \n",
            "Regression loss: 0.03492051362991333\n",
            "----------------\n",
            "Loss:  0.7611298821866512 \n",
            "Classification loss: 1.4507100582122803 \n",
            "Regression loss: 0.03577485308051109\n",
            "----------------\n",
            "Loss:  0.7190188616514206 \n",
            "Classification loss: 1.356745958328247 \n",
            "Regression loss: 0.04064588248729706\n",
            "----------------\n",
            "Loss:  0.7512175031006336 \n",
            "Classification loss: 1.396193504333496 \n",
            "Regression loss: 0.053120750933885574\n",
            "----------------\n",
            "Loss:  0.739510253071785 \n",
            "Classification loss: 1.4081966876983643 \n",
            "Regression loss: 0.035411909222602844\n",
            "----------------\n",
            "Loss:  0.7422594130039215 \n",
            "Classification loss: 1.402454137802124 \n",
            "Regression loss: 0.0410323441028595\n",
            "----------------\n",
            "Loss:  0.881696954369545 \n",
            "Classification loss: 1.6682488918304443 \n",
            "Regression loss: 0.047572508454322815\n",
            "----------------\n",
            "Loss:  0.9420643001794815 \n",
            "Classification loss: 1.7852952480316162 \n",
            "Regression loss: 0.0494166761636734\n",
            "----------------\n",
            "Loss:  0.8487017378211021 \n",
            "Classification loss: 1.5713430643081665 \n",
            "Regression loss: 0.06303020566701889\n",
            "----------------\n",
            "Loss:  0.8475451767444611 \n",
            "Classification loss: 1.564526081085205 \n",
            "Regression loss: 0.06528213620185852\n",
            "----------------\n",
            "Loss:  0.8279755115509033 \n",
            "Classification loss: 1.5551596879959106 \n",
            "Regression loss: 0.050395667552948\n",
            "----------------\n",
            "Loss:  0.8076749481260777 \n",
            "Classification loss: 1.540036916732788 \n",
            "Regression loss: 0.03765648975968361\n",
            "----------------\n",
            "Loss:  0.7601408436894417 \n",
            "Classification loss: 1.431438684463501 \n",
            "Regression loss: 0.04442150145769119\n",
            "----------------\n",
            "Loss:  0.8160083219408989 \n",
            "Classification loss: 1.5393133163452148 \n",
            "Regression loss: 0.04635166376829147\n",
            "----------------\n",
            "Loss:  0.7073099948465824 \n",
            "Classification loss: 1.2972614765167236 \n",
            "Regression loss: 0.058679256588220596\n",
            "----------------\n",
            "Loss:  0.8080249093472958 \n",
            "Classification loss: 1.5301107168197632 \n",
            "Regression loss: 0.04296955093741417\n",
            "----------------\n",
            "Loss:  0.7435275800526142 \n",
            "Classification loss: 1.3639978170394897 \n",
            "Regression loss: 0.06152867153286934\n",
            "----------------\n",
            "Loss:  0.7384132817387581 \n",
            "Classification loss: 1.3820146322250366 \n",
            "Regression loss: 0.04740596562623978\n",
            "----------------\n",
            "Loss:  0.8564958050847054 \n",
            "Classification loss: 1.6002371311187744 \n",
            "Regression loss: 0.056377239525318146\n",
            "----------------\n",
            "Loss:  0.7539084255695343 \n",
            "Classification loss: 1.4404475688934326 \n",
            "Regression loss: 0.03368464112281799\n",
            "----------------\n",
            "Loss:  0.8816908337175846 \n",
            "Classification loss: 1.6564884185791016 \n",
            "Regression loss: 0.05344662442803383\n",
            "----------------\n",
            "Loss:  0.8152000345289707 \n",
            "Classification loss: 1.5278595685958862 \n",
            "Regression loss: 0.0512702502310276\n",
            "----------------\n",
            "Loss:  0.8037617169320583 \n",
            "Classification loss: 1.5081229209899902 \n",
            "Regression loss: 0.04970025643706322\n",
            "----------------\n",
            "Loss:  0.7844461388885975 \n",
            "Classification loss: 1.5142461061477661 \n",
            "Regression loss: 0.027323085814714432\n",
            "----------------\n",
            "Loss:  0.7610448710620403 \n",
            "Classification loss: 1.4450106620788574 \n",
            "Regression loss: 0.03853954002261162\n",
            "----------------\n",
            "Loss:  0.7768990024924278 \n",
            "Classification loss: 1.4973382949829102 \n",
            "Regression loss: 0.028229855000972748\n",
            "----------------\n",
            "Loss:  0.798943392932415 \n",
            "Classification loss: 1.5207469463348389 \n",
            "Regression loss: 0.038569919764995575\n",
            "----------------\n",
            "Loss:  0.7845160439610481 \n",
            "Classification loss: 1.4858591556549072 \n",
            "Regression loss: 0.04158646613359451\n",
            "----------------\n",
            "Loss:  0.7584302425384521 \n",
            "Classification loss: 1.4312890768051147 \n",
            "Regression loss: 0.042785704135894775\n",
            "----------------\n",
            "Loss:  0.8527397103607655 \n",
            "Classification loss: 1.641550898551941 \n",
            "Regression loss: 0.031964261084795\n",
            "----------------\n",
            "Loss:  0.801901251077652 \n",
            "Classification loss: 1.4656004905700684 \n",
            "Regression loss: 0.0691010057926178\n",
            "----------------\n",
            "Loss:  0.8983113020658493 \n",
            "Classification loss: 1.649332880973816 \n",
            "Regression loss: 0.07364486157894135\n",
            "----------------\n",
            "Loss:  0.7971730157732964 \n",
            "Classification loss: 1.5076231956481934 \n",
            "Regression loss: 0.043361417949199677\n",
            "----------------\n",
            "Loss:  0.6925280764698982 \n",
            "Classification loss: 1.2779321670532227 \n",
            "Regression loss: 0.053561992943286896\n",
            "----------------\n",
            "Loss:  0.7964383438229561 \n",
            "Classification loss: 1.4934089183807373 \n",
            "Regression loss: 0.04973388463258743\n",
            "----------------\n",
            "Loss:  0.7762432992458344 \n",
            "Classification loss: 1.450587272644043 \n",
            "Regression loss: 0.050949662923812866\n",
            "----------------\n",
            "Loss:  0.677650947123766 \n",
            "Classification loss: 1.2665801048278809 \n",
            "Regression loss: 0.044360894709825516\n",
            "----------------\n",
            "Loss:  0.8609841987490654 \n",
            "Classification loss: 1.58815336227417 \n",
            "Regression loss: 0.06690751761198044\n",
            "----------------\n",
            "Loss:  0.9018402770161629 \n",
            "Classification loss: 1.6526095867156982 \n",
            "Regression loss: 0.07553548365831375\n",
            "----------------\n",
            "Loss:  0.7884529754519463 \n",
            "Classification loss: 1.4714094400405884 \n",
            "Regression loss: 0.05274825543165207\n",
            "----------------\n",
            "Loss:  0.7962174117565155 \n",
            "Classification loss: 1.4946035146713257 \n",
            "Regression loss: 0.04891565442085266\n",
            "----------------\n",
            "Loss:  0.6746733672916889 \n",
            "Classification loss: 1.2537695169448853 \n",
            "Regression loss: 0.04778860881924629\n",
            "----------------\n",
            "Loss:  0.8880042433738708 \n",
            "Classification loss: 1.699812889099121 \n",
            "Regression loss: 0.0380977988243103\n",
            "----------------\n",
            "Loss:  0.8241000100970268 \n",
            "Classification loss: 1.5363121032714844 \n",
            "Regression loss: 0.05594395846128464\n",
            "----------------\n",
            "Loss:  0.767949040979147 \n",
            "Classification loss: 1.4224852323532104 \n",
            "Regression loss: 0.05670642480254173\n",
            "----------------\n",
            "Loss:  0.7197112068533897 \n",
            "Classification loss: 1.3433632850646973 \n",
            "Regression loss: 0.04802956432104111\n",
            "----------------\n",
            "Loss:  0.8169630654156208 \n",
            "Classification loss: 1.5437543392181396 \n",
            "Regression loss: 0.04508589580655098\n",
            "----------------\n",
            "Loss:  0.7340526133775711 \n",
            "Classification loss: 1.3745145797729492 \n",
            "Regression loss: 0.0467953234910965\n",
            "----------------\n",
            "Loss:  0.8148445822298527 \n",
            "Classification loss: 1.519730567932129 \n",
            "Regression loss: 0.05497929826378822\n",
            "----------------\n",
            "Loss:  0.6926017552614212 \n",
            "Classification loss: 1.307326316833496 \n",
            "Regression loss: 0.03893859684467316\n",
            "----------------\n",
            "Loss:  0.7944466508924961 \n",
            "Classification loss: 1.4937193393707275 \n",
            "Regression loss: 0.04758698120713234\n",
            "----------------\n",
            "Loss:  0.7248224765062332 \n",
            "Classification loss: 1.384438157081604 \n",
            "Regression loss: 0.03260339796543121\n",
            "----------------\n",
            "Loss:  0.7640400230884552 \n",
            "Classification loss: 1.3809757232666016 \n",
            "Regression loss: 0.07355216145515442\n",
            "----------------\n",
            "Loss:  0.8224137090146542 \n",
            "Classification loss: 1.5615898370742798 \n",
            "Regression loss: 0.04161879047751427\n",
            "----------------\n",
            "Loss:  0.8176796063780785 \n",
            "Classification loss: 1.4914942979812622 \n",
            "Regression loss: 0.07193245738744736\n",
            "----------------\n",
            "Loss:  0.8223932683467865 \n",
            "Classification loss: 1.5107057094573975 \n",
            "Regression loss: 0.06704041361808777\n",
            "----------------\n",
            "Loss:  0.7862379290163517 \n",
            "Classification loss: 1.4858002662658691 \n",
            "Regression loss: 0.04333779588341713\n",
            "----------------\n",
            "Loss:  0.8061941638588905 \n",
            "Classification loss: 1.4985566139221191 \n",
            "Regression loss: 0.05691585689783096\n",
            "----------------\n",
            "Loss:  0.7279064357280731 \n",
            "Classification loss: 1.3934102058410645 \n",
            "Regression loss: 0.031201332807540894\n",
            "----------------\n",
            "Loss:  0.8312753029167652 \n",
            "Classification loss: 1.5451257228851318 \n",
            "Regression loss: 0.058712441474199295\n",
            "----------------\n",
            "Loss:  0.9284220039844513 \n",
            "Classification loss: 1.749871015548706 \n",
            "Regression loss: 0.05348649621009827\n",
            "----------------\n",
            "Loss:  0.7300832830369473 \n",
            "Classification loss: 1.3800694942474365 \n",
            "Regression loss: 0.04004853591322899\n",
            "----------------\n",
            "Loss:  0.846059363335371 \n",
            "Classification loss: 1.5856897830963135 \n",
            "Regression loss: 0.05321447178721428\n",
            "----------------\n",
            "Loss:  0.7928881496191025 \n",
            "Classification loss: 1.508270025253296 \n",
            "Regression loss: 0.03875313699245453\n",
            "----------------\n",
            "Loss:  0.8530633822083473 \n",
            "Classification loss: 1.6167632341384888 \n",
            "Regression loss: 0.044681765139102936\n",
            "----------------\n",
            "Loss:  0.6929792165756226 \n",
            "Classification loss: 1.3062732219696045 \n",
            "Regression loss: 0.03984260559082031\n",
            "----------------\n",
            "Loss:  0.6675350144505501 \n",
            "Classification loss: 1.2323346138000488 \n",
            "Regression loss: 0.051367707550525665\n",
            "----------------\n",
            "Loss:  0.7646074779331684 \n",
            "Classification loss: 1.4386125802993774 \n",
            "Regression loss: 0.04530118778347969\n",
            "----------------\n",
            "Loss:  0.7793473191559315 \n",
            "Classification loss: 1.4533847570419312 \n",
            "Regression loss: 0.0526549406349659\n",
            "----------------\n",
            "Loss:  0.7857704386115074 \n",
            "Classification loss: 1.4859933853149414 \n",
            "Regression loss: 0.04277374595403671\n",
            "----------------\n",
            "Loss:  0.7885732762515545 \n",
            "Classification loss: 1.5007107257843018 \n",
            "Regression loss: 0.03821791335940361\n",
            "----------------\n",
            "Loss:  0.9518347047269344 \n",
            "Classification loss: 1.8061327934265137 \n",
            "Regression loss: 0.0487683080136776\n",
            "----------------\n",
            "Loss:  0.7803957164287567 \n",
            "Classification loss: 1.3875293731689453 \n",
            "Regression loss: 0.08663102984428406\n",
            "----------------\n",
            "Loss:  0.7561737447977066 \n",
            "Classification loss: 1.4104347229003906 \n",
            "Regression loss: 0.05095638334751129\n",
            "----------------\n",
            "Loss:  0.7834551632404327 \n",
            "Classification loss: 1.4745383262634277 \n",
            "Regression loss: 0.04618600010871887\n",
            "----------------\n",
            "Loss:  0.7261518687009811 \n",
            "Classification loss: 1.3689523935317993 \n",
            "Regression loss: 0.04167567193508148\n",
            "----------------\n",
            "Loss:  0.7912809327244759 \n",
            "Classification loss: 1.4465149641036987 \n",
            "Regression loss: 0.0680234506726265\n",
            "----------------\n",
            "Loss:  0.7750313319265842 \n",
            "Classification loss: 1.4354770183563232 \n",
            "Regression loss: 0.05729282274842262\n",
            "----------------\n",
            "Loss:  0.8989049047231674 \n",
            "Classification loss: 1.708565592765808 \n",
            "Regression loss: 0.04462210834026337\n",
            "----------------\n",
            "Loss:  0.831087987869978 \n",
            "Classification loss: 1.5409915447235107 \n",
            "Regression loss: 0.06059221550822258\n",
            "----------------\n",
            "Loss:  0.9077907130122185 \n",
            "Classification loss: 1.6821870803833008 \n",
            "Regression loss: 0.06669717282056808\n",
            "----------------\n",
            "Loss:  0.7873517982661724 \n",
            "Classification loss: 1.4698255062103271 \n",
            "Regression loss: 0.052439045161008835\n",
            "----------------\n",
            "Loss:  0.7240158766508102 \n",
            "Classification loss: 1.3033983707427979 \n",
            "Regression loss: 0.07231669127941132\n",
            "----------------\n",
            "Loss:  0.8890495225787163 \n",
            "Classification loss: 1.6686251163482666 \n",
            "Regression loss: 0.05473696440458298\n",
            "----------------\n",
            "Loss:  0.6796569935977459 \n",
            "Classification loss: 1.2953654527664185 \n",
            "Regression loss: 0.03197426721453667\n",
            "----------------\n",
            "Loss:  0.8774729073047638 \n",
            "Classification loss: 1.658482551574707 \n",
            "Regression loss: 0.04823163151741028\n",
            "----------------\n",
            "Loss:  0.8554520457983017 \n",
            "Classification loss: 1.5783730745315552 \n",
            "Regression loss: 0.06626550853252411\n",
            "----------------\n",
            "Loss:  0.7707035578787327 \n",
            "Classification loss: 1.4698526859283447 \n",
            "Regression loss: 0.03577721491456032\n",
            "----------------\n",
            "Loss:  0.8114350996911526 \n",
            "Classification loss: 1.57400381565094 \n",
            "Regression loss: 0.024433191865682602\n",
            "----------------\n",
            "Loss:  0.7595532648265362 \n",
            "Classification loss: 1.4034005403518677 \n",
            "Regression loss: 0.05785299465060234\n",
            "----------------\n",
            "Loss:  0.9201438277959824 \n",
            "Classification loss: 1.679152011871338 \n",
            "Regression loss: 0.08056782186031342\n",
            "----------------\n",
            "Loss:  0.8136762268841267 \n",
            "Classification loss: 1.536087989807129 \n",
            "Regression loss: 0.04563223198056221\n",
            "----------------\n",
            "Loss:  0.8084617014974356 \n",
            "Classification loss: 1.5552394390106201 \n",
            "Regression loss: 0.03084198199212551\n",
            "----------------\n",
            "Loss:  0.6651575528085232 \n",
            "Classification loss: 1.2333979606628418 \n",
            "Regression loss: 0.04845857247710228\n",
            "----------------\n",
            "Loss:  0.769819438457489 \n",
            "Classification loss: 1.4549490213394165 \n",
            "Regression loss: 0.04234492778778076\n",
            "----------------\n",
            "Loss:  0.9152515344321728 \n",
            "Classification loss: 1.7136286497116089 \n",
            "Regression loss: 0.05843720957636833\n",
            "----------------\n",
            "Loss:  0.8155205063521862 \n",
            "Classification loss: 1.5724198818206787 \n",
            "Regression loss: 0.029310565441846848\n",
            "----------------\n",
            "Loss:  0.8726521730422974 \n",
            "Classification loss: 1.604010820388794 \n",
            "Regression loss: 0.07064676284790039\n",
            "----------------\n",
            "Loss:  0.852309949696064 \n",
            "Classification loss: 1.5821895599365234 \n",
            "Regression loss: 0.06121516972780228\n",
            "----------------\n",
            "Loss:  0.8219751119613647 \n",
            "Classification loss: 1.481020450592041 \n",
            "Regression loss: 0.08146488666534424\n",
            "----------------\n",
            "Loss:  0.8318328261375427 \n",
            "Classification loss: 1.5200594663619995 \n",
            "Regression loss: 0.07180309295654297\n",
            "----------------\n",
            "Loss:  0.7764336057007313 \n",
            "Classification loss: 1.4770987033843994 \n",
            "Regression loss: 0.03788425400853157\n",
            "----------------\n",
            "Loss:  0.7594121359288692 \n",
            "Classification loss: 1.451000690460205 \n",
            "Regression loss: 0.03391179069876671\n",
            "----------------\n",
            "Loss:  0.6927105858922005 \n",
            "Classification loss: 1.3065125942230225 \n",
            "Regression loss: 0.03945428878068924\n",
            "----------------\n",
            "Loss:  0.7691688686609268 \n",
            "Classification loss: 1.4745599031448364 \n",
            "Regression loss: 0.031888917088508606\n",
            "----------------\n",
            "Loss:  0.8199281916022301 \n",
            "Classification loss: 1.5387732982635498 \n",
            "Regression loss: 0.05054154247045517\n",
            "----------------\n",
            "Loss:  0.8678957037627697 \n",
            "Classification loss: 1.618033528327942 \n",
            "Regression loss: 0.05887893959879875\n",
            "----------------\n",
            "Loss:  0.8335796371102333 \n",
            "Classification loss: 1.5478259325027466 \n",
            "Regression loss: 0.059666670858860016\n",
            "----------------\n",
            "Loss:  0.692792970687151 \n",
            "Classification loss: 1.2952840328216553 \n",
            "Regression loss: 0.04515095427632332\n",
            "----------------\n",
            "Loss:  0.835867278277874 \n",
            "Classification loss: 1.5587823390960693 \n",
            "Regression loss: 0.056476108729839325\n",
            "----------------\n",
            "Loss:  0.7887717597186565 \n",
            "Classification loss: 1.5066908597946167 \n",
            "Regression loss: 0.03542632982134819\n",
            "----------------\n",
            "Loss:  0.7256890088319778 \n",
            "Classification loss: 1.3838632106781006 \n",
            "Regression loss: 0.03375740349292755\n",
            "----------------\n",
            "Loss:  0.8386827632784843 \n",
            "Classification loss: 1.581911325454712 \n",
            "Regression loss: 0.04772710055112839\n",
            "----------------\n",
            "Loss:  0.704403679817915 \n",
            "Classification loss: 1.32508385181427 \n",
            "Regression loss: 0.04186175391077995\n",
            "----------------\n",
            "Loss:  0.8102284073829651 \n",
            "Classification loss: 1.4874687194824219 \n",
            "Regression loss: 0.06649404764175415\n",
            "----------------\n",
            "Loss:  0.873276486992836 \n",
            "Classification loss: 1.5984159708023071 \n",
            "Regression loss: 0.07406850159168243\n",
            "----------------\n",
            "Loss:  0.774848572909832 \n",
            "Classification loss: 1.436651587486267 \n",
            "Regression loss: 0.056522779166698456\n",
            "----------------\n",
            "Loss:  0.8431932181119919 \n",
            "Classification loss: 1.5365262031555176 \n",
            "Regression loss: 0.0749301165342331\n",
            "----------------\n",
            "Loss:  0.8490185588598251 \n",
            "Classification loss: 1.5721843242645264 \n",
            "Regression loss: 0.06292639672756195\n",
            "----------------\n",
            "Loss:  0.7802091687917709 \n",
            "Classification loss: 1.463733196258545 \n",
            "Regression loss: 0.048342570662498474\n",
            "----------------\n",
            "Loss:  0.818719282746315 \n",
            "Classification loss: 1.5211119651794434 \n",
            "Regression loss: 0.05816330015659332\n",
            "----------------\n",
            "Loss:  0.9605844467878342 \n",
            "Classification loss: 1.8497856855392456 \n",
            "Regression loss: 0.035691604018211365\n",
            "----------------\n",
            "Loss:  0.8646948486566544 \n",
            "Classification loss: 1.5868399143218994 \n",
            "Regression loss: 0.07127489149570465\n",
            "----------------\n",
            "Loss:  0.7587708011269569 \n",
            "Classification loss: 1.3897404670715332 \n",
            "Regression loss: 0.06390056759119034\n",
            "----------------\n",
            "Loss:  0.7141663208603859 \n",
            "Classification loss: 1.280807614326477 \n",
            "Regression loss: 0.07376251369714737\n",
            "----------------\n",
            "Loss:  0.7761652991175652 \n",
            "Classification loss: 1.43401038646698 \n",
            "Regression loss: 0.059160105884075165\n",
            "----------------\n",
            "Loss:  0.7927333414554596 \n",
            "Classification loss: 1.439612865447998 \n",
            "Regression loss: 0.07292690873146057\n",
            "----------------\n",
            "Loss:  0.7090059034526348 \n",
            "Classification loss: 1.3278142213821411 \n",
            "Regression loss: 0.045098792761564255\n",
            "----------------\n",
            "Loss:  0.7359615229070187 \n",
            "Classification loss: 1.3793710470199585 \n",
            "Regression loss: 0.04627599939703941\n",
            "----------------\n",
            "Loss:  0.8358180224895477 \n",
            "Classification loss: 1.5444434881210327 \n",
            "Regression loss: 0.06359627842903137\n",
            "----------------\n",
            "Loss:  0.8438713364303112 \n",
            "Classification loss: 1.5829613208770752 \n",
            "Regression loss: 0.052390675991773605\n",
            "----------------\n",
            "Loss:  0.6692801415920258 \n",
            "Classification loss: 1.2419029474258423 \n",
            "Regression loss: 0.048328667879104614\n",
            "----------------\n",
            "Loss:  0.7524721510708332 \n",
            "Classification loss: 1.4140105247497559 \n",
            "Regression loss: 0.045466888695955276\n",
            "----------------\n",
            "Loss:  0.8387243449687958 \n",
            "Classification loss: 1.5729565620422363 \n",
            "Regression loss: 0.05224606394767761\n",
            "----------------\n",
            "Loss:  0.7818912789225578 \n",
            "Classification loss: 1.4646687507629395 \n",
            "Regression loss: 0.049556903541088104\n",
            "----------------\n",
            "Loss:  0.7910753935575485 \n",
            "Classification loss: 1.461487054824829 \n",
            "Regression loss: 0.06033186614513397\n",
            "----------------\n",
            "Loss:  0.6683387700468302 \n",
            "Classification loss: 1.292683720588684 \n",
            "Regression loss: 0.021996909752488136\n",
            "----------------\n",
            "Loss:  0.8785261064767838 \n",
            "Classification loss: 1.6120572090148926 \n",
            "Regression loss: 0.07249750196933746\n",
            "----------------\n",
            "Loss:  0.7607316188514233 \n",
            "Classification loss: 1.4336292743682861 \n",
            "Regression loss: 0.0439169816672802\n",
            "----------------\n",
            "Loss:  0.8345464915037155 \n",
            "Classification loss: 1.5252219438552856 \n",
            "Regression loss: 0.07193551957607269\n",
            "----------------\n",
            "Loss:  0.6595484092831612 \n",
            "Classification loss: 1.2238667011260986 \n",
            "Regression loss: 0.04761505872011185\n",
            "----------------\n",
            "Loss:  0.7332649864256382 \n",
            "Classification loss: 1.3829944133758545 \n",
            "Regression loss: 0.04176777973771095\n",
            "----------------\n",
            "Loss:  0.8659725859761238 \n",
            "Classification loss: 1.6629518270492554 \n",
            "Regression loss: 0.034496672451496124\n",
            "----------------\n",
            "Loss:  0.8423309028148651 \n",
            "Classification loss: 1.5412161350250244 \n",
            "Regression loss: 0.0717228353023529\n",
            "----------------\n",
            "Loss:  0.826276071369648 \n",
            "Classification loss: 1.5148134231567383 \n",
            "Regression loss: 0.06886935979127884\n",
            "----------------\n",
            "Loss:  0.7285775691270828 \n",
            "Classification loss: 1.3621034622192383 \n",
            "Regression loss: 0.047525838017463684\n",
            "----------------\n",
            "Loss:  0.8220060430467129 \n",
            "Classification loss: 1.55709707736969 \n",
            "Regression loss: 0.043457504361867905\n",
            "----------------\n",
            "Loss:  0.6859090328216553 \n",
            "Classification loss: 1.2572078704833984 \n",
            "Regression loss: 0.057305097579956055\n",
            "----------------\n",
            "Loss:  0.6644638180732727 \n",
            "Classification loss: 1.2806555032730103 \n",
            "Regression loss: 0.024136066436767578\n",
            "----------------\n",
            "Loss:  0.8226391300559044 \n",
            "Classification loss: 1.53579843044281 \n",
            "Regression loss: 0.05473991483449936\n",
            "----------------\n",
            "Loss:  0.743235357105732 \n",
            "Classification loss: 1.3954551219940186 \n",
            "Regression loss: 0.04550779610872269\n",
            "----------------\n",
            "Loss:  0.7830921486020088 \n",
            "Classification loss: 1.4707063436508179 \n",
            "Regression loss: 0.047738976776599884\n",
            "----------------\n",
            "Loss:  0.8457467257976532 \n",
            "Classification loss: 1.5627460479736328 \n",
            "Regression loss: 0.06437370181083679\n",
            "----------------\n",
            "Loss:  0.7163410373032093 \n",
            "Classification loss: 1.3405323028564453 \n",
            "Regression loss: 0.04607488587498665\n",
            "----------------\n",
            "Loss:  0.7425723895430565 \n",
            "Classification loss: 1.39180326461792 \n",
            "Regression loss: 0.04667075723409653\n",
            "----------------\n",
            "Loss:  0.7178150825202465 \n",
            "Classification loss: 1.3290096521377563 \n",
            "Regression loss: 0.05331025645136833\n",
            "----------------\n",
            "Loss:  0.8477996475994587 \n",
            "Classification loss: 1.5799074172973633 \n",
            "Regression loss: 0.057845938950777054\n",
            "----------------\n",
            "Loss:  0.7873627543449402 \n",
            "Classification loss: 1.5031144618988037 \n",
            "Regression loss: 0.03580552339553833\n",
            "----------------\n",
            "Loss:  0.8906600773334503 \n",
            "Classification loss: 1.6344810724258423 \n",
            "Regression loss: 0.07341954112052917\n",
            "----------------\n",
            "Loss:  0.9203497171401978 \n",
            "Classification loss: 1.6525543928146362 \n",
            "Regression loss: 0.09407252073287964\n",
            "----------------\n",
            "Loss:  0.7258850298821926 \n",
            "Classification loss: 1.3399957418441772 \n",
            "Regression loss: 0.05588715896010399\n",
            "----------------\n",
            "Loss:  0.9017271474003792 \n",
            "Classification loss: 1.7244691848754883 \n",
            "Regression loss: 0.03949255496263504\n",
            "----------------\n",
            "Loss:  0.7441147230565548 \n",
            "Classification loss: 1.4101890325546265 \n",
            "Regression loss: 0.03902020677924156\n",
            "----------------\n",
            "Loss:  0.7805977314710617 \n",
            "Classification loss: 1.4308264255523682 \n",
            "Regression loss: 0.06518451869487762\n",
            "----------------\n",
            "Loss:  0.7763273045420647 \n",
            "Classification loss: 1.466655969619751 \n",
            "Regression loss: 0.04299931973218918\n",
            "----------------\n",
            "Loss:  0.7827945128083229 \n",
            "Classification loss: 1.4745615720748901 \n",
            "Regression loss: 0.04551372677087784\n",
            "----------------\n",
            "Loss:  0.7984101921319962 \n",
            "Classification loss: 1.4782168865203857 \n",
            "Regression loss: 0.059301748871803284\n",
            "----------------\n",
            "Loss:  0.8047196939587593 \n",
            "Classification loss: 1.5552358627319336 \n",
            "Regression loss: 0.02710176259279251\n",
            "----------------\n",
            "Loss:  0.8030127584934235 \n",
            "Classification loss: 1.4814056158065796 \n",
            "Regression loss: 0.06230995059013367\n",
            "----------------\n",
            "Loss:  0.7029943205416203 \n",
            "Classification loss: 1.3367544412612915 \n",
            "Regression loss: 0.0346170999109745\n",
            "----------------\n",
            "Loss:  0.781799390912056 \n",
            "Classification loss: 1.460031270980835 \n",
            "Regression loss: 0.05178375542163849\n",
            "----------------\n",
            "Loss:  0.7359293475747108 \n",
            "Classification loss: 1.4065383672714233 \n",
            "Regression loss: 0.032660163938999176\n",
            "----------------\n",
            "Loss:  0.8206662908196449 \n",
            "Classification loss: 1.489656686782837 \n",
            "Regression loss: 0.07583794742822647\n",
            "----------------\n",
            "Loss:  0.8622387573122978 \n",
            "Classification loss: 1.5818248987197876 \n",
            "Regression loss: 0.07132630795240402\n",
            "----------------\n",
            "Loss:  0.8046793341636658 \n",
            "Classification loss: 1.4817423820495605 \n",
            "Regression loss: 0.0638081431388855\n",
            "----------------\n",
            "Loss:  0.7701523266732693 \n",
            "Classification loss: 1.4165672063827515 \n",
            "Regression loss: 0.06186872348189354\n",
            "----------------\n",
            "Loss:  0.7998388037085533 \n",
            "Classification loss: 1.4779969453811646 \n",
            "Regression loss: 0.06084033101797104\n",
            "----------------\n",
            "Loss:  0.7845908924937248 \n",
            "Classification loss: 1.4635155200958252 \n",
            "Regression loss: 0.052833132445812225\n",
            "----------------\n",
            "Loss:  0.797824501991272 \n",
            "Classification loss: 1.4897840023040771 \n",
            "Regression loss: 0.0529325008392334\n",
            "----------------\n",
            "Loss:  0.7733636051416397 \n",
            "Classification loss: 1.439955711364746 \n",
            "Regression loss: 0.05338574945926666\n",
            "----------------\n",
            "Loss:  0.7020578943192959 \n",
            "Classification loss: 1.2891125679016113 \n",
            "Regression loss: 0.05750161036849022\n",
            "----------------\n",
            "Loss:  0.7488945350050926 \n",
            "Classification loss: 1.380361557006836 \n",
            "Regression loss: 0.05871375650167465\n",
            "----------------\n",
            "Loss:  0.7800365686416626 \n",
            "Classification loss: 1.411149501800537 \n",
            "Regression loss: 0.07446181774139404\n",
            "----------------\n",
            "Loss:  0.7426446154713631 \n",
            "Classification loss: 1.40379798412323 \n",
            "Regression loss: 0.04074562340974808\n",
            "----------------\n",
            "Loss:  0.7897766977548599 \n",
            "Classification loss: 1.444838523864746 \n",
            "Regression loss: 0.06735743582248688\n",
            "----------------\n",
            "Loss:  0.856229156255722 \n",
            "Classification loss: 1.5776760578155518 \n",
            "Regression loss: 0.06739112734794617\n",
            "----------------\n",
            "Loss:  0.7527763023972511 \n",
            "Classification loss: 1.4189825057983398 \n",
            "Regression loss: 0.04328504949808121\n",
            "----------------\n",
            "Loss:  0.7738312818109989 \n",
            "Classification loss: 1.4861054420471191 \n",
            "Regression loss: 0.030778560787439346\n",
            "----------------\n",
            "Loss:  0.7618377655744553 \n",
            "Classification loss: 1.3736512660980225 \n",
            "Regression loss: 0.07501213252544403\n",
            "----------------\n",
            "Loss:  0.807313583791256 \n",
            "Classification loss: 1.5208302736282349 \n",
            "Regression loss: 0.04689844697713852\n",
            "----------------\n",
            "Loss:  0.74636035785079 \n",
            "Classification loss: 1.4215055704116821 \n",
            "Regression loss: 0.03560757264494896\n",
            "----------------\n",
            "Loss:  0.6636850908398628 \n",
            "Classification loss: 1.2475372552871704 \n",
            "Regression loss: 0.03991646319627762\n",
            "----------------\n",
            "Loss:  0.7448526062071323 \n",
            "Classification loss: 1.3879704475402832 \n",
            "Regression loss: 0.05086738243699074\n",
            "----------------\n",
            "Loss:  0.7356786280870438 \n",
            "Classification loss: 1.4044917821884155 \n",
            "Regression loss: 0.033432736992836\n",
            "----------------\n",
            "Loss:  0.7946686632931232 \n",
            "Classification loss: 1.4643621444702148 \n",
            "Regression loss: 0.06248759105801582\n",
            "----------------\n",
            "Loss:  0.7681621536612511 \n",
            "Classification loss: 1.4558460712432861 \n",
            "Regression loss: 0.040239118039608\n",
            "----------------\n",
            "Loss:  0.7490420788526535 \n",
            "Classification loss: 1.4015690088272095 \n",
            "Regression loss: 0.04825757443904877\n",
            "----------------\n",
            "Loss:  0.7126144543290138 \n",
            "Classification loss: 1.374708890914917 \n",
            "Regression loss: 0.02526000887155533\n",
            "----------------\n",
            "Loss:  0.8467620275914669 \n",
            "Classification loss: 1.5775067806243896 \n",
            "Regression loss: 0.05800863727927208\n",
            "----------------\n",
            "Loss:  0.7629187144339085 \n",
            "Classification loss: 1.439266324043274 \n",
            "Regression loss: 0.0432855524122715\n",
            "----------------\n",
            "Loss:  0.7692627608776093 \n",
            "Classification loss: 1.4277279376983643 \n",
            "Regression loss: 0.055398792028427124\n",
            "----------------\n",
            "Loss:  0.7522231824696064 \n",
            "Classification loss: 1.382308006286621 \n",
            "Regression loss: 0.06106917932629585\n",
            "----------------\n",
            "Loss:  0.7938909120857716 \n",
            "Classification loss: 1.5031849145889282 \n",
            "Regression loss: 0.04229845479130745\n",
            "----------------\n",
            "Loss:  0.874919880181551 \n",
            "Classification loss: 1.6313806772232056 \n",
            "Regression loss: 0.059229541569948196\n",
            "----------------\n",
            "Loss:  0.7166999615728855 \n",
            "Classification loss: 1.3516335487365723 \n",
            "Regression loss: 0.04088318720459938\n",
            "----------------\n",
            "Loss:  0.7952028587460518 \n",
            "Classification loss: 1.4578226804733276 \n",
            "Regression loss: 0.06629151850938797\n",
            "----------------\n",
            "Loss:  0.8165649808943272 \n",
            "Classification loss: 1.5303552150726318 \n",
            "Regression loss: 0.051387373358011246\n",
            "----------------\n",
            "Loss:  0.7866742014884949 \n",
            "Classification loss: 1.5056836605072021 \n",
            "Regression loss: 0.0338323712348938\n",
            "----------------\n",
            "Loss:  0.7069777622818947 \n",
            "Classification loss: 1.3598824739456177 \n",
            "Regression loss: 0.027036525309085846\n",
            "----------------\n",
            "Loss:  0.8654636144638062 \n",
            "Classification loss: 1.6189876794815063 \n",
            "Regression loss: 0.05596977472305298\n",
            "----------------\n",
            "Loss:  0.7107595019042492 \n",
            "Classification loss: 1.3470288515090942 \n",
            "Regression loss: 0.03724507614970207\n",
            "----------------\n",
            "Loss:  0.8607031963765621 \n",
            "Classification loss: 1.5970869064331055 \n",
            "Regression loss: 0.062159743160009384\n",
            "----------------\n",
            "Loss:  0.7892891094088554 \n",
            "Classification loss: 1.4710984230041504 \n",
            "Regression loss: 0.05373989790678024\n",
            "----------------\n",
            "Loss:  0.810426265001297 \n",
            "Classification loss: 1.522918462753296 \n",
            "Regression loss: 0.04896703362464905\n",
            "----------------\n",
            "Loss:  0.7129822969436646 \n",
            "Classification loss: 1.3462034463882446 \n",
            "Regression loss: 0.039880573749542236\n",
            "----------------\n",
            "Loss:  0.8106630370020866 \n",
            "Classification loss: 1.5212316513061523 \n",
            "Regression loss: 0.05004721134901047\n",
            "----------------\n",
            "Loss:  0.7696898430585861 \n",
            "Classification loss: 1.4261292219161987 \n",
            "Regression loss: 0.056625232100486755\n",
            "----------------\n",
            "Loss:  0.7762018665671349 \n",
            "Classification loss: 1.446252703666687 \n",
            "Regression loss: 0.05307551473379135\n",
            "----------------\n",
            "Loss:  0.7356278598308563 \n",
            "Classification loss: 1.4051177501678467 \n",
            "Regression loss: 0.03306898474693298\n",
            "----------------\n",
            "Loss:  0.709942415356636 \n",
            "Classification loss: 1.29695463180542 \n",
            "Regression loss: 0.061465099453926086\n",
            "----------------\n",
            "Loss:  0.7564761228859425 \n",
            "Classification loss: 1.4107763767242432 \n",
            "Regression loss: 0.05108793452382088\n",
            "----------------\n",
            "Loss:  0.6808218397200108 \n",
            "Classification loss: 1.2667021751403809 \n",
            "Regression loss: 0.04747075214982033\n",
            "----------------\n",
            "Loss:  0.8832198083400726 \n",
            "Classification loss: 1.654923439025879 \n",
            "Regression loss: 0.05575808882713318\n",
            "----------------\n",
            "Loss:  0.727722529321909 \n",
            "Classification loss: 1.338778018951416 \n",
            "Regression loss: 0.05833351984620094\n",
            "----------------\n",
            "Loss:  0.8656727448105812 \n",
            "Classification loss: 1.6305301189422607 \n",
            "Regression loss: 0.050407685339450836\n",
            "----------------\n",
            "Loss:  0.7528040446341038 \n",
            "Classification loss: 1.4072608947753906 \n",
            "Regression loss: 0.04917359724640846\n",
            "----------------\n",
            "Loss:  0.7476500831544399 \n",
            "Classification loss: 1.4481232166290283 \n",
            "Regression loss: 0.023588474839925766\n",
            "----------------\n",
            "Loss:  0.7094225324690342 \n",
            "Classification loss: 1.3058127164840698 \n",
            "Regression loss: 0.05651617422699928\n",
            "----------------\n",
            "Loss:  0.7573984824120998 \n",
            "Classification loss: 1.4334628582000732 \n",
            "Regression loss: 0.04066705331206322\n",
            "----------------\n",
            "Loss:  0.80169677734375 \n",
            "Classification loss: 1.426641821861267 \n",
            "Regression loss: 0.08837586641311646\n",
            "----------------\n",
            "Loss:  0.6826518625020981 \n",
            "Classification loss: 1.240525484085083 \n",
            "Regression loss: 0.06238912045955658\n",
            "----------------\n",
            "Loss:  0.7839740365743637 \n",
            "Classification loss: 1.4222874641418457 \n",
            "Regression loss: 0.07283030450344086\n",
            "----------------\n",
            "Loss:  0.8063841760158539 \n",
            "Classification loss: 1.5163191556930542 \n",
            "Regression loss: 0.04822459816932678\n",
            "----------------\n",
            "Loss:  0.8084986731410027 \n",
            "Classification loss: 1.4752388000488281 \n",
            "Regression loss: 0.07087927311658859\n",
            "----------------\n",
            "Loss:  0.755417313426733 \n",
            "Classification loss: 1.4505972862243652 \n",
            "Regression loss: 0.0301186703145504\n",
            "----------------\n",
            "Loss:  0.748941320925951 \n",
            "Classification loss: 1.3940905332565308 \n",
            "Regression loss: 0.05189605429768562\n",
            "----------------\n",
            "Loss:  0.7339848875999451 \n",
            "Classification loss: 1.3807625770568848 \n",
            "Regression loss: 0.043603599071502686\n",
            "----------------\n",
            "Loss:  0.8052465058863163 \n",
            "Classification loss: 1.5204744338989258 \n",
            "Regression loss: 0.04500928893685341\n",
            "----------------\n",
            "Loss:  0.8191748075187206 \n",
            "Classification loss: 1.529302716255188 \n",
            "Regression loss: 0.05452344939112663\n",
            "----------------\n",
            "Loss:  0.7989237792789936 \n",
            "Classification loss: 1.502305030822754 \n",
            "Regression loss: 0.04777126386761665\n",
            "----------------\n",
            "Loss:  0.8992012217640877 \n",
            "Classification loss: 1.6844578981399536 \n",
            "Regression loss: 0.05697227269411087\n",
            "----------------\n",
            "Loss:  0.9402612112462521 \n",
            "Classification loss: 1.7673611640930176 \n",
            "Regression loss: 0.05658062919974327\n",
            "----------------\n",
            "Loss:  0.8052777498960495 \n",
            "Classification loss: 1.5046756267547607 \n",
            "Regression loss: 0.05293993651866913\n",
            "----------------\n",
            "Loss:  0.7826874405145645 \n",
            "Classification loss: 1.4278297424316406 \n",
            "Regression loss: 0.0687725692987442\n",
            "----------------\n",
            "Loss:  0.7852671425789595 \n",
            "Classification loss: 1.5277389287948608 \n",
            "Regression loss: 0.021397678181529045\n",
            "----------------\n",
            "Loss:  0.750756286084652 \n",
            "Classification loss: 1.4048418998718262 \n",
            "Regression loss: 0.04833533614873886\n",
            "----------------\n",
            "Loss:  0.7563068568706512 \n",
            "Classification loss: 1.4366843700408936 \n",
            "Regression loss: 0.03796467185020447\n",
            "----------------\n",
            "Loss:  0.8084219172596931 \n",
            "Classification loss: 1.5514371395111084 \n",
            "Regression loss: 0.03270334750413895\n",
            "----------------\n",
            "Loss:  0.8420426994562149 \n",
            "Classification loss: 1.6074395179748535 \n",
            "Regression loss: 0.03832294046878815\n",
            "----------------\n",
            "Loss:  0.6946375146508217 \n",
            "Classification loss: 1.269152283668518 \n",
            "Regression loss: 0.06006137281656265\n",
            "----------------\n",
            "Loss:  0.7445175498723984 \n",
            "Classification loss: 1.4199155569076538 \n",
            "Regression loss: 0.03455977141857147\n",
            "----------------\n",
            "Loss:  0.7434248588979244 \n",
            "Classification loss: 1.375709056854248 \n",
            "Regression loss: 0.0555703304708004\n",
            "----------------\n",
            "Loss:  0.7613091729581356 \n",
            "Classification loss: 1.4466229677200317 \n",
            "Regression loss: 0.037997689098119736\n",
            "----------------\n",
            "Loss:  0.7104366607964039 \n",
            "Classification loss: 1.3281562328338623 \n",
            "Regression loss: 0.04635854437947273\n",
            "----------------\n",
            "Loss:  0.7511672824621201 \n",
            "Classification loss: 1.392021656036377 \n",
            "Regression loss: 0.05515645444393158\n",
            "----------------\n",
            "Loss:  0.7382254116237164 \n",
            "Classification loss: 1.3874391317367554 \n",
            "Regression loss: 0.04450584575533867\n",
            "----------------\n",
            "Loss:  0.6820150762796402 \n",
            "Classification loss: 1.2568646669387817 \n",
            "Regression loss: 0.05358274281024933\n",
            "----------------\n",
            "Loss:  0.6377149242907763 \n",
            "Classification loss: 1.2351781129837036 \n",
            "Regression loss: 0.020125867798924446\n",
            "----------------\n",
            "Loss:  0.7418972104787827 \n",
            "Classification loss: 1.3532778024673462 \n",
            "Regression loss: 0.06525830924510956\n",
            "----------------\n",
            "Loss:  0.7291416898369789 \n",
            "Classification loss: 1.344797968864441 \n",
            "Regression loss: 0.05674270540475845\n",
            "----------------\n",
            "Loss:  0.8468699119985104 \n",
            "Classification loss: 1.5708469152450562 \n",
            "Regression loss: 0.061446454375982285\n",
            "----------------\n",
            "Loss:  0.7232148572802544 \n",
            "Classification loss: 1.3618069887161255 \n",
            "Regression loss: 0.04231136292219162\n",
            "----------------\n",
            "Loss:  0.7787926457822323 \n",
            "Classification loss: 1.4623100757598877 \n",
            "Regression loss: 0.04763760790228844\n",
            "----------------\n",
            "Loss:  0.9064747802913189 \n",
            "Classification loss: 1.720632553100586 \n",
            "Regression loss: 0.046158503741025925\n",
            "----------------\n",
            "Loss:  0.8428081572055817 \n",
            "Classification loss: 1.620050072669983 \n",
            "Regression loss: 0.03278312087059021\n",
            "----------------\n",
            "Loss:  0.7448610812425613 \n",
            "Classification loss: 1.3739955425262451 \n",
            "Regression loss: 0.05786330997943878\n",
            "----------------\n",
            "Loss:  0.7065622955560684 \n",
            "Classification loss: 1.3056646585464478 \n",
            "Regression loss: 0.05372996628284454\n",
            "----------------\n",
            "Loss:  0.7733415812253952 \n",
            "Classification loss: 1.4632961750030518 \n",
            "Regression loss: 0.041693493723869324\n",
            "----------------\n",
            "Loss:  0.7264650985598564 \n",
            "Classification loss: 1.361997127532959 \n",
            "Regression loss: 0.04546653479337692\n",
            "----------------\n",
            "Loss:  0.8046458587050438 \n",
            "Classification loss: 1.5556516647338867 \n",
            "Regression loss: 0.026820026338100433\n",
            "----------------\n",
            "Loss:  0.7667228393256664 \n",
            "Classification loss: 1.4249467849731445 \n",
            "Regression loss: 0.05424944683909416\n",
            "----------------\n",
            "Loss:  0.7952748239040375 \n",
            "Classification loss: 1.4737694263458252 \n",
            "Regression loss: 0.05839011073112488\n",
            "----------------\n",
            "Loss:  0.811607088893652 \n",
            "Classification loss: 1.543242335319519 \n",
            "Regression loss: 0.03998592123389244\n",
            "----------------\n",
            "Loss:  0.7889687307178974 \n",
            "Classification loss: 1.4822605848312378 \n",
            "Regression loss: 0.04783843830227852\n",
            "----------------\n",
            "Loss:  0.7673612833023071 \n",
            "Classification loss: 1.3955761194229126 \n",
            "Regression loss: 0.06957322359085083\n",
            "----------------\n",
            "Loss:  0.7614537216722965 \n",
            "Classification loss: 1.4172906875610352 \n",
            "Regression loss: 0.052808377891778946\n",
            "----------------\n",
            "Loss:  0.7164876535534859 \n",
            "Classification loss: 1.377312421798706 \n",
            "Regression loss: 0.027831442654132843\n",
            "----------------\n",
            "Loss:  0.8934652954339981 \n",
            "Classification loss: 1.636621356010437 \n",
            "Regression loss: 0.0751546174287796\n",
            "----------------\n",
            "Loss:  0.7208515778183937 \n",
            "Classification loss: 1.332031488418579 \n",
            "Regression loss: 0.054835833609104156\n",
            "----------------\n",
            "Loss:  0.8155113533139229 \n",
            "Classification loss: 1.5094528198242188 \n",
            "Regression loss: 0.06078494340181351\n",
            "----------------\n",
            "Loss:  0.8067899122834206 \n",
            "Classification loss: 1.4919288158416748 \n",
            "Regression loss: 0.06082550436258316\n",
            "----------------\n",
            "Loss:  0.6539008617401123 \n",
            "Classification loss: 1.2458300590515137 \n",
            "Regression loss: 0.03098583221435547\n",
            "----------------\n",
            "Loss:  0.8006806969642639 \n",
            "Classification loss: 1.4577817916870117 \n",
            "Regression loss: 0.07178980112075806\n",
            "----------------\n",
            "Loss:  0.7912319600582123 \n",
            "Classification loss: 1.4589041471481323 \n",
            "Regression loss: 0.06177988648414612\n",
            "----------------\n",
            "Loss:  0.630312766879797 \n",
            "Classification loss: 1.1443989276885986 \n",
            "Regression loss: 0.058113303035497665\n",
            "----------------\n",
            "Loss:  0.7764532789587975 \n",
            "Classification loss: 1.4399144649505615 \n",
            "Regression loss: 0.05649604648351669\n",
            "----------------\n",
            "Loss:  0.7366541549563408 \n",
            "Classification loss: 1.4043500423431396 \n",
            "Regression loss: 0.034479133784770966\n",
            "----------------\n",
            "Loss:  0.66490213945508 \n",
            "Classification loss: 1.2458226680755615 \n",
            "Regression loss: 0.04199080541729927\n",
            "----------------\n",
            "Loss:  0.7141370605677366 \n",
            "Classification loss: 1.375414490699768 \n",
            "Regression loss: 0.026429815217852592\n",
            "----------------\n",
            "Loss:  0.7984955161809921 \n",
            "Classification loss: 1.4527947902679443 \n",
            "Regression loss: 0.07209812104701996\n",
            "----------------\n",
            "Loss:  0.8693800158798695 \n",
            "Classification loss: 1.640798807144165 \n",
            "Regression loss: 0.04898061230778694\n",
            "----------------\n",
            "Loss:  0.8703304976224899 \n",
            "Classification loss: 1.565661907196045 \n",
            "Regression loss: 0.08749954402446747\n",
            "----------------\n",
            "Loss:  0.8161978796124458 \n",
            "Classification loss: 1.5224721431732178 \n",
            "Regression loss: 0.054961808025836945\n",
            "----------------\n",
            "Loss:  0.8179962411522865 \n",
            "Classification loss: 1.5339043140411377 \n",
            "Regression loss: 0.05104408413171768\n",
            "----------------\n",
            "Loss:  0.8412082828581333 \n",
            "Classification loss: 1.5908399820327759 \n",
            "Regression loss: 0.04578829184174538\n",
            "----------------\n",
            "Loss:  0.773076519370079 \n",
            "Classification loss: 1.4691388607025146 \n",
            "Regression loss: 0.038507089018821716\n",
            "----------------\n",
            "Loss:  0.8315247595310211 \n",
            "Classification loss: 1.6065950393676758 \n",
            "Regression loss: 0.028227239847183228\n",
            "----------------\n",
            "Loss:  0.7808407060801983 \n",
            "Classification loss: 1.467619776725769 \n",
            "Regression loss: 0.047030817717313766\n",
            "----------------\n",
            "Loss:  0.7711686939001083 \n",
            "Classification loss: 1.4081292152404785 \n",
            "Regression loss: 0.06710408627986908\n",
            "----------------\n",
            "Loss:  0.7096125110983849 \n",
            "Classification loss: 1.3503440618515015 \n",
            "Regression loss: 0.034440480172634125\n",
            "----------------\n",
            "Loss:  0.8982168585062027 \n",
            "Classification loss: 1.6674672365188599 \n",
            "Regression loss: 0.06448324024677277\n",
            "----------------\n",
            "Loss:  0.7199862897396088 \n",
            "Classification loss: 1.3264521360397339 \n",
            "Regression loss: 0.05676022171974182\n",
            "----------------\n",
            "Loss:  0.8095236457884312 \n",
            "Classification loss: 1.5552897453308105 \n",
            "Regression loss: 0.031878773123025894\n",
            "----------------\n",
            "Loss:  0.7380510047078133 \n",
            "Classification loss: 1.3817191123962402 \n",
            "Regression loss: 0.047191448509693146\n",
            "----------------\n",
            "Loss:  0.7926561944186687 \n",
            "Classification loss: 1.491536021232605 \n",
            "Regression loss: 0.04688818380236626\n",
            "----------------\n",
            "Loss:  0.7257945388555527 \n",
            "Classification loss: 1.3449498414993286 \n",
            "Regression loss: 0.05331961810588837\n",
            "----------------\n",
            "Loss:  0.9144179597496986 \n",
            "Classification loss: 1.707300066947937 \n",
            "Regression loss: 0.06076792627573013\n",
            "----------------\n",
            "Loss:  0.79184540361166 \n",
            "Classification loss: 1.4869145154953003 \n",
            "Regression loss: 0.04838814586400986\n",
            "----------------\n",
            "Loss:  0.745595283806324 \n",
            "Classification loss: 1.4004154205322266 \n",
            "Regression loss: 0.045387573540210724\n",
            "----------------\n",
            "Loss:  0.8449020534753799 \n",
            "Classification loss: 1.5751880407333374 \n",
            "Regression loss: 0.05730803310871124\n",
            "----------------\n",
            "Loss:  0.7780711315572262 \n",
            "Classification loss: 1.4700673818588257 \n",
            "Regression loss: 0.04303744062781334\n",
            "----------------\n",
            "Loss:  0.7678763940930367 \n",
            "Classification loss: 1.4569519758224487 \n",
            "Regression loss: 0.039400406181812286\n",
            "----------------\n",
            "Loss:  0.7869174480438232 \n",
            "Classification loss: 1.446566104888916 \n",
            "Regression loss: 0.06363439559936523\n",
            "----------------\n",
            "Loss:  0.8469442576169968 \n",
            "Classification loss: 1.5491046905517578 \n",
            "Regression loss: 0.07239191234111786\n",
            "----------------\n",
            "Loss:  0.6317618303000927 \n",
            "Classification loss: 1.1811048984527588 \n",
            "Regression loss: 0.0412093810737133\n",
            "----------------\n",
            "Loss:  0.7804800048470497 \n",
            "Classification loss: 1.3974686861038208 \n",
            "Regression loss: 0.08174566179513931\n",
            "----------------\n",
            "Loss:  0.8969738855957985 \n",
            "Classification loss: 1.6898735761642456 \n",
            "Regression loss: 0.05203709751367569\n",
            "----------------\n",
            "Loss:  0.7795072346925735 \n",
            "Classification loss: 1.4213752746582031 \n",
            "Regression loss: 0.06881959736347198\n",
            "----------------\n",
            "Loss:  0.7649559639394283 \n",
            "Classification loss: 1.4536716938018799 \n",
            "Regression loss: 0.03812011703848839\n",
            "----------------\n",
            "Loss:  0.6974046677350998 \n",
            "Classification loss: 1.2809630632400513 \n",
            "Regression loss: 0.05692313611507416\n",
            "----------------\n",
            "Loss:  0.7189318872988224 \n",
            "Classification loss: 1.3336554765701294 \n",
            "Regression loss: 0.052104149013757706\n",
            "----------------\n",
            "Loss:  0.7824929654598236 \n",
            "Classification loss: 1.458308458328247 \n",
            "Regression loss: 0.05333873629570007\n",
            "----------------\n",
            "Loss:  0.7061695903539658 \n",
            "Classification loss: 1.3000688552856445 \n",
            "Regression loss: 0.056135162711143494\n",
            "----------------\n",
            "Loss:  0.7010473236441612 \n",
            "Classification loss: 1.293633222579956 \n",
            "Regression loss: 0.0542307123541832\n",
            "----------------\n",
            "Loss:  0.7306203991174698 \n",
            "Classification loss: 1.366523027420044 \n",
            "Regression loss: 0.047358885407447815\n",
            "----------------\n",
            "Loss:  0.8290416710078716 \n",
            "Classification loss: 1.5760499238967896 \n",
            "Regression loss: 0.04101670905947685\n",
            "----------------\n",
            "Loss:  0.7953626736998558 \n",
            "Classification loss: 1.5234417915344238 \n",
            "Regression loss: 0.03364177793264389\n",
            "----------------\n",
            "Loss:  0.6695871613919735 \n",
            "Classification loss: 1.23707115650177 \n",
            "Regression loss: 0.051051583141088486\n",
            "----------------\n",
            "Loss:  0.8044220358133316 \n",
            "Classification loss: 1.481398344039917 \n",
            "Regression loss: 0.06372286379337311\n",
            "----------------\n",
            "Loss:  0.716770451515913 \n",
            "Classification loss: 1.32626473903656 \n",
            "Regression loss: 0.05363808199763298\n",
            "----------------\n",
            "Loss:  0.8177725821733475 \n",
            "Classification loss: 1.5110164880752563 \n",
            "Regression loss: 0.0622643381357193\n",
            "----------------\n",
            "Loss:  0.8182842470705509 \n",
            "Classification loss: 1.5282390117645264 \n",
            "Regression loss: 0.054164741188287735\n",
            "----------------\n",
            "Loss:  0.738163024187088 \n",
            "Classification loss: 1.419970989227295 \n",
            "Regression loss: 0.028177529573440552\n",
            "----------------\n",
            "Loss:  0.6482256948947906 \n",
            "Classification loss: 1.1900217533111572 \n",
            "Regression loss: 0.053214818239212036\n",
            "----------------\n",
            "Loss:  0.7959346771240234 \n",
            "Classification loss: 1.5244410037994385 \n",
            "Regression loss: 0.0337141752243042\n",
            "----------------\n",
            "Loss:  0.826524905860424 \n",
            "Classification loss: 1.5494859218597412 \n",
            "Regression loss: 0.051781944930553436\n",
            "----------------\n",
            "Loss:  0.8648772947490215 \n",
            "Classification loss: 1.6217076778411865 \n",
            "Regression loss: 0.05402345582842827\n",
            "----------------\n",
            "Loss:  0.6756299883127213 \n",
            "Classification loss: 1.253253698348999 \n",
            "Regression loss: 0.04900313913822174\n",
            "----------------\n",
            "Loss:  0.8018132746219635 \n",
            "Classification loss: 1.494625449180603 \n",
            "Regression loss: 0.05450055003166199\n",
            "----------------\n",
            "Loss:  0.736636083573103 \n",
            "Classification loss: 1.402889609336853 \n",
            "Regression loss: 0.03519127890467644\n",
            "----------------\n",
            "Loss:  0.7590021193027496 \n",
            "Classification loss: 1.397855520248413 \n",
            "Regression loss: 0.06007435917854309\n",
            "----------------\n",
            "Loss:  0.8116879463195801 \n",
            "Classification loss: 1.4817256927490234 \n",
            "Regression loss: 0.07082509994506836\n",
            "----------------\n",
            "Loss:  0.7715507857501507 \n",
            "Classification loss: 1.419329285621643 \n",
            "Regression loss: 0.06188614293932915\n",
            "----------------\n",
            "Loss:  0.7491790950298309 \n",
            "Classification loss: 1.367766261100769 \n",
            "Regression loss: 0.06529596447944641\n",
            "----------------\n",
            "Loss:  0.779588133096695 \n",
            "Classification loss: 1.43556809425354 \n",
            "Regression loss: 0.06180408596992493\n",
            "----------------\n",
            "Loss:  0.8293746411800385 \n",
            "Classification loss: 1.588973879814148 \n",
            "Regression loss: 0.03488770127296448\n",
            "----------------\n",
            "Loss:  0.7613370195031166 \n",
            "Classification loss: 1.4073352813720703 \n",
            "Regression loss: 0.05766937881708145\n",
            "----------------\n",
            "Loss:  0.7500329352915287 \n",
            "Classification loss: 1.416802167892456 \n",
            "Regression loss: 0.041631851345300674\n",
            "----------------\n",
            "Loss:  0.7264072895050049 \n",
            "Classification loss: 1.3696467876434326 \n",
            "Regression loss: 0.041583895683288574\n",
            "----------------\n",
            "Loss:  0.7197681330144405 \n",
            "Classification loss: 1.3354419469833374 \n",
            "Regression loss: 0.052047159522771835\n",
            "----------------\n",
            "Loss:  0.8284140676259995 \n",
            "Classification loss: 1.5610371828079224 \n",
            "Regression loss: 0.04789547622203827\n",
            "----------------\n",
            "Loss:  0.8349674418568611 \n",
            "Classification loss: 1.601770281791687 \n",
            "Regression loss: 0.03408230096101761\n",
            "----------------\n",
            "Loss:  0.7005326710641384 \n",
            "Classification loss: 1.3430595397949219 \n",
            "Regression loss: 0.029002901166677475\n",
            "----------------\n",
            "Loss:  0.806281466037035 \n",
            "Classification loss: 1.501465082168579 \n",
            "Regression loss: 0.05554892495274544\n",
            "----------------\n",
            "Loss:  0.7696865610778332 \n",
            "Classification loss: 1.441821813583374 \n",
            "Regression loss: 0.048775654286146164\n",
            "----------------\n",
            "Loss:  0.8703905157744884 \n",
            "Classification loss: 1.6619254350662231 \n",
            "Regression loss: 0.03942779824137688\n",
            "----------------\n",
            "Loss:  0.7212793305516243 \n",
            "Classification loss: 1.3342647552490234 \n",
            "Regression loss: 0.05414695292711258\n",
            "----------------\n",
            "Loss:  0.7960511818528175 \n",
            "Classification loss: 1.4724092483520508 \n",
            "Regression loss: 0.059846557676792145\n",
            "----------------\n",
            "Loss:  0.7821325957775116 \n",
            "Classification loss: 1.43233060836792 \n",
            "Regression loss: 0.06596729159355164\n",
            "----------------\n",
            "Loss:  0.8307619169354439 \n",
            "Classification loss: 1.581053614616394 \n",
            "Regression loss: 0.04023510962724686\n",
            "----------------\n",
            "Loss:  0.7624206654727459 \n",
            "Classification loss: 1.4370174407958984 \n",
            "Regression loss: 0.04391194507479668\n",
            "----------------\n",
            "Loss:  0.8240447491407394 \n",
            "Classification loss: 1.5545275211334229 \n",
            "Regression loss: 0.046780988574028015\n",
            "----------------\n",
            "Loss:  0.8092486038804054 \n",
            "Classification loss: 1.4815759658813477 \n",
            "Regression loss: 0.0684606209397316\n",
            "----------------\n",
            "Loss:  0.7927037328481674 \n",
            "Classification loss: 1.4712464809417725 \n",
            "Regression loss: 0.05708049237728119\n",
            "----------------\n",
            "Loss:  0.7998107858002186 \n",
            "Classification loss: 1.5015203952789307 \n",
            "Regression loss: 0.04905058816075325\n",
            "----------------\n",
            "Loss:  0.7464961260557175 \n",
            "Classification loss: 1.4030966758728027 \n",
            "Regression loss: 0.0449477881193161\n",
            "----------------\n",
            "Loss:  0.7376965172588825 \n",
            "Classification loss: 1.391026258468628 \n",
            "Regression loss: 0.04218338802456856\n",
            "----------------\n",
            "Loss:  0.7585804834961891 \n",
            "Classification loss: 1.4001312255859375 \n",
            "Regression loss: 0.05851487070322037\n",
            "----------------\n",
            "Loss:  0.6463476568460464 \n",
            "Classification loss: 1.2078931331634521 \n",
            "Regression loss: 0.042401090264320374\n",
            "----------------\n",
            "Loss:  0.8510008044540882 \n",
            "Classification loss: 1.5791549682617188 \n",
            "Regression loss: 0.061423320323228836\n",
            "----------------\n",
            "Loss:  0.7728365808725357 \n",
            "Classification loss: 1.453680157661438 \n",
            "Regression loss: 0.04599650204181671\n",
            "----------------\n",
            "Loss:  0.6816674657166004 \n",
            "Classification loss: 1.2965987920761108 \n",
            "Regression loss: 0.033368069678545\n",
            "----------------\n",
            "Loss:  0.8067169114947319 \n",
            "Classification loss: 1.4981629848480225 \n",
            "Regression loss: 0.05763541907072067\n",
            "----------------\n",
            "Loss:  0.6897837221622467 \n",
            "Classification loss: 1.3319873809814453 \n",
            "Regression loss: 0.023790031671524048\n",
            "----------------\n",
            "Loss:  0.7499705404043198 \n",
            "Classification loss: 1.4265170097351074 \n",
            "Regression loss: 0.03671203553676605\n",
            "----------------\n",
            "Loss:  0.824483472853899 \n",
            "Classification loss: 1.590412974357605 \n",
            "Regression loss: 0.029276985675096512\n",
            "----------------\n",
            "Loss:  0.7687949240207672 \n",
            "Classification loss: 1.4384220838546753 \n",
            "Regression loss: 0.049583882093429565\n",
            "----------------\n",
            "Loss:  0.8174428455531597 \n",
            "Classification loss: 1.5229685306549072 \n",
            "Regression loss: 0.0559585802257061\n",
            "----------------\n",
            "Loss:  0.7516401894390583 \n",
            "Classification loss: 1.415799617767334 \n",
            "Regression loss: 0.04374038055539131\n",
            "----------------\n",
            "Loss:  0.7669187486171722 \n",
            "Classification loss: 1.3908865451812744 \n",
            "Regression loss: 0.07147547602653503\n",
            "----------------\n",
            "Loss:  0.8137670941650867 \n",
            "Classification loss: 1.5514287948608398 \n",
            "Regression loss: 0.038052696734666824\n",
            "----------------\n",
            "Loss:  0.8012771680951118 \n",
            "Classification loss: 1.4984793663024902 \n",
            "Regression loss: 0.05203748494386673\n",
            "----------------\n",
            "Loss:  0.710663516074419 \n",
            "Classification loss: 1.3115830421447754 \n",
            "Regression loss: 0.054871995002031326\n",
            "----------------\n",
            "Loss:  0.7524239346385002 \n",
            "Classification loss: 1.4345605373382568 \n",
            "Regression loss: 0.035143665969371796\n",
            "----------------\n",
            "Loss:  0.822514995932579 \n",
            "Classification loss: 1.5168101787567139 \n",
            "Regression loss: 0.0641099065542221\n",
            "----------------\n",
            "Loss:  0.7854618653655052 \n",
            "Classification loss: 1.5336834192276 \n",
            "Regression loss: 0.01862015575170517\n",
            "----------------\n",
            "Loss:  0.7450821325182915 \n",
            "Classification loss: 1.41617751121521 \n",
            "Regression loss: 0.03699337691068649\n",
            "----------------\n",
            "Loss:  0.6763642244040966 \n",
            "Classification loss: 1.2305949926376343 \n",
            "Regression loss: 0.061066728085279465\n",
            "----------------\n",
            "Loss:  0.7399823032319546 \n",
            "Classification loss: 1.3687834739685059 \n",
            "Regression loss: 0.055590566247701645\n",
            "----------------\n",
            "Loss:  0.8072137832641602 \n",
            "Classification loss: 1.5000183582305908 \n",
            "Regression loss: 0.057204604148864746\n",
            "----------------\n",
            "Loss:  0.6899113226681948 \n",
            "Classification loss: 1.3192741870880127 \n",
            "Regression loss: 0.030274229124188423\n",
            "----------------\n",
            "Loss:  0.7759870290756226 \n",
            "Classification loss: 1.4945392608642578 \n",
            "Regression loss: 0.028717398643493652\n",
            "----------------\n",
            "Loss:  0.7354410402476788 \n",
            "Classification loss: 1.3749719858169556 \n",
            "Regression loss: 0.047955047339200974\n",
            "----------------\n",
            "Loss:  0.7327625378966331 \n",
            "Classification loss: 1.3490040302276611 \n",
            "Regression loss: 0.05826052278280258\n",
            "----------------\n",
            "Loss:  0.8352766558527946 \n",
            "Classification loss: 1.5362942218780518 \n",
            "Regression loss: 0.06712954491376877\n",
            "----------------\n",
            "Loss:  0.6893825456500053 \n",
            "Classification loss: 1.3154065608978271 \n",
            "Regression loss: 0.031679265201091766\n",
            "----------------\n",
            "Loss:  0.8248239308595657 \n",
            "Classification loss: 1.560547113418579 \n",
            "Regression loss: 0.044550374150276184\n",
            "----------------\n",
            "Loss:  0.8190342672169209 \n",
            "Classification loss: 1.5581214427947998 \n",
            "Regression loss: 0.03997354581952095\n",
            "----------------\n",
            "Loss:  0.9311557337641716 \n",
            "Classification loss: 1.7323839664459229 \n",
            "Regression loss: 0.06496375054121017\n",
            "----------------\n",
            "Loss:  0.7644971832633018 \n",
            "Classification loss: 1.4268913269042969 \n",
            "Regression loss: 0.05105151981115341\n",
            "----------------\n",
            "Loss:  0.7147292606532574 \n",
            "Classification loss: 1.3419491052627563 \n",
            "Regression loss: 0.043754708021879196\n",
            "----------------\n",
            "Loss:  0.8400703631341457 \n",
            "Classification loss: 1.5911272764205933 \n",
            "Regression loss: 0.044506724923849106\n",
            "----------------\n",
            "Loss:  0.8146785721182823 \n",
            "Classification loss: 1.5664852857589722 \n",
            "Regression loss: 0.031435929238796234\n",
            "----------------\n",
            "Loss:  0.7181625626981258 \n",
            "Classification loss: 1.369053602218628 \n",
            "Regression loss: 0.033635761588811874\n",
            "----------------\n",
            "Loss:  0.6283465847373009 \n",
            "Classification loss: 1.193497896194458 \n",
            "Regression loss: 0.03159763664007187\n",
            "----------------\n",
            "Loss:  0.6951444484293461 \n",
            "Classification loss: 1.335942268371582 \n",
            "Regression loss: 0.02717331424355507\n",
            "----------------\n",
            "Loss:  0.8397893086075783 \n",
            "Classification loss: 1.5636322498321533 \n",
            "Regression loss: 0.05797318369150162\n",
            "----------------\n",
            "Loss:  0.7158471830189228 \n",
            "Classification loss: 1.346297025680542 \n",
            "Regression loss: 0.04269867017865181\n",
            "----------------\n",
            "Loss:  0.7523044385015965 \n",
            "Classification loss: 1.4379220008850098 \n",
            "Regression loss: 0.03334343805909157\n",
            "----------------\n",
            "Loss:  0.7568250820040703 \n",
            "Classification loss: 1.4131648540496826 \n",
            "Regression loss: 0.05024265497922897\n",
            "----------------\n",
            "Loss:  0.7486587837338448 \n",
            "Classification loss: 1.438645601272583 \n",
            "Regression loss: 0.029335983097553253\n",
            "----------------\n",
            "Loss:  0.707954928278923 \n",
            "Classification loss: 1.316347360610962 \n",
            "Regression loss: 0.04978124797344208\n",
            "----------------\n",
            "Loss:  0.6804525405168533 \n",
            "Classification loss: 1.2326277494430542 \n",
            "Regression loss: 0.06413866579532623\n",
            "----------------\n",
            "Loss:  0.795484721660614 \n",
            "Classification loss: 1.4769043922424316 \n",
            "Regression loss: 0.05703252553939819\n",
            "----------------\n",
            "Loss:  0.7163344509899616 \n",
            "Classification loss: 1.3468804359436035 \n",
            "Regression loss: 0.042894233018159866\n",
            "----------------\n",
            "Loss:  0.6624816134572029 \n",
            "Classification loss: 1.1942858695983887 \n",
            "Regression loss: 0.06533867865800858\n",
            "----------------\n",
            "Loss:  0.783742792904377 \n",
            "Classification loss: 1.437922477722168 \n",
            "Regression loss: 0.064781554043293\n",
            "----------------\n",
            "Loss:  0.87225291877985 \n",
            "Classification loss: 1.5934393405914307 \n",
            "Regression loss: 0.07553324848413467\n",
            "----------------\n",
            "Loss:  0.7333064898848534 \n",
            "Classification loss: 1.4002350568771362 \n",
            "Regression loss: 0.03318896144628525\n",
            "----------------\n",
            "Loss:  0.7034280076622963 \n",
            "Classification loss: 1.3482028245925903 \n",
            "Regression loss: 0.02932659536600113\n",
            "----------------\n",
            "Loss:  0.7329099029302597 \n",
            "Classification loss: 1.3135943412780762 \n",
            "Regression loss: 0.07611273229122162\n",
            "----------------\n",
            "Loss:  0.6627782061696053 \n",
            "Classification loss: 1.2542544603347778 \n",
            "Regression loss: 0.03565097600221634\n",
            "----------------\n",
            "Loss:  0.8251532353460789 \n",
            "Classification loss: 1.58700430393219 \n",
            "Regression loss: 0.0316510833799839\n",
            "----------------\n",
            "Loss:  0.7789981886744499 \n",
            "Classification loss: 1.4471173286437988 \n",
            "Regression loss: 0.05543952435255051\n",
            "----------------\n",
            "Loss:  0.7375061102211475 \n",
            "Classification loss: 1.394423246383667 \n",
            "Regression loss: 0.04029448702931404\n",
            "----------------\n",
            "Loss:  0.7650771886110306 \n",
            "Classification loss: 1.3988349437713623 \n",
            "Regression loss: 0.06565971672534943\n",
            "----------------\n",
            "Loss:  0.7139718234539032 \n",
            "Classification loss: 1.3564956188201904 \n",
            "Regression loss: 0.03572401404380798\n",
            "----------------\n",
            "Loss:  0.7743742503225803 \n",
            "Classification loss: 1.4384160041809082 \n",
            "Regression loss: 0.055166248232126236\n",
            "----------------\n",
            "Loss:  0.8413168340921402 \n",
            "Classification loss: 1.6090710163116455 \n",
            "Regression loss: 0.036781325936317444\n",
            "----------------\n",
            "Loss:  0.7926906049251556 \n",
            "Classification loss: 1.409399151802063 \n",
            "Regression loss: 0.08799102902412415\n",
            "----------------\n",
            "Loss:  0.6506489291787148 \n",
            "Classification loss: 1.2182352542877197 \n",
            "Regression loss: 0.04153130203485489\n",
            "----------------\n",
            "Loss:  0.736330296844244 \n",
            "Classification loss: 1.3953771591186523 \n",
            "Regression loss: 0.03864171728491783\n",
            "----------------\n",
            "Loss:  0.8705648258328438 \n",
            "Classification loss: 1.616520643234253 \n",
            "Regression loss: 0.062304504215717316\n",
            "----------------\n",
            "Loss:  0.8303001970052719 \n",
            "Classification loss: 1.5561485290527344 \n",
            "Regression loss: 0.052225932478904724\n",
            "----------------\n",
            "Loss:  0.8902144730091095 \n",
            "Classification loss: 1.676856517791748 \n",
            "Regression loss: 0.051786214113235474\n",
            "----------------\n",
            "Loss:  0.7355552017688751 \n",
            "Classification loss: 1.3419688940048218 \n",
            "Regression loss: 0.06457075476646423\n",
            "----------------\n",
            "Loss:  0.806208111345768 \n",
            "Classification loss: 1.477562665939331 \n",
            "Regression loss: 0.06742677837610245\n",
            "----------------\n",
            "Loss:  0.7505794651806355 \n",
            "Classification loss: 1.4227213859558105 \n",
            "Regression loss: 0.03921877220273018\n",
            "----------------\n",
            "Loss:  0.6750228926539421 \n",
            "Classification loss: 1.2259511947631836 \n",
            "Regression loss: 0.06204729527235031\n",
            "----------------\n",
            "Loss:  0.7527628652751446 \n",
            "Classification loss: 1.3903498649597168 \n",
            "Regression loss: 0.05758793279528618\n",
            "----------------\n",
            "Loss:  0.7891826629638672 \n",
            "Classification loss: 1.4954006671905518 \n",
            "Regression loss: 0.04148232936859131\n",
            "----------------\n",
            "Loss:  0.8086535036563873 \n",
            "Classification loss: 1.5243124961853027 \n",
            "Regression loss: 0.04649725556373596\n",
            "----------------\n",
            "Loss:  0.7156072333455086 \n",
            "Classification loss: 1.3693381547927856 \n",
            "Regression loss: 0.030938155949115753\n",
            "----------------\n",
            "Loss:  0.7300207987427711 \n",
            "Classification loss: 1.3774558305740356 \n",
            "Regression loss: 0.041292883455753326\n",
            "----------------\n",
            "Loss:  0.6992837488651276 \n",
            "Classification loss: 1.2934356927871704 \n",
            "Regression loss: 0.05256590247154236\n",
            "----------------\n",
            "Loss:  0.751288928091526 \n",
            "Classification loss: 1.457390308380127 \n",
            "Regression loss: 0.022593773901462555\n",
            "----------------\n",
            "Loss:  0.7146421149373055 \n",
            "Classification loss: 1.320749282836914 \n",
            "Regression loss: 0.05426747351884842\n",
            "----------------\n",
            "Loss:  0.7631161399185658 \n",
            "Classification loss: 1.4577515125274658 \n",
            "Regression loss: 0.03424038365483284\n",
            "----------------\n",
            "Loss:  0.7806112393736839 \n",
            "Classification loss: 1.4742155075073242 \n",
            "Regression loss: 0.04350348562002182\n",
            "----------------\n",
            "Loss:  0.6997898146510124 \n",
            "Classification loss: 1.2869751453399658 \n",
            "Regression loss: 0.05630224198102951\n",
            "----------------\n",
            "Loss:  0.7243100591003895 \n",
            "Classification loss: 1.3544998168945312 \n",
            "Regression loss: 0.047060150653123856\n",
            "----------------\n",
            "Loss:  0.8584167174994946 \n",
            "Classification loss: 1.626863718032837 \n",
            "Regression loss: 0.044984858483076096\n",
            "----------------\n",
            "Loss:  0.7834602445363998 \n",
            "Classification loss: 1.3948416709899902 \n",
            "Regression loss: 0.08603940904140472\n",
            "----------------\n",
            "Loss:  0.7190838307142258 \n",
            "Classification loss: 1.3076337575912476 \n",
            "Regression loss: 0.06526695191860199\n",
            "----------------\n",
            "Loss:  0.7033101208508015 \n",
            "Classification loss: 1.3312149047851562 \n",
            "Regression loss: 0.03770266845822334\n",
            "----------------\n",
            "Loss:  0.702624149620533 \n",
            "Classification loss: 1.3225858211517334 \n",
            "Regression loss: 0.04133123904466629\n",
            "----------------\n",
            "Loss:  0.6831200979650021 \n",
            "Classification loss: 1.2921959161758423 \n",
            "Regression loss: 0.03702213987708092\n",
            "----------------\n",
            "Loss:  0.7436850219964981 \n",
            "Classification loss: 1.3837720155715942 \n",
            "Regression loss: 0.05179901421070099\n",
            "----------------\n",
            "Loss:  0.7410690784454346 \n",
            "Classification loss: 1.3495734930038452 \n",
            "Regression loss: 0.06628233194351196\n",
            "----------------\n",
            "Loss:  0.6815574280917645 \n",
            "Classification loss: 1.2917945384979248 \n",
            "Regression loss: 0.03566015884280205\n",
            "----------------\n",
            "Loss:  0.6619811840355396 \n",
            "Classification loss: 1.2751723527908325 \n",
            "Regression loss: 0.024395007640123367\n",
            "----------------\n",
            "Loss:  0.7009194195270538 \n",
            "Classification loss: 1.3392444849014282 \n",
            "Regression loss: 0.03129717707633972\n",
            "----------------\n",
            "Loss:  0.6930132247507572 \n",
            "Classification loss: 1.3281736373901367 \n",
            "Regression loss: 0.028926406055688858\n",
            "----------------\n",
            "Loss:  0.788921419531107 \n",
            "Classification loss: 1.5044989585876465 \n",
            "Regression loss: 0.03667194023728371\n",
            "----------------\n",
            "Loss:  0.7450422942638397 \n",
            "Classification loss: 1.4044313430786133 \n",
            "Regression loss: 0.04282662272453308\n",
            "----------------\n",
            "Loss:  0.679271399974823 \n",
            "Classification loss: 1.2944865226745605 \n",
            "Regression loss: 0.032028138637542725\n",
            "----------------\n",
            "Loss:  0.6577751077711582 \n",
            "Classification loss: 1.2485709190368652 \n",
            "Regression loss: 0.0334896482527256\n",
            "----------------\n",
            "Loss:  0.6393376141786575 \n",
            "Classification loss: 1.1708331108093262 \n",
            "Regression loss: 0.053921058773994446\n",
            "----------------\n",
            "Loss:  0.7255998477339745 \n",
            "Classification loss: 1.3446683883666992 \n",
            "Regression loss: 0.05326565355062485\n",
            "----------------\n",
            "Loss:  0.6785380877554417 \n",
            "Classification loss: 1.3010077476501465 \n",
            "Regression loss: 0.028034213930368423\n",
            "----------------\n",
            "Loss:  0.7198639139533043 \n",
            "Classification loss: 1.406540036201477 \n",
            "Regression loss: 0.016593895852565765\n",
            "----------------\n",
            "Loss:  0.7470726110041142 \n",
            "Classification loss: 1.4219692945480347 \n",
            "Regression loss: 0.03608796373009682\n",
            "----------------\n",
            "Loss:  0.7902537509799004 \n",
            "Classification loss: 1.5102131366729736 \n",
            "Regression loss: 0.035147182643413544\n",
            "----------------\n",
            "Loss:  0.7245015017688274 \n",
            "Classification loss: 1.3456554412841797 \n",
            "Regression loss: 0.051673781126737595\n",
            "----------------\n",
            "Loss:  0.8376453071832657 \n",
            "Classification loss: 1.6076889038085938 \n",
            "Regression loss: 0.03380085527896881\n",
            "----------------\n",
            "Loss:  0.8445748761296272 \n",
            "Classification loss: 1.5993986129760742 \n",
            "Regression loss: 0.04487556964159012\n",
            "----------------\n",
            "Loss:  0.7269165515899658 \n",
            "Classification loss: 1.3642122745513916 \n",
            "Regression loss: 0.04481041431427002\n",
            "----------------\n",
            "Loss:  0.8859675973653793 \n",
            "Classification loss: 1.6161283254623413 \n",
            "Regression loss: 0.07790343463420868\n",
            "----------------\n",
            "Loss:  0.7701841071248055 \n",
            "Classification loss: 1.427005410194397 \n",
            "Regression loss: 0.056681402027606964\n",
            "----------------\n",
            "Loss:  0.7242191694676876 \n",
            "Classification loss: 1.3585309982299805 \n",
            "Regression loss: 0.04495367035269737\n",
            "----------------\n",
            "Loss:  0.8289153091609478 \n",
            "Classification loss: 1.564866065979004 \n",
            "Regression loss: 0.04648227617144585\n",
            "----------------\n",
            "Loss:  0.6878781281411648 \n",
            "Classification loss: 1.3231250047683716 \n",
            "Regression loss: 0.02631562575697899\n",
            "----------------\n",
            "Loss:  0.7411661893129349 \n",
            "Classification loss: 1.3570194244384766 \n",
            "Regression loss: 0.0626564770936966\n",
            "----------------\n",
            "Loss:  0.9139339737594128 \n",
            "Classification loss: 1.7141382694244385 \n",
            "Regression loss: 0.05686483904719353\n",
            "----------------\n",
            "Loss:  0.6777138821780682 \n",
            "Classification loss: 1.2600183486938477 \n",
            "Regression loss: 0.04770470783114433\n",
            "----------------\n",
            "Loss:  0.7838800698518753 \n",
            "Classification loss: 1.4221385717391968 \n",
            "Regression loss: 0.07281078398227692\n",
            "----------------\n",
            "Loss:  0.8189819753170013 \n",
            "Classification loss: 1.578758716583252 \n",
            "Regression loss: 0.029602617025375366\n",
            "----------------\n",
            "Loss:  0.6793409250676632 \n",
            "Classification loss: 1.276720404624939 \n",
            "Regression loss: 0.04098072275519371\n",
            "----------------\n",
            "Loss:  0.7377832792699337 \n",
            "Classification loss: 1.4108843803405762 \n",
            "Regression loss: 0.032341089099645615\n",
            "----------------\n",
            "Loss:  0.8333638533949852 \n",
            "Classification loss: 1.5301625728607178 \n",
            "Regression loss: 0.06828256696462631\n",
            "----------------\n",
            "Loss:  0.7614671513438225 \n",
            "Classification loss: 1.4221949577331543 \n",
            "Regression loss: 0.05036967247724533\n",
            "----------------\n",
            "Loss:  0.6875546835362911 \n",
            "Classification loss: 1.282163381576538 \n",
            "Regression loss: 0.04647299274802208\n",
            "----------------\n",
            "Loss:  0.8205488957464695 \n",
            "Classification loss: 1.5433249473571777 \n",
            "Regression loss: 0.04888642206788063\n",
            "----------------\n",
            "Loss:  0.773445300757885 \n",
            "Classification loss: 1.4503074884414673 \n",
            "Regression loss: 0.04829155653715134\n",
            "----------------\n",
            "Loss:  0.8301082290709019 \n",
            "Classification loss: 1.5578222274780273 \n",
            "Regression loss: 0.0511971153318882\n",
            "----------------\n",
            "Loss:  0.7641971334815025 \n",
            "Classification loss: 1.4156427383422852 \n",
            "Regression loss: 0.056375764310359955\n",
            "----------------\n",
            "Loss:  0.8037392590194941 \n",
            "Classification loss: 1.5576810836791992 \n",
            "Regression loss: 0.024898717179894447\n",
            "----------------\n",
            "Loss:  0.6347136944532394 \n",
            "Classification loss: 1.1982409954071045 \n",
            "Regression loss: 0.035593196749687195\n",
            "----------------\n",
            "Loss:  0.8124624565243721 \n",
            "Classification loss: 1.5179731845855713 \n",
            "Regression loss: 0.053475864231586456\n",
            "----------------\n",
            "Loss:  0.8406534492969513 \n",
            "Classification loss: 1.5109721422195435 \n",
            "Regression loss: 0.08516737818717957\n",
            "----------------\n",
            "Loss:  0.7401345893740654 \n",
            "Classification loss: 1.348839521408081 \n",
            "Regression loss: 0.06571482867002487\n",
            "----------------\n",
            "Loss:  0.8219442926347256 \n",
            "Classification loss: 1.542026400566101 \n",
            "Regression loss: 0.050931092351675034\n",
            "----------------\n",
            "Loss:  0.6348832100629807 \n",
            "Classification loss: 1.222316026687622 \n",
            "Regression loss: 0.023725196719169617\n",
            "----------------\n",
            "Loss:  0.8000702485442162 \n",
            "Classification loss: 1.4901769161224365 \n",
            "Regression loss: 0.054981790482997894\n",
            "----------------\n",
            "Loss:  0.7749201692640781 \n",
            "Classification loss: 1.4596285820007324 \n",
            "Regression loss: 0.04510587826371193\n",
            "----------------\n",
            "Loss:  0.8235968574881554 \n",
            "Classification loss: 1.540372371673584 \n",
            "Regression loss: 0.05341067165136337\n",
            "----------------\n",
            "Loss:  0.6510566249489784 \n",
            "Classification loss: 1.2190401554107666 \n",
            "Regression loss: 0.04153654724359512\n",
            "----------------\n",
            "Loss:  0.8131154626607895 \n",
            "Classification loss: 1.499391794204712 \n",
            "Regression loss: 0.06341956555843353\n",
            "----------------\n",
            "Loss:  0.6545630991458893 \n",
            "Classification loss: 1.2377898693084717 \n",
            "Regression loss: 0.03566816449165344\n",
            "----------------\n",
            "Loss:  0.7889588251709938 \n",
            "Classification loss: 1.4769436120986938 \n",
            "Regression loss: 0.05048701912164688\n",
            "----------------\n",
            "Loss:  0.778260163962841 \n",
            "Classification loss: 1.4534105062484741 \n",
            "Regression loss: 0.05155491083860397\n",
            "----------------\n",
            "Loss:  0.6646720953285694 \n",
            "Classification loss: 1.2124454975128174 \n",
            "Regression loss: 0.05844934657216072\n",
            "----------------\n",
            "Loss:  0.8357228934764862 \n",
            "Classification loss: 1.5068728923797607 \n",
            "Regression loss: 0.08228644728660583\n",
            "----------------\n",
            "Loss:  0.8705596476793289 \n",
            "Classification loss: 1.6144623756408691 \n",
            "Regression loss: 0.06332845985889435\n",
            "----------------\n",
            "Loss:  0.741903766989708 \n",
            "Classification loss: 1.3707468509674072 \n",
            "Regression loss: 0.056530341506004333\n",
            "----------------\n",
            "Loss:  0.6348626334220171 \n",
            "Classification loss: 1.230107307434082 \n",
            "Regression loss: 0.019808979704976082\n",
            "----------------\n",
            "Loss:  0.7382374778389931 \n",
            "Classification loss: 1.3264707326889038 \n",
            "Regression loss: 0.07500211149454117\n",
            "----------------\n",
            "Loss:  0.770147442817688 \n",
            "Classification loss: 1.4386409521102905 \n",
            "Regression loss: 0.050826966762542725\n",
            "----------------\n",
            "Loss:  0.7858293056488037 \n",
            "Classification loss: 1.489442229270935 \n",
            "Regression loss: 0.04110819101333618\n",
            "----------------\n",
            "Loss:  0.852063000202179 \n",
            "Classification loss: 1.5911924839019775 \n",
            "Regression loss: 0.056466758251190186\n",
            "----------------\n",
            "Loss:  0.7467743195593357 \n",
            "Classification loss: 1.401524543762207 \n",
            "Regression loss: 0.04601204767823219\n",
            "----------------\n",
            "Loss:  0.8158972561359406 \n",
            "Classification loss: 1.5575021505355835 \n",
            "Regression loss: 0.037146180868148804\n",
            "----------------\n",
            "Loss:  0.8591839075088501 \n",
            "Classification loss: 1.5584800243377686 \n",
            "Regression loss: 0.07994389533996582\n",
            "----------------\n",
            "Loss:  0.7724287509918213 \n",
            "Classification loss: 1.449068307876587 \n",
            "Regression loss: 0.04789459705352783\n",
            "----------------\n",
            "Loss:  0.7308914624154568 \n",
            "Classification loss: 1.3596268892288208 \n",
            "Regression loss: 0.05107801780104637\n",
            "----------------\n",
            "Loss:  0.8213043212890625 \n",
            "Classification loss: 1.5087661743164062 \n",
            "Regression loss: 0.06692123413085938\n",
            "----------------\n",
            "Loss:  0.72284384816885 \n",
            "Classification loss: 1.3663175106048584 \n",
            "Regression loss: 0.039685092866420746\n",
            "----------------\n",
            "Loss:  0.7787502408027649 \n",
            "Classification loss: 1.4444959163665771 \n",
            "Regression loss: 0.05650228261947632\n",
            "----------------\n",
            "Loss:  0.7194172367453575 \n",
            "Classification loss: 1.3199882507324219 \n",
            "Regression loss: 0.059423111379146576\n",
            "----------------\n",
            "Loss:  0.7453505173325539 \n",
            "Classification loss: 1.409257411956787 \n",
            "Regression loss: 0.04072181135416031\n",
            "----------------\n",
            "Loss:  0.6519442424178123 \n",
            "Classification loss: 1.2139898538589478 \n",
            "Regression loss: 0.04494931548833847\n",
            "----------------\n",
            "Loss:  0.6815986409783363 \n",
            "Classification loss: 1.2715673446655273 \n",
            "Regression loss: 0.04581496864557266\n",
            "----------------\n",
            "Loss:  0.7343132756650448 \n",
            "Classification loss: 1.349352478981018 \n",
            "Regression loss: 0.05963703617453575\n",
            "----------------\n",
            "Loss:  0.7079318836331367 \n",
            "Classification loss: 1.3048685789108276 \n",
            "Regression loss: 0.05549759417772293\n",
            "----------------\n",
            "Loss:  0.6782090179622173 \n",
            "Classification loss: 1.2716269493103027 \n",
            "Regression loss: 0.042395543307065964\n",
            "----------------\n",
            "Loss:  0.7743582874536514 \n",
            "Classification loss: 1.4424059391021729 \n",
            "Regression loss: 0.053155317902565\n",
            "----------------\n",
            "Loss:  0.7707151621580124 \n",
            "Classification loss: 1.4639204740524292 \n",
            "Regression loss: 0.03875492513179779\n",
            "----------------\n",
            "Loss:  0.7596524804830551 \n",
            "Classification loss: 1.3914687633514404 \n",
            "Regression loss: 0.0639180988073349\n",
            "----------------\n",
            "Loss:  0.7448587901890278 \n",
            "Classification loss: 1.3949971199035645 \n",
            "Regression loss: 0.04736023023724556\n",
            "----------------\n",
            "Loss:  0.7773089855909348 \n",
            "Classification loss: 1.4606842994689941 \n",
            "Regression loss: 0.04696683585643768\n",
            "----------------\n",
            "Loss:  0.8028217032551765 \n",
            "Classification loss: 1.456566333770752 \n",
            "Regression loss: 0.07453853636980057\n",
            "----------------\n",
            "Loss:  0.7447715029120445 \n",
            "Classification loss: 1.376212477684021 \n",
            "Regression loss: 0.05666526407003403\n",
            "----------------\n",
            "Loss:  0.8249852433800697 \n",
            "Classification loss: 1.5097200870513916 \n",
            "Regression loss: 0.07012519985437393\n",
            "----------------\n",
            "Loss:  0.7580694891512394 \n",
            "Classification loss: 1.417096734046936 \n",
            "Regression loss: 0.04952112212777138\n",
            "----------------\n",
            "Loss:  0.7862334884703159 \n",
            "Classification loss: 1.4989088773727417 \n",
            "Regression loss: 0.036779049783945084\n",
            "----------------\n",
            "Loss:  0.8246254175901413 \n",
            "Classification loss: 1.564668893814087 \n",
            "Regression loss: 0.04229097068309784\n",
            "----------------\n",
            "Loss:  0.7802902311086655 \n",
            "Classification loss: 1.4280591011047363 \n",
            "Regression loss: 0.0662606805562973\n",
            "----------------\n",
            "Loss:  0.7235493287444115 \n",
            "Classification loss: 1.3022525310516357 \n",
            "Regression loss: 0.0724230632185936\n",
            "----------------\n",
            "Loss:  0.7409275211393833 \n",
            "Classification loss: 1.3882179260253906 \n",
            "Regression loss: 0.046818558126688004\n",
            "----------------\n",
            "Loss:  0.7553363516926765 \n",
            "Classification loss: 1.4402220249176025 \n",
            "Regression loss: 0.035225339233875275\n",
            "----------------\n",
            "Loss:  0.6827860623598099 \n",
            "Classification loss: 1.300220251083374 \n",
            "Regression loss: 0.032675936818122864\n",
            "----------------\n",
            "Loss:  0.6453371644020081 \n",
            "Classification loss: 1.2234272956848145 \n",
            "Regression loss: 0.03362351655960083\n",
            "----------------\n",
            "Loss:  0.7627955824136734 \n",
            "Classification loss: 1.3986103534698486 \n",
            "Regression loss: 0.06349040567874908\n",
            "----------------\n",
            "Loss:  0.780320193618536 \n",
            "Classification loss: 1.4364941120147705 \n",
            "Regression loss: 0.06207313761115074\n",
            "----------------\n",
            "Loss:  0.6857752874493599 \n",
            "Classification loss: 1.2895712852478027 \n",
            "Regression loss: 0.04098964482545853\n",
            "----------------\n",
            "Loss:  0.6867550648748875 \n",
            "Classification loss: 1.2998489141464233 \n",
            "Regression loss: 0.036830607801675797\n",
            "----------------\n",
            "Loss:  0.7131410650908947 \n",
            "Classification loss: 1.336878776550293 \n",
            "Regression loss: 0.044701676815748215\n",
            "----------------\n",
            "Loss:  0.7238002344965935 \n",
            "Classification loss: 1.384793758392334 \n",
            "Regression loss: 0.03140335530042648\n",
            "----------------\n",
            "Loss:  0.7025407403707504 \n",
            "Classification loss: 1.3302648067474365 \n",
            "Regression loss: 0.037408336997032166\n",
            "----------------\n",
            "Loss:  0.7684596963226795 \n",
            "Classification loss: 1.4247307777404785 \n",
            "Regression loss: 0.05609430745244026\n",
            "----------------\n",
            "Loss:  0.697720967233181 \n",
            "Classification loss: 1.2886855602264404 \n",
            "Regression loss: 0.053378187119960785\n",
            "----------------\n",
            "Loss:  0.8221472948789597 \n",
            "Classification loss: 1.5432875156402588 \n",
            "Regression loss: 0.05050353705883026\n",
            "----------------\n",
            "Loss:  0.6997821815311909 \n",
            "Classification loss: 1.3370213508605957 \n",
            "Regression loss: 0.03127150610089302\n",
            "----------------\n",
            "Loss:  0.6816418021917343 \n",
            "Classification loss: 1.2608237266540527 \n",
            "Regression loss: 0.05122993886470795\n",
            "----------------\n",
            "Loss:  0.6678333170711994 \n",
            "Classification loss: 1.2247415781021118 \n",
            "Regression loss: 0.05546252802014351\n",
            "----------------\n",
            "Loss:  0.6513473410159349 \n",
            "Classification loss: 1.24239182472229 \n",
            "Regression loss: 0.030151428654789925\n",
            "----------------\n",
            "Loss:  0.7150837406516075 \n",
            "Classification loss: 1.322167158126831 \n",
            "Regression loss: 0.054000161588191986\n",
            "----------------\n",
            "Loss:  0.7708792015910149 \n",
            "Classification loss: 1.451174259185791 \n",
            "Regression loss: 0.045292071998119354\n",
            "----------------\n",
            "Loss:  0.7489272132515907 \n",
            "Classification loss: 1.416101336479187 \n",
            "Regression loss: 0.04087654501199722\n",
            "----------------\n",
            "Loss:  0.6732534542679787 \n",
            "Classification loss: 1.2700574398040771 \n",
            "Regression loss: 0.038224734365940094\n",
            "----------------\n",
            "Loss:  0.8794895112514496 \n",
            "Classification loss: 1.6031482219696045 \n",
            "Regression loss: 0.07791540026664734\n",
            "----------------\n",
            "Loss:  0.67585663869977 \n",
            "Classification loss: 1.2608370780944824 \n",
            "Regression loss: 0.04543809965252876\n",
            "----------------\n",
            "Loss:  0.7254200465977192 \n",
            "Classification loss: 1.3810911178588867 \n",
            "Regression loss: 0.03487448766827583\n",
            "----------------\n",
            "Loss:  0.6912697702646255 \n",
            "Classification loss: 1.2925841808319092 \n",
            "Regression loss: 0.04497767984867096\n",
            "----------------\n",
            "Loss:  0.7688099294900894 \n",
            "Classification loss: 1.412703514099121 \n",
            "Regression loss: 0.06245817244052887\n",
            "----------------\n",
            "Loss:  0.8758802562952042 \n",
            "Classification loss: 1.6753990650177002 \n",
            "Regression loss: 0.038180723786354065\n",
            "----------------\n",
            "Loss:  0.6892854943871498 \n",
            "Classification loss: 1.2967963218688965 \n",
            "Regression loss: 0.04088733345270157\n",
            "----------------\n",
            "Loss:  0.7318148091435432 \n",
            "Classification loss: 1.3844729661941528 \n",
            "Regression loss: 0.03957832604646683\n",
            "----------------\n",
            "Loss:  0.7236691154539585 \n",
            "Classification loss: 1.3530312776565552 \n",
            "Regression loss: 0.04715347662568092\n",
            "----------------\n",
            "Loss:  0.7310355976223946 \n",
            "Classification loss: 1.3861082792282104 \n",
            "Regression loss: 0.03798145800828934\n",
            "----------------\n",
            "Loss:  0.6957383677363396 \n",
            "Classification loss: 1.2608063220977783 \n",
            "Regression loss: 0.06533520668745041\n",
            "----------------\n",
            "Loss:  0.8210209161043167 \n",
            "Classification loss: 1.5181440114974976 \n",
            "Regression loss: 0.06194891035556793\n",
            "----------------\n",
            "Loss:  0.7549857385456562 \n",
            "Classification loss: 1.3907572031021118 \n",
            "Regression loss: 0.059607136994600296\n",
            "----------------\n",
            "Loss:  0.7615272291004658 \n",
            "Classification loss: 1.4431142807006836 \n",
            "Regression loss: 0.03997008875012398\n",
            "----------------\n",
            "Loss:  0.8366791866719723 \n",
            "Classification loss: 1.563432216644287 \n",
            "Regression loss: 0.05496307834982872\n",
            "----------------\n",
            "Loss:  0.7675750479102135 \n",
            "Classification loss: 1.3996323347091675 \n",
            "Regression loss: 0.06775888055562973\n",
            "----------------\n",
            "Loss:  0.8061559647321701 \n",
            "Classification loss: 1.4774945974349976 \n",
            "Regression loss: 0.06740866601467133\n",
            "----------------\n",
            "Loss:  0.6714482679963112 \n",
            "Classification loss: 1.264086365699768 \n",
            "Regression loss: 0.039405085146427155\n",
            "----------------\n",
            "Loss:  0.8574212193489075 \n",
            "Classification loss: 1.5869178771972656 \n",
            "Regression loss: 0.06396228075027466\n",
            "----------------\n",
            "Loss:  0.7448085695505142 \n",
            "Classification loss: 1.423585057258606 \n",
            "Regression loss: 0.03301604092121124\n",
            "----------------\n",
            "Loss:  0.7174840271472931 \n",
            "Classification loss: 1.283328652381897 \n",
            "Regression loss: 0.0758197009563446\n",
            "----------------\n",
            "Loss:  0.7915404662489891 \n",
            "Classification loss: 1.4475903511047363 \n",
            "Regression loss: 0.06774529069662094\n",
            "----------------\n",
            "Loss:  0.7290367260575294 \n",
            "Classification loss: 1.3365347385406494 \n",
            "Regression loss: 0.06076935678720474\n",
            "----------------\n",
            "Loss:  0.7171666137874126 \n",
            "Classification loss: 1.343620777130127 \n",
            "Regression loss: 0.04535622522234917\n",
            "----------------\n",
            "Loss:  0.7655652984976768 \n",
            "Classification loss: 1.4280359745025635 \n",
            "Regression loss: 0.05154731124639511\n",
            "----------------\n",
            "Loss:  0.7965172305703163 \n",
            "Classification loss: 1.494728922843933 \n",
            "Regression loss: 0.04915276914834976\n",
            "----------------\n",
            "Loss:  0.7365557700395584 \n",
            "Classification loss: 1.3992834091186523 \n",
            "Regression loss: 0.03691406548023224\n",
            "----------------\n",
            "Loss:  0.7648131810128689 \n",
            "Classification loss: 1.478947639465332 \n",
            "Regression loss: 0.025339361280202866\n",
            "----------------\n",
            "Loss:  0.7142517380416393 \n",
            "Classification loss: 1.3062031269073486 \n",
            "Regression loss: 0.06115017458796501\n",
            "----------------\n",
            "Loss:  0.7930705435574055 \n",
            "Classification loss: 1.5134291648864746 \n",
            "Regression loss: 0.03635596111416817\n",
            "----------------\n",
            "Loss:  0.8294419497251511 \n",
            "Classification loss: 1.526801347732544 \n",
            "Regression loss: 0.06604127585887909\n",
            "----------------\n",
            "Loss:  0.8429946079850197 \n",
            "Classification loss: 1.5398489236831665 \n",
            "Regression loss: 0.07307014614343643\n",
            "----------------\n",
            "Loss:  0.8021868988871574 \n",
            "Classification loss: 1.5238980054855347 \n",
            "Regression loss: 0.040237896144390106\n",
            "----------------\n",
            "Loss:  0.7500583752989769 \n",
            "Classification loss: 1.4027488231658936 \n",
            "Regression loss: 0.04868396371603012\n",
            "----------------\n",
            "Loss:  0.7121260724961758 \n",
            "Classification loss: 1.3159356117248535 \n",
            "Regression loss: 0.05415826663374901\n",
            "----------------\n",
            "Loss:  0.7755875512957573 \n",
            "Classification loss: 1.4747322797775269 \n",
            "Regression loss: 0.038221411406993866\n",
            "----------------\n",
            "Loss:  0.7184200882911682 \n",
            "Classification loss: 1.3106837272644043 \n",
            "Regression loss: 0.06307822465896606\n",
            "----------------\n",
            "Loss:  0.6927256882190704 \n",
            "Classification loss: 1.2668383121490479 \n",
            "Regression loss: 0.05930653214454651\n",
            "----------------\n",
            "Loss:  0.8051554001867771 \n",
            "Classification loss: 1.4934221506118774 \n",
            "Regression loss: 0.058444324880838394\n",
            "----------------\n",
            "Loss:  0.711167898029089 \n",
            "Classification loss: 1.321115493774414 \n",
            "Regression loss: 0.05061015114188194\n",
            "----------------\n",
            "Loss:  0.7081518210470676 \n",
            "Classification loss: 1.3200665712356567 \n",
            "Regression loss: 0.04811853542923927\n",
            "----------------\n",
            "Loss:  0.7254410460591316 \n",
            "Classification loss: 1.324971079826355 \n",
            "Regression loss: 0.06295550614595413\n",
            "----------------\n",
            "Loss:  0.7196590043604374 \n",
            "Classification loss: 1.3623723983764648 \n",
            "Regression loss: 0.03847280517220497\n",
            "----------------\n",
            "Loss:  0.8324837684631348 \n",
            "Classification loss: 1.585071325302124 \n",
            "Regression loss: 0.039948105812072754\n",
            "----------------\n",
            "Loss:  0.7378161773085594 \n",
            "Classification loss: 1.356988549232483 \n",
            "Regression loss: 0.05932190269231796\n",
            "----------------\n",
            "Loss:  0.8532530330121517 \n",
            "Classification loss: 1.6294589042663574 \n",
            "Regression loss: 0.03852358087897301\n",
            "----------------\n",
            "Loss:  0.8151364624500275 \n",
            "Classification loss: 1.5065271854400635 \n",
            "Regression loss: 0.06187286972999573\n",
            "----------------\n",
            "Loss:  0.8273320496082306 \n",
            "Classification loss: 1.5159645080566406 \n",
            "Regression loss: 0.06934979557991028\n",
            "----------------\n",
            "Loss:  0.7290276736021042 \n",
            "Classification loss: 1.3809216022491455 \n",
            "Regression loss: 0.03856687247753143\n",
            "----------------\n",
            "Loss:  0.8014249727129936 \n",
            "Classification loss: 1.5122814178466797 \n",
            "Regression loss: 0.04528426378965378\n",
            "----------------\n",
            "Loss:  0.8221892304718494 \n",
            "Classification loss: 1.539734125137329 \n",
            "Regression loss: 0.05232216790318489\n",
            "----------------\n",
            "Loss:  0.6356289386749268 \n",
            "Classification loss: 1.149850845336914 \n",
            "Regression loss: 0.06070351600646973\n",
            "----------------\n",
            "Loss:  0.722205325961113 \n",
            "Classification loss: 1.3284622430801392 \n",
            "Regression loss: 0.057974204421043396\n",
            "----------------\n",
            "Loss:  0.8840923644602299 \n",
            "Classification loss: 1.6518312692642212 \n",
            "Regression loss: 0.05817672982811928\n",
            "----------------\n",
            "Loss:  0.6948794722557068 \n",
            "Classification loss: 1.2903192043304443 \n",
            "Regression loss: 0.04971987009048462\n",
            "----------------\n",
            "Loss:  0.7565123438835144 \n",
            "Classification loss: 1.3820326328277588 \n",
            "Regression loss: 0.06549602746963501\n",
            "----------------\n",
            "Loss:  0.6790259890258312 \n",
            "Classification loss: 1.2660369873046875 \n",
            "Regression loss: 0.04600749537348747\n",
            "----------------\n",
            "Loss:  0.7042139545083046 \n",
            "Classification loss: 1.3110136985778809 \n",
            "Regression loss: 0.048707105219364166\n",
            "----------------\n",
            "Loss:  0.6452898681163788 \n",
            "Classification loss: 1.2034249305725098 \n",
            "Regression loss: 0.0435774028301239\n",
            "----------------\n",
            "Loss:  0.6479020304977894 \n",
            "Classification loss: 1.211233139038086 \n",
            "Regression loss: 0.042285460978746414\n",
            "----------------\n",
            "Loss:  0.8239791244268417 \n",
            "Classification loss: 1.4941579103469849 \n",
            "Regression loss: 0.0769001692533493\n",
            "----------------\n",
            "Loss:  0.6883141472935677 \n",
            "Classification loss: 1.2816851139068604 \n",
            "Regression loss: 0.04747159034013748\n",
            "----------------\n",
            "Loss:  0.7378662340342999 \n",
            "Classification loss: 1.3832529783248901 \n",
            "Regression loss: 0.04623974487185478\n",
            "----------------\n",
            "Loss:  0.7957196421921253 \n",
            "Classification loss: 1.4685280323028564 \n",
            "Regression loss: 0.0614556260406971\n",
            "----------------\n",
            "Loss:  0.6792661733925343 \n",
            "Classification loss: 1.2577555179595947 \n",
            "Regression loss: 0.05038841441273689\n",
            "----------------\n",
            "Loss:  0.7374613583087921 \n",
            "Classification loss: 1.3961061239242554 \n",
            "Regression loss: 0.03940829634666443\n",
            "----------------\n",
            "Loss:  0.7044528461992741 \n",
            "Classification loss: 1.3351281881332397 \n",
            "Regression loss: 0.03688875213265419\n",
            "----------------\n",
            "Loss:  0.6930230259895325 \n",
            "Classification loss: 1.2969012260437012 \n",
            "Regression loss: 0.044572412967681885\n",
            "----------------\n",
            "Loss:  0.802065197378397 \n",
            "Classification loss: 1.5095036029815674 \n",
            "Regression loss: 0.047313395887613297\n",
            "----------------\n",
            "Loss:  0.8007862716913223 \n",
            "Classification loss: 1.5462936162948608 \n",
            "Regression loss: 0.027639463543891907\n",
            "----------------\n",
            "Loss:  0.6864404454827309 \n",
            "Classification loss: 1.3060832023620605 \n",
            "Regression loss: 0.03339884430170059\n",
            "----------------\n",
            "Loss:  0.7434975653886795 \n",
            "Classification loss: 1.3775911331176758 \n",
            "Regression loss: 0.054701998829841614\n",
            "----------------\n",
            "Loss:  0.7246785499155521 \n",
            "Classification loss: 1.3687291145324707 \n",
            "Regression loss: 0.04031399264931679\n",
            "----------------\n",
            "Loss:  0.8431212864816189 \n",
            "Classification loss: 1.5768465995788574 \n",
            "Regression loss: 0.05469798669219017\n",
            "----------------\n",
            "Loss:  0.8267807960510254 \n",
            "Classification loss: 1.5561141967773438 \n",
            "Regression loss: 0.048723697662353516\n",
            "----------------\n",
            "Loss:  0.7046142220497131 \n",
            "Classification loss: 1.259417176246643 \n",
            "Regression loss: 0.0749056339263916\n",
            "----------------\n",
            "Loss:  0.8377476260066032 \n",
            "Classification loss: 1.5388470888137817 \n",
            "Regression loss: 0.06832408159971237\n",
            "----------------\n",
            "Loss:  0.7052712962031364 \n",
            "Classification loss: 1.3268709182739258 \n",
            "Regression loss: 0.04183583706617355\n",
            "----------------\n",
            "Loss:  0.6786791086196899 \n",
            "Classification loss: 1.317283272743225 \n",
            "Regression loss: 0.020037472248077393\n",
            "----------------\n",
            "Loss:  0.6898076869547367 \n",
            "Classification loss: 1.2977749109268188 \n",
            "Regression loss: 0.040920231491327286\n",
            "----------------\n",
            "Loss:  0.8379888385534286 \n",
            "Classification loss: 1.547140121459961 \n",
            "Regression loss: 0.06441877782344818\n",
            "----------------\n",
            "Loss:  0.710425615310669 \n",
            "Classification loss: 1.3205523490905762 \n",
            "Regression loss: 0.05014944076538086\n",
            "----------------\n",
            "Loss:  0.7244273461401463 \n",
            "Classification loss: 1.3557487726211548 \n",
            "Regression loss: 0.04655295982956886\n",
            "----------------\n",
            "Loss:  0.8345253020524979 \n",
            "Classification loss: 1.5965707302093506 \n",
            "Regression loss: 0.03623993694782257\n",
            "----------------\n",
            "Loss:  0.7694130726158619 \n",
            "Classification loss: 1.4744236469268799 \n",
            "Regression loss: 0.03220124915242195\n",
            "----------------\n",
            "Loss:  0.7953237742185593 \n",
            "Classification loss: 1.5064194202423096 \n",
            "Regression loss: 0.04211406409740448\n",
            "----------------\n",
            "Loss:  0.749826043844223 \n",
            "Classification loss: 1.422447681427002 \n",
            "Regression loss: 0.038602203130722046\n",
            "----------------\n",
            "Loss:  0.8734782040119171 \n",
            "Classification loss: 1.6158336400985718 \n",
            "Regression loss: 0.06556138396263123\n",
            "----------------\n",
            "Loss:  0.8872772678732872 \n",
            "Classification loss: 1.6051435470581055 \n",
            "Regression loss: 0.08470549434423447\n",
            "----------------\n",
            "Loss:  0.6428080350160599 \n",
            "Classification loss: 1.2049918174743652 \n",
            "Regression loss: 0.04031212627887726\n",
            "----------------\n",
            "Loss:  0.7189019173383713 \n",
            "Classification loss: 1.3441720008850098 \n",
            "Regression loss: 0.046815916895866394\n",
            "----------------\n",
            "Loss:  0.7614768706262112 \n",
            "Classification loss: 1.4337847232818604 \n",
            "Regression loss: 0.04458450898528099\n",
            "----------------\n",
            "Loss:  0.7628841027617455 \n",
            "Classification loss: 1.4356786012649536 \n",
            "Regression loss: 0.045044802129268646\n",
            "----------------\n",
            "Loss:  0.6700040623545647 \n",
            "Classification loss: 1.2747080326080322 \n",
            "Regression loss: 0.03265004605054855\n",
            "----------------\n",
            "Loss:  0.8242989629507065 \n",
            "Classification loss: 1.5349671840667725 \n",
            "Regression loss: 0.05681537091732025\n",
            "----------------\n",
            "Loss:  0.6566965878009796 \n",
            "Classification loss: 1.22108793258667 \n",
            "Regression loss: 0.04615262150764465\n",
            "----------------\n",
            "Loss:  0.7080933079123497 \n",
            "Classification loss: 1.317540168762207 \n",
            "Regression loss: 0.049323223531246185\n",
            "----------------\n",
            "Loss:  0.6779031455516815 \n",
            "Classification loss: 1.2312458753585815 \n",
            "Regression loss: 0.06228020787239075\n",
            "----------------\n",
            "Loss:  0.782961294054985 \n",
            "Classification loss: 1.424880862236023 \n",
            "Regression loss: 0.07052086293697357\n",
            "----------------\n",
            "Loss:  0.751165896654129 \n",
            "Classification loss: 1.4198211431503296 \n",
            "Regression loss: 0.04125532507896423\n",
            "----------------\n",
            "Loss:  0.707226675003767 \n",
            "Classification loss: 1.322218894958496 \n",
            "Regression loss: 0.04611722752451897\n",
            "----------------\n",
            "Loss:  0.6990264318883419 \n",
            "Classification loss: 1.3097641468048096 \n",
            "Regression loss: 0.04414435848593712\n",
            "----------------\n",
            "Loss:  0.6953126788139343 \n",
            "Classification loss: 1.2803237438201904 \n",
            "Regression loss: 0.05515080690383911\n",
            "----------------\n",
            "Loss:  0.8323128968477249 \n",
            "Classification loss: 1.5218712091445923 \n",
            "Regression loss: 0.07137729227542877\n",
            "----------------\n",
            "Loss:  0.7172581441700459 \n",
            "Classification loss: 1.359290599822998 \n",
            "Regression loss: 0.03761284425854683\n",
            "----------------\n",
            "Loss:  0.7270008567720652 \n",
            "Classification loss: 1.394253134727478 \n",
            "Regression loss: 0.02987428940832615\n",
            "----------------\n",
            "Loss:  0.7205076180398464 \n",
            "Classification loss: 1.3308947086334229 \n",
            "Regression loss: 0.055060263723134995\n",
            "----------------\n",
            "Loss:  0.7281674258410931 \n",
            "Classification loss: 1.3877817392349243 \n",
            "Regression loss: 0.034276556223630905\n",
            "----------------\n",
            "Loss:  0.7084773927927017 \n",
            "Classification loss: 1.3454898595809937 \n",
            "Regression loss: 0.035732463002204895\n",
            "----------------\n",
            "Loss:  0.789971362799406 \n",
            "Classification loss: 1.479112148284912 \n",
            "Regression loss: 0.05041528865695\n",
            "----------------\n",
            "Loss:  0.8314217105507851 \n",
            "Classification loss: 1.5128177404403687 \n",
            "Regression loss: 0.07501284033060074\n",
            "----------------\n",
            "Loss:  0.8653546720743179 \n",
            "Classification loss: 1.5946028232574463 \n",
            "Regression loss: 0.06805326044559479\n",
            "----------------\n",
            "Loss:  0.7472753748297691 \n",
            "Classification loss: 1.3555293083190918 \n",
            "Regression loss: 0.06951072067022324\n",
            "----------------\n",
            "Loss:  0.6496788784861565 \n",
            "Classification loss: 1.2534980773925781 \n",
            "Regression loss: 0.0229298397898674\n",
            "----------------\n",
            "Loss:  0.6529690250754356 \n",
            "Classification loss: 1.1991350650787354 \n",
            "Regression loss: 0.05340149253606796\n",
            "----------------\n",
            "Loss:  0.765438336879015 \n",
            "Classification loss: 1.4518895149230957 \n",
            "Regression loss: 0.03949357941746712\n",
            "----------------\n",
            "Loss:  0.7235199734568596 \n",
            "Classification loss: 1.3124971389770508 \n",
            "Regression loss: 0.0672714039683342\n",
            "----------------\n",
            "Loss:  0.7388877272605896 \n",
            "Classification loss: 1.395404577255249 \n",
            "Regression loss: 0.04118543863296509\n",
            "----------------\n",
            "Loss:  0.7207606583833694 \n",
            "Classification loss: 1.3236572742462158 \n",
            "Regression loss: 0.058932021260261536\n",
            "----------------\n",
            "Loss:  0.7739513628184795 \n",
            "Classification loss: 1.4553353786468506 \n",
            "Regression loss: 0.046283673495054245\n",
            "----------------\n",
            "Loss:  0.8545920327305794 \n",
            "Classification loss: 1.56243097782135 \n",
            "Regression loss: 0.07337654381990433\n",
            "----------------\n",
            "Loss:  0.670875858515501 \n",
            "Classification loss: 1.264711618423462 \n",
            "Regression loss: 0.038520049303770065\n",
            "----------------\n",
            "Loss:  0.8121823780238628 \n",
            "Classification loss: 1.5347058773040771 \n",
            "Regression loss: 0.044829439371824265\n",
            "----------------\n",
            "Loss:  0.7839546762406826 \n",
            "Classification loss: 1.4603946208953857 \n",
            "Regression loss: 0.05375736579298973\n",
            "----------------\n",
            "Loss:  0.6360335238277912 \n",
            "Classification loss: 1.17341947555542 \n",
            "Regression loss: 0.04932378605008125\n",
            "----------------\n",
            "Loss:  0.6538057997822762 \n",
            "Classification loss: 1.2356092929840088 \n",
            "Regression loss: 0.03600115329027176\n",
            "----------------\n",
            "Loss:  0.6523079797625542 \n",
            "Classification loss: 1.1958080530166626 \n",
            "Regression loss: 0.05440395325422287\n",
            "----------------\n",
            "Loss:  0.8096170127391815 \n",
            "Classification loss: 1.4884140491485596 \n",
            "Regression loss: 0.06540998816490173\n",
            "----------------\n",
            "Loss:  0.7988271191716194 \n",
            "Classification loss: 1.4812698364257812 \n",
            "Regression loss: 0.05819220095872879\n",
            "----------------\n",
            "Loss:  0.786371324211359 \n",
            "Classification loss: 1.467142939567566 \n",
            "Regression loss: 0.052799854427576065\n",
            "----------------\n",
            "Loss:  0.7483510822057724 \n",
            "Classification loss: 1.4136093854904175 \n",
            "Regression loss: 0.04154638946056366\n",
            "----------------\n",
            "Loss:  0.7424816004931927 \n",
            "Classification loss: 1.4097052812576294 \n",
            "Regression loss: 0.037628959864377975\n",
            "----------------\n",
            "Loss:  0.7551249861717224 \n",
            "Classification loss: 1.4177703857421875 \n",
            "Regression loss: 0.04623979330062866\n",
            "----------------\n",
            "Loss:  0.8214993998408318 \n",
            "Classification loss: 1.5540426969528198 \n",
            "Regression loss: 0.044478051364421844\n",
            "----------------\n",
            "Loss:  0.6236826106905937 \n",
            "Classification loss: 1.1657445430755615 \n",
            "Regression loss: 0.04081033915281296\n",
            "----------------\n",
            "Loss:  0.7119089737534523 \n",
            "Classification loss: 1.3293297290802002 \n",
            "Regression loss: 0.0472441092133522\n",
            "----------------\n",
            "Loss:  0.7784262038767338 \n",
            "Classification loss: 1.4689151048660278 \n",
            "Regression loss: 0.043968651443719864\n",
            "----------------\n",
            "Loss:  0.7433095537126064 \n",
            "Classification loss: 1.4136996269226074 \n",
            "Regression loss: 0.03645974025130272\n",
            "----------------\n",
            "Loss:  0.7164314091205597 \n",
            "Classification loss: 1.3632676601409912 \n",
            "Regression loss: 0.03479757905006409\n",
            "----------------\n",
            "Loss:  0.720201876014471 \n",
            "Classification loss: 1.3303053379058838 \n",
            "Regression loss: 0.05504920706152916\n",
            "----------------\n",
            "Loss:  0.7434189915657043 \n",
            "Classification loss: 1.4077290296554565 \n",
            "Regression loss: 0.039554476737976074\n",
            "----------------\n",
            "Loss:  0.9029580354690552 \n",
            "Classification loss: 1.6757750511169434 \n",
            "Regression loss: 0.0650705099105835\n",
            "----------------\n",
            "Loss:  0.8148461021482944 \n",
            "Classification loss: 1.5220425128936768 \n",
            "Regression loss: 0.05382484570145607\n",
            "----------------\n",
            "Loss:  0.6930261366069317 \n",
            "Classification loss: 1.2616052627563477 \n",
            "Regression loss: 0.06222350522875786\n",
            "----------------\n",
            "Loss:  0.7050553746521473 \n",
            "Classification loss: 1.286649227142334 \n",
            "Regression loss: 0.0617307610809803\n",
            "----------------\n",
            "Loss:  0.8028147481381893 \n",
            "Classification loss: 1.501847743988037 \n",
            "Regression loss: 0.05189087614417076\n",
            "----------------\n",
            "Loss:  0.6881450191140175 \n",
            "Classification loss: 1.3002409934997559 \n",
            "Regression loss: 0.03802452236413956\n",
            "----------------\n",
            "Loss:  0.72463358938694 \n",
            "Classification loss: 1.3796406984329224 \n",
            "Regression loss: 0.03481324017047882\n",
            "----------------\n",
            "Loss:  0.8619661703705788 \n",
            "Classification loss: 1.5791375637054443 \n",
            "Regression loss: 0.0723973885178566\n",
            "----------------\n",
            "Loss:  0.905042439699173 \n",
            "Classification loss: 1.7002379894256592 \n",
            "Regression loss: 0.054923444986343384\n",
            "----------------\n",
            "Loss:  0.7898076251149178 \n",
            "Classification loss: 1.4961328506469727 \n",
            "Regression loss: 0.04174119979143143\n",
            "----------------\n",
            "Loss:  0.669483132660389 \n",
            "Classification loss: 1.2605159282684326 \n",
            "Regression loss: 0.03922516852617264\n",
            "----------------\n",
            "Loss:  0.6980431377887726 \n",
            "Classification loss: 1.2777702808380127 \n",
            "Regression loss: 0.059157997369766235\n",
            "----------------\n",
            "Loss:  0.7821451239287853 \n",
            "Classification loss: 1.465382695198059 \n",
            "Regression loss: 0.04945377632975578\n",
            "----------------\n",
            "Loss:  0.6818320080637932 \n",
            "Classification loss: 1.2817964553833008 \n",
            "Regression loss: 0.04093378037214279\n",
            "----------------\n",
            "Loss:  0.734937995672226 \n",
            "Classification loss: 1.3833671808242798 \n",
            "Regression loss: 0.04325440526008606\n",
            "----------------\n",
            "Loss:  0.7725056186318398 \n",
            "Classification loss: 1.4431835412979126 \n",
            "Regression loss: 0.05091384798288345\n",
            "----------------\n",
            "Loss:  0.740990161895752 \n",
            "Classification loss: 1.3671666383743286 \n",
            "Regression loss: 0.057406842708587646\n",
            "----------------\n",
            "Loss:  0.7648799866437912 \n",
            "Classification loss: 1.437692642211914 \n",
            "Regression loss: 0.04603366553783417\n",
            "----------------\n",
            "Loss:  0.7765845134854317 \n",
            "Classification loss: 1.443593978881836 \n",
            "Regression loss: 0.0547875240445137\n",
            "----------------\n",
            "Loss:  0.6924950517714024 \n",
            "Classification loss: 1.2992703914642334 \n",
            "Regression loss: 0.04285985603928566\n",
            "----------------\n",
            "Loss:  0.7862722091376781 \n",
            "Classification loss: 1.504050374031067 \n",
            "Regression loss: 0.0342470221221447\n",
            "----------------\n",
            "Loss:  0.7431915253400803 \n",
            "Classification loss: 1.3656494617462158 \n",
            "Regression loss: 0.06036679446697235\n",
            "----------------\n",
            "Loss:  0.7977171242237091 \n",
            "Classification loss: 1.4795533418655396 \n",
            "Regression loss: 0.05794045329093933\n",
            "----------------\n",
            "Loss:  0.8915784135460854 \n",
            "Classification loss: 1.669344186782837 \n",
            "Regression loss: 0.0569063201546669\n",
            "----------------\n",
            "Loss:  0.6689117774367332 \n",
            "Classification loss: 1.2235018014907837 \n",
            "Regression loss: 0.0571608766913414\n",
            "----------------\n",
            "Loss:  0.7374365478754044 \n",
            "Classification loss: 1.3662060499191284 \n",
            "Regression loss: 0.05433352291584015\n",
            "----------------\n",
            "Loss:  0.6966328397393227 \n",
            "Classification loss: 1.329486608505249 \n",
            "Regression loss: 0.03188953548669815\n",
            "----------------\n",
            "Loss:  0.7422192841768265 \n",
            "Classification loss: 1.357893943786621 \n",
            "Regression loss: 0.06327231228351593\n",
            "----------------\n",
            "Loss:  0.816824421286583 \n",
            "Classification loss: 1.5236599445343018 \n",
            "Regression loss: 0.05499444901943207\n",
            "----------------\n",
            "Loss:  0.759231761097908 \n",
            "Classification loss: 1.4425342082977295 \n",
            "Regression loss: 0.037964656949043274\n",
            "----------------\n",
            "Loss:  0.7437821999192238 \n",
            "Classification loss: 1.3848822116851807 \n",
            "Regression loss: 0.05134109407663345\n",
            "----------------\n",
            "Loss:  0.6417111679911613 \n",
            "Classification loss: 1.196890115737915 \n",
            "Regression loss: 0.04326611012220383\n",
            "----------------\n",
            "Loss:  0.8167073577642441 \n",
            "Classification loss: 1.495861291885376 \n",
            "Regression loss: 0.06877671182155609\n",
            "----------------\n",
            "Loss:  0.6746805310249329 \n",
            "Classification loss: 1.2570133209228516 \n",
            "Regression loss: 0.04617387056350708\n",
            "----------------\n",
            "Loss:  0.8007315397262573 \n",
            "Classification loss: 1.464534044265747 \n",
            "Regression loss: 0.06846451759338379\n",
            "----------------\n",
            "Loss:  0.724168136715889 \n",
            "Classification loss: 1.3820394277572632 \n",
            "Regression loss: 0.033148422837257385\n",
            "----------------\n",
            "Loss:  0.8584659323096275 \n",
            "Classification loss: 1.5681158304214478 \n",
            "Regression loss: 0.07440801709890366\n",
            "----------------\n",
            "Loss:  0.7382660582661629 \n",
            "Classification loss: 1.3349822759628296 \n",
            "Regression loss: 0.07077492028474808\n",
            "----------------\n",
            "Loss:  0.7970877960324287 \n",
            "Classification loss: 1.4944796562194824 \n",
            "Regression loss: 0.04984796792268753\n",
            "----------------\n",
            "Loss:  0.7308032996952534 \n",
            "Classification loss: 1.360594630241394 \n",
            "Regression loss: 0.05050598457455635\n",
            "----------------\n",
            "Loss:  0.6068875603377819 \n",
            "Classification loss: 1.1446118354797363 \n",
            "Regression loss: 0.03458164259791374\n",
            "----------------\n",
            "Loss:  0.9179370403289795 \n",
            "Classification loss: 1.6840195655822754 \n",
            "Regression loss: 0.0759272575378418\n",
            "----------------\n",
            "Loss:  0.6973935794085264 \n",
            "Classification loss: 1.3358418941497803 \n",
            "Regression loss: 0.029472632333636284\n",
            "----------------\n",
            "Loss:  0.7309547290205956 \n",
            "Classification loss: 1.366335391998291 \n",
            "Regression loss: 0.04778703302145004\n",
            "----------------\n",
            "Loss:  0.7493980079889297 \n",
            "Classification loss: 1.3692727088928223 \n",
            "Regression loss: 0.06476165354251862\n",
            "----------------\n",
            "Loss:  0.6766472868621349 \n",
            "Classification loss: 1.2842342853546143 \n",
            "Regression loss: 0.034530144184827805\n",
            "----------------\n",
            "Loss:  0.7898046523332596 \n",
            "Classification loss: 1.440025806427002 \n",
            "Regression loss: 0.0697917491197586\n",
            "----------------\n",
            "Loss:  0.673483744263649 \n",
            "Classification loss: 1.2564035654067993 \n",
            "Regression loss: 0.04528196156024933\n",
            "----------------\n",
            "Loss:  0.6760670393705368 \n",
            "Classification loss: 1.256277322769165 \n",
            "Regression loss: 0.047928377985954285\n",
            "----------------\n",
            "Loss:  0.77076106518507 \n",
            "Classification loss: 1.449756383895874 \n",
            "Regression loss: 0.045882873237133026\n",
            "----------------\n",
            "Loss:  0.7107267081737518 \n",
            "Classification loss: 1.3438544273376465 \n",
            "Regression loss: 0.03879949450492859\n",
            "----------------\n",
            "Loss:  0.712003331631422 \n",
            "Classification loss: 1.3085265159606934 \n",
            "Regression loss: 0.05774007365107536\n",
            "----------------\n",
            "Loss:  0.7196076922118664 \n",
            "Classification loss: 1.352550745010376 \n",
            "Regression loss: 0.04333231970667839\n",
            "----------------\n",
            "Loss:  0.6715821847319603 \n",
            "Classification loss: 1.2572524547576904 \n",
            "Regression loss: 0.04295595735311508\n",
            "----------------\n",
            "Loss:  0.8386393338441849 \n",
            "Classification loss: 1.5378936529159546 \n",
            "Regression loss: 0.06969250738620758\n",
            "----------------\n",
            "Loss:  0.6854032203555107 \n",
            "Classification loss: 1.2769138813018799 \n",
            "Regression loss: 0.04694627970457077\n",
            "----------------\n",
            "Loss:  0.7076108641922474 \n",
            "Classification loss: 1.3432502746582031 \n",
            "Regression loss: 0.03598572686314583\n",
            "----------------\n",
            "Loss:  0.7995840236544609 \n",
            "Classification loss: 1.5089277029037476 \n",
            "Regression loss: 0.04512017220258713\n",
            "----------------\n",
            "Loss:  0.7474266961216927 \n",
            "Classification loss: 1.4228554964065552 \n",
            "Regression loss: 0.03599894791841507\n",
            "----------------\n",
            "Loss:  0.7092291414737701 \n",
            "Classification loss: 1.3342173099517822 \n",
            "Regression loss: 0.04212048649787903\n",
            "----------------\n",
            "Loss:  0.6101277805864811 \n",
            "Classification loss: 1.1647822856903076 \n",
            "Regression loss: 0.027736637741327286\n",
            "----------------\n",
            "Loss:  0.8007424250245094 \n",
            "Classification loss: 1.4607219696044922 \n",
            "Regression loss: 0.07038144022226334\n",
            "----------------\n",
            "Loss:  0.7704795300960541 \n",
            "Classification loss: 1.4488052129745483 \n",
            "Regression loss: 0.04607692360877991\n",
            "----------------\n",
            "Loss:  0.7161661423742771 \n",
            "Classification loss: 1.3076010942459106 \n",
            "Regression loss: 0.06236559525132179\n",
            "----------------\n",
            "Loss:  0.5964049436151981 \n",
            "Classification loss: 1.1197572946548462 \n",
            "Regression loss: 0.03652629628777504\n",
            "----------------\n",
            "Loss:  0.5565221440047026 \n",
            "Classification loss: 1.0537666082382202 \n",
            "Regression loss: 0.02963883988559246\n",
            "----------------\n",
            "Loss:  0.7627714686095715 \n",
            "Classification loss: 1.431646704673767 \n",
            "Regression loss: 0.04694811627268791\n",
            "----------------\n",
            "Loss:  0.845319326967001 \n",
            "Classification loss: 1.5941249132156372 \n",
            "Regression loss: 0.04825687035918236\n",
            "----------------\n",
            "Loss:  0.7580281645059586 \n",
            "Classification loss: 1.419105052947998 \n",
            "Regression loss: 0.048475638031959534\n",
            "----------------\n",
            "Loss:  0.6487960815429688 \n",
            "Classification loss: 1.2238906621932983 \n",
            "Regression loss: 0.03685075044631958\n",
            "----------------\n",
            "Loss:  0.734860198572278 \n",
            "Classification loss: 1.4234906435012817 \n",
            "Regression loss: 0.023114876821637154\n",
            "----------------\n",
            "Loss:  0.7656654454767704 \n",
            "Classification loss: 1.4200632572174072 \n",
            "Regression loss: 0.05563381686806679\n",
            "----------------\n",
            "Loss:  0.7544859647750854 \n",
            "Classification loss: 1.384731411933899 \n",
            "Regression loss: 0.062120258808135986\n",
            "----------------\n",
            "Loss:  0.6604652963578701 \n",
            "Classification loss: 1.2512896060943604 \n",
            "Regression loss: 0.034820493310689926\n",
            "----------------\n",
            "Loss:  0.8270344510674477 \n",
            "Classification loss: 1.4879553318023682 \n",
            "Regression loss: 0.08305678516626358\n",
            "----------------\n",
            "Loss:  0.6858502551913261 \n",
            "Classification loss: 1.2334179878234863 \n",
            "Regression loss: 0.06914126127958298\n",
            "----------------\n",
            "Loss:  0.6244171224534512 \n",
            "Classification loss: 1.162071704864502 \n",
            "Regression loss: 0.04338127002120018\n",
            "----------------\n",
            "Loss:  0.7634396702051163 \n",
            "Classification loss: 1.4259839057922363 \n",
            "Regression loss: 0.05044771730899811\n",
            "----------------\n",
            "Loss:  0.7498609386384487 \n",
            "Classification loss: 1.4185559749603271 \n",
            "Regression loss: 0.04058295115828514\n",
            "----------------\n",
            "Loss:  0.7155809998512268 \n",
            "Classification loss: 1.326500654220581 \n",
            "Regression loss: 0.05233067274093628\n",
            "----------------\n",
            "Loss:  0.7642845325171947 \n",
            "Classification loss: 1.4320693016052246 \n",
            "Regression loss: 0.04824988171458244\n",
            "----------------\n",
            "Loss:  0.8493735417723656 \n",
            "Classification loss: 1.565477728843689 \n",
            "Regression loss: 0.06663467735052109\n",
            "----------------\n",
            "Loss:  0.7669606357812881 \n",
            "Classification loss: 1.4579882621765137 \n",
            "Regression loss: 0.03796650469303131\n",
            "----------------\n",
            "Loss:  0.7323284298181534 \n",
            "Classification loss: 1.37247633934021 \n",
            "Regression loss: 0.0460902601480484\n",
            "----------------\n",
            "Loss:  0.8030327446758747 \n",
            "Classification loss: 1.495529294013977 \n",
            "Regression loss: 0.055268097668886185\n",
            "----------------\n",
            "Loss:  0.7663128655403852 \n",
            "Classification loss: 1.4842865467071533 \n",
            "Regression loss: 0.024169592186808586\n",
            "----------------\n",
            "Loss:  0.6051803268492222 \n",
            "Classification loss: 1.1567617654800415 \n",
            "Regression loss: 0.02679944410920143\n",
            "----------------\n",
            "Loss:  0.7747611925005913 \n",
            "Classification loss: 1.463108777999878 \n",
            "Regression loss: 0.04320680350065231\n",
            "----------------\n",
            "Loss:  0.6884491480886936 \n",
            "Classification loss: 1.2973146438598633 \n",
            "Regression loss: 0.03979182615876198\n",
            "----------------\n",
            "Loss:  0.7055654227733612 \n",
            "Classification loss: 1.3639144897460938 \n",
            "Regression loss: 0.02360817790031433\n",
            "----------------\n",
            "Loss:  0.6664984449744225 \n",
            "Classification loss: 1.257206916809082 \n",
            "Regression loss: 0.03789498656988144\n",
            "----------------\n",
            "Loss:  0.6978000327944756 \n",
            "Classification loss: 1.251281976699829 \n",
            "Regression loss: 0.072159044444561\n",
            "----------------\n",
            "Loss:  0.6975584924221039 \n",
            "Classification loss: 1.306309461593628 \n",
            "Regression loss: 0.04440376162528992\n",
            "----------------\n",
            "Loss:  0.7542149089276791 \n",
            "Classification loss: 1.4238884449005127 \n",
            "Regression loss: 0.042270686477422714\n",
            "----------------\n",
            "Loss:  0.7762378454208374 \n",
            "Classification loss: 1.4757628440856934 \n",
            "Regression loss: 0.03835642337799072\n",
            "----------------\n",
            "Loss:  0.801367923617363 \n",
            "Classification loss: 1.469323992729187 \n",
            "Regression loss: 0.06670592725276947\n",
            "----------------\n",
            "Loss:  0.7453792318701744 \n",
            "Classification loss: 1.426058053970337 \n",
            "Regression loss: 0.03235020488500595\n",
            "----------------\n",
            "Loss:  0.7533369697630405 \n",
            "Classification loss: 1.4132604598999023 \n",
            "Regression loss: 0.04670673981308937\n",
            "----------------\n",
            "Loss:  0.6835603192448616 \n",
            "Classification loss: 1.2605328559875488 \n",
            "Regression loss: 0.05329389125108719\n",
            "----------------\n",
            "Loss:  0.6769625768065453 \n",
            "Classification loss: 1.2570123672485352 \n",
            "Regression loss: 0.04845639318227768\n",
            "----------------\n",
            "Loss:  0.685131162405014 \n",
            "Classification loss: 1.2880650758743286 \n",
            "Regression loss: 0.04109862446784973\n",
            "----------------\n",
            "Loss:  0.6836563125252724 \n",
            "Classification loss: 1.2792524099349976 \n",
            "Regression loss: 0.04403010755777359\n",
            "----------------\n",
            "Loss:  0.7440689578652382 \n",
            "Classification loss: 1.38434898853302 \n",
            "Regression loss: 0.05189446359872818\n",
            "----------------\n",
            "Loss:  0.6802862994372845 \n",
            "Classification loss: 1.2929171323776245 \n",
            "Regression loss: 0.033827733248472214\n",
            "----------------\n",
            "Loss:  0.8397055752575397 \n",
            "Classification loss: 1.573630928993225 \n",
            "Regression loss: 0.0528901107609272\n",
            "----------------\n",
            "Loss:  0.7519886903464794 \n",
            "Classification loss: 1.4150737524032593 \n",
            "Regression loss: 0.04445181414484978\n",
            "----------------\n",
            "Loss:  0.7502671293914318 \n",
            "Classification loss: 1.4121934175491333 \n",
            "Regression loss: 0.04417042061686516\n",
            "----------------\n",
            "Loss:  0.6601045057177544 \n",
            "Classification loss: 1.2529981136322021 \n",
            "Regression loss: 0.03360544890165329\n",
            "----------------\n",
            "Loss:  0.6538115367293358 \n",
            "Classification loss: 1.2391566038131714 \n",
            "Regression loss: 0.03423323482275009\n",
            "----------------\n",
            "Loss:  0.7020141966640949 \n",
            "Classification loss: 1.3216114044189453 \n",
            "Regression loss: 0.04120849445462227\n",
            "----------------\n",
            "Loss:  0.6786120682954788 \n",
            "Classification loss: 1.2681673765182495 \n",
            "Regression loss: 0.044528380036354065\n",
            "----------------\n",
            "Loss:  0.667299672961235 \n",
            "Classification loss: 1.2658448219299316 \n",
            "Regression loss: 0.034377261996269226\n",
            "----------------\n",
            "Loss:  0.7319844514131546 \n",
            "Classification loss: 1.3736300468444824 \n",
            "Regression loss: 0.04516942799091339\n",
            "----------------\n",
            "Loss:  0.7549056112766266 \n",
            "Classification loss: 1.3641886711120605 \n",
            "Regression loss: 0.07281127572059631\n",
            "----------------\n",
            "Loss:  0.8943064212799072 \n",
            "Classification loss: 1.6766587495803833 \n",
            "Regression loss: 0.055977046489715576\n",
            "----------------\n",
            "Loss:  0.7010001838207245 \n",
            "Classification loss: 1.3194937705993652 \n",
            "Regression loss: 0.04125329852104187\n",
            "----------------\n",
            "Loss:  0.723573349416256 \n",
            "Classification loss: 1.3296059370040894 \n",
            "Regression loss: 0.05877038091421127\n",
            "----------------\n",
            "Loss:  0.5980370081961155 \n",
            "Classification loss: 1.100798487663269 \n",
            "Regression loss: 0.04763776436448097\n",
            "----------------\n",
            "Loss:  0.7568371668457985 \n",
            "Classification loss: 1.417360782623291 \n",
            "Regression loss: 0.048156775534152985\n",
            "----------------\n",
            "Loss:  0.7708993330597878 \n",
            "Classification loss: 1.4687480926513672 \n",
            "Regression loss: 0.036525286734104156\n",
            "----------------\n",
            "Loss:  0.7391013950109482 \n",
            "Classification loss: 1.3302292823791504 \n",
            "Regression loss: 0.07398675382137299\n",
            "----------------\n",
            "Loss:  0.7083011642098427 \n",
            "Classification loss: 1.315558671951294 \n",
            "Regression loss: 0.05052182823419571\n",
            "----------------\n",
            "Loss:  0.7694128975272179 \n",
            "Classification loss: 1.4156031608581543 \n",
            "Regression loss: 0.06161131709814072\n",
            "----------------\n",
            "Loss:  0.8210159577429295 \n",
            "Classification loss: 1.5388190746307373 \n",
            "Regression loss: 0.051606420427560806\n",
            "----------------\n",
            "Loss:  0.7475499846041203 \n",
            "Classification loss: 1.446596384048462 \n",
            "Regression loss: 0.024251792579889297\n",
            "----------------\n",
            "Loss:  0.7356957048177719 \n",
            "Classification loss: 1.393174409866333 \n",
            "Regression loss: 0.03910849988460541\n",
            "----------------\n",
            "Loss:  0.6680169776082039 \n",
            "Classification loss: 1.2447502613067627 \n",
            "Regression loss: 0.04564184695482254\n",
            "----------------\n",
            "Loss:  0.8484151661396027 \n",
            "Classification loss: 1.5870740413665771 \n",
            "Regression loss: 0.05487814545631409\n",
            "----------------\n",
            "Loss:  0.7611844465136528 \n",
            "Classification loss: 1.4222335815429688 \n",
            "Regression loss: 0.050067655742168427\n",
            "----------------\n",
            "Loss:  0.7425643429160118 \n",
            "Classification loss: 1.411344289779663 \n",
            "Regression loss: 0.03689219802618027\n",
            "----------------\n",
            "Loss:  0.7177136242389679 \n",
            "Classification loss: 1.3213331699371338 \n",
            "Regression loss: 0.057047039270401\n",
            "----------------\n",
            "Loss:  0.6923441290855408 \n",
            "Classification loss: 1.286237120628357 \n",
            "Regression loss: 0.049225568771362305\n",
            "----------------\n",
            "Loss:  0.6408097371459007 \n",
            "Classification loss: 1.1839525699615479 \n",
            "Regression loss: 0.0488334521651268\n",
            "----------------\n",
            "Loss:  0.7308181524276733 \n",
            "Classification loss: 1.3805433511734009 \n",
            "Regression loss: 0.0405464768409729\n",
            "----------------\n",
            "Loss:  0.6853917725384235 \n",
            "Classification loss: 1.2951024770736694 \n",
            "Regression loss: 0.03784053400158882\n",
            "----------------\n",
            "Loss:  0.7140646204352379 \n",
            "Classification loss: 1.345710277557373 \n",
            "Regression loss: 0.04120948165655136\n",
            "----------------\n",
            "Loss:  0.6996294930577278 \n",
            "Classification loss: 1.3344573974609375 \n",
            "Regression loss: 0.032400794327259064\n",
            "----------------\n",
            "Loss:  0.6033240966498852 \n",
            "Classification loss: 1.1146950721740723 \n",
            "Regression loss: 0.045976560562849045\n",
            "----------------\n",
            "Loss:  0.7887611724436283 \n",
            "Classification loss: 1.4625065326690674 \n",
            "Regression loss: 0.05750790610909462\n",
            "----------------\n",
            "Loss:  0.6530108004808426 \n",
            "Classification loss: 1.180594563484192 \n",
            "Regression loss: 0.06271351873874664\n",
            "----------------\n",
            "Loss:  0.7537472881376743 \n",
            "Classification loss: 1.420311689376831 \n",
            "Regression loss: 0.043591443449258804\n",
            "----------------\n",
            "Loss:  0.7488220185041428 \n",
            "Classification loss: 1.3723406791687012 \n",
            "Regression loss: 0.06265167891979218\n",
            "----------------\n",
            "Loss:  0.7651411220431328 \n",
            "Classification loss: 1.460947036743164 \n",
            "Regression loss: 0.03466760367155075\n",
            "----------------\n",
            "Loss:  0.7296844907104969 \n",
            "Classification loss: 1.3499795198440552 \n",
            "Regression loss: 0.054694730788469315\n",
            "----------------\n",
            "Loss:  0.8352707922458649 \n",
            "Classification loss: 1.5726261138916016 \n",
            "Regression loss: 0.04895773530006409\n",
            "----------------\n",
            "Loss:  0.797088798135519 \n",
            "Classification loss: 1.5125720500946045 \n",
            "Regression loss: 0.04080277308821678\n",
            "----------------\n",
            "Loss:  0.6808265447616577 \n",
            "Classification loss: 1.2623732089996338 \n",
            "Regression loss: 0.04963994026184082\n",
            "----------------\n",
            "Loss:  0.7875503227114677 \n",
            "Classification loss: 1.4918996095657349 \n",
            "Regression loss: 0.04160051792860031\n",
            "----------------\n",
            "Loss:  0.7733136899769306 \n",
            "Classification loss: 1.4787980318069458 \n",
            "Regression loss: 0.03391467407345772\n",
            "----------------\n",
            "Loss:  0.7679660879075527 \n",
            "Classification loss: 1.4334064722061157 \n",
            "Regression loss: 0.05126285180449486\n",
            "----------------\n",
            "Loss:  0.7669808007776737 \n",
            "Classification loss: 1.4730383157730103 \n",
            "Regression loss: 0.030461642891168594\n",
            "----------------\n",
            "Loss:  0.7478368803858757 \n",
            "Classification loss: 1.3977203369140625 \n",
            "Regression loss: 0.04897671192884445\n",
            "----------------\n",
            "Loss:  0.6623577140271664 \n",
            "Classification loss: 1.2475281953811646 \n",
            "Regression loss: 0.03859361633658409\n",
            "----------------\n",
            "Loss:  0.820087805390358 \n",
            "Classification loss: 1.5125254392623901 \n",
            "Regression loss: 0.0638250857591629\n",
            "----------------\n",
            "Loss:  0.9468931406736374 \n",
            "Classification loss: 1.7000226974487305 \n",
            "Regression loss: 0.09688179194927216\n",
            "----------------\n",
            "Loss:  0.782826628535986 \n",
            "Classification loss: 1.461524486541748 \n",
            "Regression loss: 0.05206438526511192\n",
            "----------------\n",
            "Loss:  0.7772034183144569 \n",
            "Classification loss: 1.4623281955718994 \n",
            "Regression loss: 0.04603932052850723\n",
            "----------------\n",
            "Loss:  0.7296111136674881 \n",
            "Classification loss: 1.3696820735931396 \n",
            "Regression loss: 0.044770076870918274\n",
            "----------------\n",
            "Loss:  0.7776887640357018 \n",
            "Classification loss: 1.4719247817993164 \n",
            "Regression loss: 0.04172637313604355\n",
            "----------------\n",
            "Loss:  0.7426855191588402 \n",
            "Classification loss: 1.4435672760009766 \n",
            "Regression loss: 0.020901881158351898\n",
            "----------------\n",
            "Loss:  0.71361811645329 \n",
            "Classification loss: 1.3724066019058228 \n",
            "Regression loss: 0.02741481550037861\n",
            "----------------\n",
            "Loss:  0.7741157859563828 \n",
            "Classification loss: 1.4642261266708374 \n",
            "Regression loss: 0.04200272262096405\n",
            "----------------\n",
            "Loss:  0.817339263856411 \n",
            "Classification loss: 1.519566535949707 \n",
            "Regression loss: 0.057555995881557465\n",
            "----------------\n",
            "Loss:  0.7584775984287262 \n",
            "Classification loss: 1.429145097732544 \n",
            "Regression loss: 0.043905049562454224\n",
            "----------------\n",
            "Loss:  0.7143424227833748 \n",
            "Classification loss: 1.3068397045135498 \n",
            "Regression loss: 0.060922570526599884\n",
            "----------------\n",
            "Loss:  0.7564385011792183 \n",
            "Classification loss: 1.4226406812667847 \n",
            "Regression loss: 0.04511816054582596\n",
            "----------------\n",
            "Loss:  0.6594677194952965 \n",
            "Classification loss: 1.2473111152648926 \n",
            "Regression loss: 0.03581216186285019\n",
            "----------------\n",
            "Loss:  0.7295639738440514 \n",
            "Classification loss: 1.3826879262924194 \n",
            "Regression loss: 0.038220010697841644\n",
            "----------------\n",
            "Loss:  0.6915233582258224 \n",
            "Classification loss: 1.3086696863174438 \n",
            "Regression loss: 0.037188515067100525\n",
            "----------------\n",
            "Loss:  0.8847723603248596 \n",
            "Classification loss: 1.6408941745758057 \n",
            "Regression loss: 0.06432527303695679\n",
            "----------------\n",
            "Loss:  0.7529927156865597 \n",
            "Classification loss: 1.411672592163086 \n",
            "Regression loss: 0.04715641960501671\n",
            "----------------\n",
            "Loss:  0.6770893335342407 \n",
            "Classification loss: 1.2885425090789795 \n",
            "Regression loss: 0.03281807899475098\n",
            "----------------\n",
            "Loss:  0.7024880461394787 \n",
            "Classification loss: 1.3405532836914062 \n",
            "Regression loss: 0.03221140429377556\n",
            "----------------\n",
            "Loss:  0.7128941603004932 \n",
            "Classification loss: 1.3225007057189941 \n",
            "Regression loss: 0.05164380744099617\n",
            "----------------\n",
            "Loss:  0.7456423491239548 \n",
            "Classification loss: 1.4033656120300293 \n",
            "Regression loss: 0.043959543108940125\n",
            "----------------\n",
            "Loss:  0.7840753197669983 \n",
            "Classification loss: 1.45442533493042 \n",
            "Regression loss: 0.05686265230178833\n",
            "----------------\n",
            "Loss:  0.6442521698772907 \n",
            "Classification loss: 1.2223509550094604 \n",
            "Regression loss: 0.0330766923725605\n",
            "----------------\n",
            "Loss:  0.7766077853739262 \n",
            "Classification loss: 1.446031093597412 \n",
            "Regression loss: 0.05359223857522011\n",
            "----------------\n",
            "Loss:  0.7402623556554317 \n",
            "Classification loss: 1.3882415294647217 \n",
            "Regression loss: 0.04614159092307091\n",
            "----------------\n",
            "Loss:  0.7626949474215508 \n",
            "Classification loss: 1.4418962001800537 \n",
            "Regression loss: 0.041746847331523895\n",
            "----------------\n",
            "Loss:  0.7878566905856133 \n",
            "Classification loss: 1.4677444696426392 \n",
            "Regression loss: 0.05398445576429367\n",
            "----------------\n",
            "Loss:  0.7319918051362038 \n",
            "Classification loss: 1.3413991928100586 \n",
            "Regression loss: 0.06129220873117447\n",
            "----------------\n",
            "Loss:  0.7978784516453743 \n",
            "Classification loss: 1.4627883434295654 \n",
            "Regression loss: 0.06648427993059158\n",
            "----------------\n",
            "Loss:  0.8257095534354448 \n",
            "Classification loss: 1.5901402235031128 \n",
            "Regression loss: 0.030639441683888435\n",
            "----------------\n",
            "Loss:  0.6425959840416908 \n",
            "Classification loss: 1.1852025985717773 \n",
            "Regression loss: 0.049994684755802155\n",
            "----------------\n",
            "Loss:  0.7538770139217377 \n",
            "Classification loss: 1.3934489488601685 \n",
            "Regression loss: 0.05715253949165344\n",
            "----------------\n",
            "Loss:  0.7465252727270126 \n",
            "Classification loss: 1.3936606645584106 \n",
            "Regression loss: 0.04969494044780731\n",
            "----------------\n",
            "Loss:  0.6891227401793003 \n",
            "Classification loss: 1.311051607131958 \n",
            "Regression loss: 0.033596936613321304\n",
            "----------------\n",
            "Loss:  0.8270043171942234 \n",
            "Classification loss: 1.5731325149536133 \n",
            "Regression loss: 0.04043805971741676\n",
            "----------------\n",
            "Loss:  0.7845320105552673 \n",
            "Classification loss: 1.479191780090332 \n",
            "Regression loss: 0.04493612051010132\n",
            "----------------\n",
            "Loss:  0.88435984775424 \n",
            "Classification loss: 1.6728601455688477 \n",
            "Regression loss: 0.04792977496981621\n",
            "----------------\n",
            "Loss:  0.7853235751390457 \n",
            "Classification loss: 1.4343888759613037 \n",
            "Regression loss: 0.06812913715839386\n",
            "----------------\n",
            "Loss:  0.7813830375671387 \n",
            "Classification loss: 1.399571418762207 \n",
            "Regression loss: 0.08159732818603516\n",
            "----------------\n",
            "Loss:  0.7127177566289902 \n",
            "Classification loss: 1.2914141416549683 \n",
            "Regression loss: 0.06701068580150604\n",
            "----------------\n",
            "Loss:  0.7678681127727032 \n",
            "Classification loss: 1.4399259090423584 \n",
            "Regression loss: 0.04790515825152397\n",
            "----------------\n",
            "Loss:  0.6705494076013565 \n",
            "Classification loss: 1.2733100652694702 \n",
            "Regression loss: 0.0338943749666214\n",
            "----------------\n",
            "Loss:  0.853782132267952 \n",
            "Classification loss: 1.6016696691513062 \n",
            "Regression loss: 0.05294729769229889\n",
            "----------------\n",
            "Loss:  0.814348928630352 \n",
            "Classification loss: 1.5017473697662354 \n",
            "Regression loss: 0.06347524374723434\n",
            "----------------\n",
            "Loss:  0.848253034055233 \n",
            "Classification loss: 1.5841933488845825 \n",
            "Regression loss: 0.05615635961294174\n",
            "----------------\n",
            "Loss:  0.6387005038559437 \n",
            "Classification loss: 1.1964662075042725 \n",
            "Regression loss: 0.04046740010380745\n",
            "----------------\n",
            "Loss:  0.6353405099362135 \n",
            "Classification loss: 1.21527099609375 \n",
            "Regression loss: 0.027705011889338493\n",
            "----------------\n",
            "Loss:  0.792048342525959 \n",
            "Classification loss: 1.4934241771697998 \n",
            "Regression loss: 0.04533625394105911\n",
            "----------------\n",
            "Loss:  0.7809849381446838 \n",
            "Classification loss: 1.4338271617889404 \n",
            "Regression loss: 0.06407135725021362\n",
            "----------------\n",
            "Loss:  0.7490168623626232 \n",
            "Classification loss: 1.395348310470581 \n",
            "Regression loss: 0.05134270712733269\n",
            "----------------\n",
            "Loss:  0.7804510518908501 \n",
            "Classification loss: 1.4542021751403809 \n",
            "Regression loss: 0.05334996432065964\n",
            "----------------\n",
            "Loss:  0.6997816115617752 \n",
            "Classification loss: 1.327514410018921 \n",
            "Regression loss: 0.03602440655231476\n",
            "----------------\n",
            "Loss:  0.8265858739614487 \n",
            "Classification loss: 1.4891852140426636 \n",
            "Regression loss: 0.08199326694011688\n",
            "----------------\n",
            "Loss:  0.807349156588316 \n",
            "Classification loss: 1.5225696563720703 \n",
            "Regression loss: 0.04606432840228081\n",
            "----------------\n",
            "Loss:  0.7874395251274109 \n",
            "Classification loss: 1.4460252523422241 \n",
            "Regression loss: 0.06442689895629883\n",
            "----------------\n",
            "Loss:  0.682247057557106 \n",
            "Classification loss: 1.2650494575500488 \n",
            "Regression loss: 0.049722328782081604\n",
            "----------------\n",
            "Loss:  0.7809955589473248 \n",
            "Classification loss: 1.442012071609497 \n",
            "Regression loss: 0.05998952314257622\n",
            "----------------\n",
            "Loss:  0.7059291303157806 \n",
            "Classification loss: 1.2977416515350342 \n",
            "Regression loss: 0.05705830454826355\n",
            "----------------\n",
            "Loss:  0.6498405039310455 \n",
            "Classification loss: 1.1656678915023804 \n",
            "Regression loss: 0.06700655817985535\n",
            "----------------\n",
            "Loss:  0.7562157697975636 \n",
            "Classification loss: 1.4167393445968628 \n",
            "Regression loss: 0.047846097499132156\n",
            "----------------\n",
            "Loss:  0.7304608039557934 \n",
            "Classification loss: 1.403639554977417 \n",
            "Regression loss: 0.028641026467084885\n",
            "----------------\n",
            "Loss:  0.7912205532193184 \n",
            "Classification loss: 1.508387565612793 \n",
            "Regression loss: 0.037026770412921906\n",
            "----------------\n",
            "Loss:  0.760089784860611 \n",
            "Classification loss: 1.3750048875808716 \n",
            "Regression loss: 0.07258734107017517\n",
            "----------------\n",
            "Loss:  0.728808157145977 \n",
            "Classification loss: 1.351395606994629 \n",
            "Regression loss: 0.05311035364866257\n",
            "----------------\n",
            "Loss:  0.7006561420857906 \n",
            "Classification loss: 1.2795112133026123 \n",
            "Regression loss: 0.06090053543448448\n",
            "----------------\n",
            "Loss:  0.7199262119829655 \n",
            "Classification loss: 1.3748681545257568 \n",
            "Regression loss: 0.03249213472008705\n",
            "----------------\n",
            "Loss:  0.788985975086689 \n",
            "Classification loss: 1.477006435394287 \n",
            "Regression loss: 0.05048275738954544\n",
            "----------------\n",
            "Loss:  0.7521283980458975 \n",
            "Classification loss: 1.451730728149414 \n",
            "Regression loss: 0.026263033971190453\n",
            "----------------\n",
            "Loss:  0.8065185658633709 \n",
            "Classification loss: 1.4950125217437744 \n",
            "Regression loss: 0.05901230499148369\n",
            "----------------\n",
            "Loss:  0.7323362305760384 \n",
            "Classification loss: 1.3596261739730835 \n",
            "Regression loss: 0.05252314358949661\n",
            "----------------\n",
            "Loss:  0.7872149646282196 \n",
            "Classification loss: 1.524724006652832 \n",
            "Regression loss: 0.02485296130180359\n",
            "----------------\n",
            "Loss:  0.6273631788790226 \n",
            "Classification loss: 1.157053828239441 \n",
            "Regression loss: 0.04883626475930214\n",
            "----------------\n",
            "Loss:  0.6289747506380081 \n",
            "Classification loss: 1.1766079664230347 \n",
            "Regression loss: 0.040670767426490784\n",
            "----------------\n",
            "Loss:  0.7276611402630806 \n",
            "Classification loss: 1.3624904155731201 \n",
            "Regression loss: 0.04641593247652054\n",
            "----------------\n",
            "Loss:  0.6733744964003563 \n",
            "Classification loss: 1.2834393978118896 \n",
            "Regression loss: 0.03165479749441147\n",
            "----------------\n",
            "Loss:  0.7246956527233124 \n",
            "Classification loss: 1.3373717069625854 \n",
            "Regression loss: 0.05600979924201965\n",
            "----------------\n",
            "Loss:  0.867371641099453 \n",
            "Classification loss: 1.5845483541488647 \n",
            "Regression loss: 0.0750974640250206\n",
            "----------------\n",
            "Loss:  0.6592409908771515 \n",
            "Classification loss: 1.2591184377670288 \n",
            "Regression loss: 0.029681771993637085\n",
            "----------------\n",
            "Loss:  0.6442328132688999 \n",
            "Classification loss: 1.166487455368042 \n",
            "Regression loss: 0.06098908558487892\n",
            "----------------\n",
            "Loss:  0.7136610634624958 \n",
            "Classification loss: 1.36411714553833 \n",
            "Regression loss: 0.031602490693330765\n",
            "----------------\n",
            "Loss:  0.6953664757311344 \n",
            "Classification loss: 1.2976253032684326 \n",
            "Regression loss: 0.046553824096918106\n",
            "----------------\n",
            "Loss:  0.6239052638411522 \n",
            "Classification loss: 1.1723899841308594 \n",
            "Regression loss: 0.037710271775722504\n",
            "----------------\n",
            "Loss:  0.8036077618598938 \n",
            "Classification loss: 1.4317781925201416 \n",
            "Regression loss: 0.087718665599823\n",
            "----------------\n",
            "Loss:  0.7683744058012962 \n",
            "Classification loss: 1.451409101486206 \n",
            "Regression loss: 0.04266985505819321\n",
            "----------------\n",
            "Loss:  0.7683729603886604 \n",
            "Classification loss: 1.416184425354004 \n",
            "Regression loss: 0.06028074771165848\n",
            "----------------\n",
            "Loss:  0.7608740702271461 \n",
            "Classification loss: 1.4558247327804565 \n",
            "Regression loss: 0.03296170383691788\n",
            "----------------\n",
            "Loss:  0.6691148765385151 \n",
            "Classification loss: 1.2879390716552734 \n",
            "Regression loss: 0.025145340710878372\n",
            "----------------\n",
            "Loss:  0.6562506631016731 \n",
            "Classification loss: 1.2485712766647339 \n",
            "Regression loss: 0.03196502476930618\n",
            "----------------\n",
            "Loss:  0.6923473253846169 \n",
            "Classification loss: 1.3225687742233276 \n",
            "Regression loss: 0.031062938272953033\n",
            "----------------\n",
            "Loss:  0.7435331866145134 \n",
            "Classification loss: 1.3881562948226929 \n",
            "Regression loss: 0.04945503920316696\n",
            "----------------\n",
            "Loss:  0.691311776638031 \n",
            "Classification loss: 1.2810357809066772 \n",
            "Regression loss: 0.05079388618469238\n",
            "----------------\n",
            "Loss:  0.6853272058069706 \n",
            "Classification loss: 1.301274299621582 \n",
            "Regression loss: 0.03469005599617958\n",
            "----------------\n",
            "Loss:  0.7800361141562462 \n",
            "Classification loss: 1.4195412397384644 \n",
            "Regression loss: 0.07026549428701401\n",
            "----------------\n",
            "Loss:  0.6719649583101273 \n",
            "Classification loss: 1.2481061220169067 \n",
            "Regression loss: 0.04791189730167389\n",
            "----------------\n",
            "Loss:  0.8407983109354973 \n",
            "Classification loss: 1.5542519092559814 \n",
            "Regression loss: 0.06367235630750656\n",
            "----------------\n",
            "Loss:  0.6915915124118328 \n",
            "Classification loss: 1.2913384437561035 \n",
            "Regression loss: 0.04592229053378105\n",
            "----------------\n",
            "Loss:  0.6707761771976948 \n",
            "Classification loss: 1.2266477346420288 \n",
            "Regression loss: 0.057452309876680374\n",
            "----------------\n",
            "Loss:  0.6978077031672001 \n",
            "Classification loss: 1.3376708030700684 \n",
            "Regression loss: 0.02897230163216591\n",
            "----------------\n",
            "Loss:  0.6201262027025223 \n",
            "Classification loss: 1.1791590452194214 \n",
            "Regression loss: 0.030546680092811584\n",
            "----------------\n",
            "Loss:  0.7448012679815292 \n",
            "Classification loss: 1.3801064491271973 \n",
            "Regression loss: 0.0547480434179306\n",
            "----------------\n",
            "Loss:  0.7447704784572124 \n",
            "Classification loss: 1.3897671699523926 \n",
            "Regression loss: 0.04988689348101616\n",
            "----------------\n",
            "Loss:  0.6686990559101105 \n",
            "Classification loss: 1.2284520864486694 \n",
            "Regression loss: 0.05447301268577576\n",
            "----------------\n",
            "Loss:  0.6548768579959869 \n",
            "Classification loss: 1.222428798675537 \n",
            "Regression loss: 0.043662458658218384\n",
            "----------------\n",
            "Loss:  0.6609560251235962 \n",
            "Classification loss: 1.2243859767913818 \n",
            "Regression loss: 0.04876303672790527\n",
            "----------------\n",
            "Loss:  0.7716832458972931 \n",
            "Classification loss: 1.4134141206741333 \n",
            "Regression loss: 0.06497618556022644\n",
            "----------------\n",
            "Loss:  0.6813933663070202 \n",
            "Classification loss: 1.3105225563049316 \n",
            "Regression loss: 0.026132088154554367\n",
            "----------------\n",
            "Loss:  0.7162545993924141 \n",
            "Classification loss: 1.3469074964523315 \n",
            "Regression loss: 0.04280085116624832\n",
            "----------------\n",
            "Loss:  0.7772102989256382 \n",
            "Classification loss: 1.4386018514633179 \n",
            "Regression loss: 0.05790937319397926\n",
            "----------------\n",
            "Loss:  0.7049320563673973 \n",
            "Classification loss: 1.3244447708129883 \n",
            "Regression loss: 0.04270967096090317\n",
            "----------------\n",
            "Loss:  0.7554190307855606 \n",
            "Classification loss: 1.4308178424835205 \n",
            "Regression loss: 0.040010109543800354\n",
            "----------------\n",
            "Loss:  0.7256516925990582 \n",
            "Classification loss: 1.3469040393829346 \n",
            "Regression loss: 0.052199672907590866\n",
            "----------------\n",
            "Loss:  0.6154046207666397 \n",
            "Classification loss: 1.1626451015472412 \n",
            "Regression loss: 0.034082069993019104\n",
            "----------------\n",
            "Loss:  0.7829235680401325 \n",
            "Classification loss: 1.4490277767181396 \n",
            "Regression loss: 0.0584096796810627\n",
            "----------------\n",
            "Loss:  0.7381322979927063 \n",
            "Classification loss: 1.3714649677276611 \n",
            "Regression loss: 0.05239981412887573\n",
            "----------------\n",
            "Loss:  0.7208658903837204 \n",
            "Classification loss: 1.3198981285095215 \n",
            "Regression loss: 0.060916826128959656\n",
            "----------------\n",
            "Loss:  0.7367846965789795 \n",
            "Classification loss: 1.3837082386016846 \n",
            "Regression loss: 0.04493057727813721\n",
            "----------------\n",
            "Loss:  0.6998554542660713 \n",
            "Classification loss: 1.332472324371338 \n",
            "Regression loss: 0.033619292080402374\n",
            "----------------\n",
            "Loss:  0.9535735994577408 \n",
            "Classification loss: 1.7550410032272339 \n",
            "Regression loss: 0.07605309784412384\n",
            "----------------\n",
            "Loss:  0.7679223008453846 \n",
            "Classification loss: 1.4217352867126465 \n",
            "Regression loss: 0.057054657489061356\n",
            "----------------\n",
            "Loss:  0.6714790798723698 \n",
            "Classification loss: 1.27180814743042 \n",
            "Regression loss: 0.035575006157159805\n",
            "----------------\n",
            "Loss:  0.6784989610314369 \n",
            "Classification loss: 1.2951565980911255 \n",
            "Regression loss: 0.030920661985874176\n",
            "----------------\n",
            "Loss:  0.7291436903178692 \n",
            "Classification loss: 1.3584688901901245 \n",
            "Regression loss: 0.04990924522280693\n",
            "----------------\n",
            "Loss:  0.7811576351523399 \n",
            "Classification loss: 1.4643892049789429 \n",
            "Regression loss: 0.0489630326628685\n",
            "----------------\n",
            "Loss:  0.7142259813845158 \n",
            "Classification loss: 1.3265442848205566 \n",
            "Regression loss: 0.05095383897423744\n",
            "----------------\n",
            "Loss:  0.8016907647252083 \n",
            "Classification loss: 1.4867749214172363 \n",
            "Regression loss: 0.05830330401659012\n",
            "----------------\n",
            "Loss:  0.7117207199335098 \n",
            "Classification loss: 1.3066591024398804 \n",
            "Regression loss: 0.05839116871356964\n",
            "----------------\n",
            "Loss:  0.6774900816380978 \n",
            "Classification loss: 1.2487406730651855 \n",
            "Regression loss: 0.05311974510550499\n",
            "----------------\n",
            "Loss:  0.7008227631449699 \n",
            "Classification loss: 1.2522087097167969 \n",
            "Regression loss: 0.0747184082865715\n",
            "----------------\n",
            "Loss:  0.7778637781739235 \n",
            "Classification loss: 1.4851572513580322 \n",
            "Regression loss: 0.03528515249490738\n",
            "----------------\n",
            "Loss:  0.7212532609701157 \n",
            "Classification loss: 1.353291392326355 \n",
            "Regression loss: 0.04460756480693817\n",
            "----------------\n",
            "Loss:  0.6807961761951447 \n",
            "Classification loss: 1.257965087890625 \n",
            "Regression loss: 0.05181363224983215\n",
            "----------------\n",
            "Loss:  0.6949131228029728 \n",
            "Classification loss: 1.2655813694000244 \n",
            "Regression loss: 0.06212243810296059\n",
            "----------------\n",
            "Loss:  0.6270084604620934 \n",
            "Classification loss: 1.1850626468658447 \n",
            "Regression loss: 0.03447713702917099\n",
            "----------------\n",
            "Loss:  0.7398788183927536 \n",
            "Classification loss: 1.3831045627593994 \n",
            "Regression loss: 0.048326537013053894\n",
            "----------------\n",
            "Loss:  0.6924607194960117 \n",
            "Classification loss: 1.3093397617340088 \n",
            "Regression loss: 0.03779083862900734\n",
            "----------------\n",
            "Loss:  0.7457285560667515 \n",
            "Classification loss: 1.3977820873260498 \n",
            "Regression loss: 0.04683751240372658\n",
            "----------------\n",
            "Loss:  0.7658092379570007 \n",
            "Classification loss: 1.3965048789978027 \n",
            "Regression loss: 0.06755679845809937\n",
            "----------------\n",
            "Loss:  0.7226592265069485 \n",
            "Classification loss: 1.3742170333862305 \n",
            "Regression loss: 0.03555070981383324\n",
            "----------------\n",
            "Loss:  0.6408326588571072 \n",
            "Classification loss: 1.2042447328567505 \n",
            "Regression loss: 0.03871029242873192\n",
            "----------------\n",
            "Loss:  0.7997564449906349 \n",
            "Classification loss: 1.4984992742538452 \n",
            "Regression loss: 0.05050680786371231\n",
            "----------------\n",
            "Loss:  0.6945574022829533 \n",
            "Classification loss: 1.3065195083618164 \n",
            "Regression loss: 0.04129764810204506\n",
            "----------------\n",
            "Loss:  0.7218189127743244 \n",
            "Classification loss: 1.3561301231384277 \n",
            "Regression loss: 0.04375385120511055\n",
            "----------------\n",
            "Loss:  0.675803892314434 \n",
            "Classification loss: 1.2799382209777832 \n",
            "Regression loss: 0.03583478182554245\n",
            "----------------\n",
            "Loss:  0.7930001616477966 \n",
            "Classification loss: 1.4726321697235107 \n",
            "Regression loss: 0.05668407678604126\n",
            "----------------\n",
            "Loss:  0.6809852868318558 \n",
            "Classification loss: 1.2485017776489258 \n",
            "Regression loss: 0.05673439800739288\n",
            "----------------\n",
            "Loss:  0.7477870471775532 \n",
            "Classification loss: 1.3991600275039673 \n",
            "Regression loss: 0.048207033425569534\n",
            "----------------\n",
            "Loss:  0.6927953697741032 \n",
            "Classification loss: 1.2757668495178223 \n",
            "Regression loss: 0.05491194501519203\n",
            "----------------\n",
            "Loss:  0.7721752524375916 \n",
            "Classification loss: 1.460495948791504 \n",
            "Regression loss: 0.0419272780418396\n",
            "----------------\n",
            "Loss:  0.7316856198012829 \n",
            "Classification loss: 1.3929800987243652 \n",
            "Regression loss: 0.035195570439100266\n",
            "----------------\n",
            "Loss:  0.7385315299034119 \n",
            "Classification loss: 1.3879483938217163 \n",
            "Regression loss: 0.04455733299255371\n",
            "----------------\n",
            "Loss:  0.7812644504010677 \n",
            "Classification loss: 1.4812670946121216 \n",
            "Regression loss: 0.04063090309500694\n",
            "----------------\n",
            "Loss:  0.7460030801594257 \n",
            "Classification loss: 1.4099949598312378 \n",
            "Regression loss: 0.04100560024380684\n",
            "----------------\n",
            "Loss:  0.673796184360981 \n",
            "Classification loss: 1.2343391180038452 \n",
            "Regression loss: 0.05662662535905838\n",
            "----------------\n",
            "Loss:  0.6936074830591679 \n",
            "Classification loss: 1.296120524406433 \n",
            "Regression loss: 0.04554722085595131\n",
            "----------------\n",
            "Loss:  0.7370387315750122 \n",
            "Classification loss: 1.3784260749816895 \n",
            "Regression loss: 0.04782569408416748\n",
            "----------------\n",
            "Loss:  0.6820550002157688 \n",
            "Classification loss: 1.2741528749465942 \n",
            "Regression loss: 0.044978562742471695\n",
            "----------------\n",
            "Loss:  0.8081713616847992 \n",
            "Classification loss: 1.4934512376785278 \n",
            "Regression loss: 0.06144574284553528\n",
            "----------------\n",
            "Loss:  0.7385200448334217 \n",
            "Classification loss: 1.3924362659454346 \n",
            "Regression loss: 0.04230191186070442\n",
            "----------------\n",
            "Loss:  0.6409052014350891 \n",
            "Classification loss: 1.1515586376190186 \n",
            "Regression loss: 0.06512588262557983\n",
            "----------------\n",
            "Loss:  0.8277498036623001 \n",
            "Classification loss: 1.5116426944732666 \n",
            "Regression loss: 0.07192845642566681\n",
            "----------------\n",
            "Loss:  0.7091474831104279 \n",
            "Classification loss: 1.329535722732544 \n",
            "Regression loss: 0.044379621744155884\n",
            "----------------\n",
            "Loss:  0.7911339364945889 \n",
            "Classification loss: 1.4639737606048584 \n",
            "Regression loss: 0.05914705619215965\n",
            "----------------\n",
            "Loss:  0.7100580632686615 \n",
            "Classification loss: 1.3313817977905273 \n",
            "Regression loss: 0.04436716437339783\n",
            "----------------\n",
            "Loss:  0.6189812254160643 \n",
            "Classification loss: 1.1791932582855225 \n",
            "Regression loss: 0.029384596273303032\n",
            "----------------\n",
            "Loss:  0.8493881225585938 \n",
            "Classification loss: 1.5800687074661255 \n",
            "Regression loss: 0.059353768825531006\n",
            "----------------\n",
            "Loss:  0.75411606580019 \n",
            "Classification loss: 1.4593971967697144 \n",
            "Regression loss: 0.024417467415332794\n",
            "----------------\n",
            "Loss:  0.6285537034273148 \n",
            "Classification loss: 1.1758965253829956 \n",
            "Regression loss: 0.040605440735816956\n",
            "----------------\n",
            "Loss:  0.6319205574691296 \n",
            "Classification loss: 1.1890419721603394 \n",
            "Regression loss: 0.037399571388959885\n",
            "----------------\n",
            "Loss:  0.7316948547959328 \n",
            "Classification loss: 1.3438799381256104 \n",
            "Regression loss: 0.059754885733127594\n",
            "----------------\n",
            "Loss:  0.6023011971265078 \n",
            "Classification loss: 1.1459242105484009 \n",
            "Regression loss: 0.02933909185230732\n",
            "----------------\n",
            "Loss:  0.632289245724678 \n",
            "Classification loss: 1.1691641807556152 \n",
            "Regression loss: 0.04770715534687042\n",
            "----------------\n",
            "Loss:  0.6612534187734127 \n",
            "Classification loss: 1.2342140674591064 \n",
            "Regression loss: 0.04414638504385948\n",
            "----------------\n",
            "Loss:  0.6850519441068172 \n",
            "Classification loss: 1.2536299228668213 \n",
            "Regression loss: 0.0582369826734066\n",
            "----------------\n",
            "Loss:  0.6569787710905075 \n",
            "Classification loss: 1.2437734603881836 \n",
            "Regression loss: 0.03509204089641571\n",
            "----------------\n",
            "Loss:  0.8103406094014645 \n",
            "Classification loss: 1.575087547302246 \n",
            "Regression loss: 0.022796835750341415\n",
            "----------------\n",
            "Loss:  0.6281817201524973 \n",
            "Classification loss: 1.1981220245361328 \n",
            "Regression loss: 0.029120707884430885\n",
            "----------------\n",
            "Loss:  0.7065935544669628 \n",
            "Classification loss: 1.3269572257995605 \n",
            "Regression loss: 0.04311494156718254\n",
            "----------------\n",
            "Loss:  0.7479925751686096 \n",
            "Classification loss: 1.3646876811981201 \n",
            "Regression loss: 0.06564873456954956\n",
            "----------------\n",
            "Loss:  0.8658146485686302 \n",
            "Classification loss: 1.6223206520080566 \n",
            "Regression loss: 0.0546543225646019\n",
            "----------------\n",
            "Loss:  0.7068725116550922 \n",
            "Classification loss: 1.325300693511963 \n",
            "Regression loss: 0.044222164899110794\n",
            "----------------\n",
            "Loss:  0.6867983490228653 \n",
            "Classification loss: 1.283992052078247 \n",
            "Regression loss: 0.04480232298374176\n",
            "----------------\n",
            "Loss:  0.7850602567195892 \n",
            "Classification loss: 1.420785903930664 \n",
            "Regression loss: 0.0746673047542572\n",
            "----------------\n",
            "Loss:  0.7194626443088055 \n",
            "Classification loss: 1.3354030847549438 \n",
            "Regression loss: 0.05176110193133354\n",
            "----------------\n",
            "Loss:  0.6545818746089935 \n",
            "Classification loss: 1.2372829914093018 \n",
            "Regression loss: 0.03594037890434265\n",
            "----------------\n",
            "Loss:  0.7712072394788265 \n",
            "Classification loss: 1.4781414270401 \n",
            "Regression loss: 0.032136525958776474\n",
            "----------------\n",
            "Loss:  0.6683030351996422 \n",
            "Classification loss: 1.273352026939392 \n",
            "Regression loss: 0.031627021729946136\n",
            "----------------\n",
            "Loss:  0.8027393892407417 \n",
            "Classification loss: 1.4979970455169678 \n",
            "Regression loss: 0.05374086648225784\n",
            "----------------\n",
            "Loss:  0.6944125965237617 \n",
            "Classification loss: 1.3004050254821777 \n",
            "Regression loss: 0.04421008378267288\n",
            "----------------\n",
            "Loss:  0.7445821985602379 \n",
            "Classification loss: 1.4138414859771729 \n",
            "Regression loss: 0.03766145557165146\n",
            "----------------\n",
            "Loss:  0.7593005485832691 \n",
            "Classification loss: 1.4021235704421997 \n",
            "Regression loss: 0.058238763362169266\n",
            "----------------\n",
            "Loss:  0.6837116554379463 \n",
            "Classification loss: 1.306152105331421 \n",
            "Regression loss: 0.03063560277223587\n",
            "----------------\n",
            "Loss:  0.6918626986443996 \n",
            "Classification loss: 1.291408896446228 \n",
            "Regression loss: 0.04615825042128563\n",
            "----------------\n",
            "Loss:  0.7527213394641876 \n",
            "Classification loss: 1.3734302520751953 \n",
            "Regression loss: 0.06600621342658997\n",
            "----------------\n",
            "Loss:  0.8108877390623093 \n",
            "Classification loss: 1.4473237991333008 \n",
            "Regression loss: 0.08722583949565887\n",
            "----------------\n",
            "Loss:  0.7760320492088795 \n",
            "Classification loss: 1.4586982727050781 \n",
            "Regression loss: 0.04668291285634041\n",
            "----------------\n",
            "Loss:  0.6520715206861496 \n",
            "Classification loss: 1.2241652011871338 \n",
            "Regression loss: 0.0399889200925827\n",
            "----------------\n",
            "Loss:  0.7508290223777294 \n",
            "Classification loss: 1.4394296407699585 \n",
            "Regression loss: 0.031114201992750168\n",
            "----------------\n",
            "Loss:  0.9559777453541756 \n",
            "Classification loss: 1.7868348360061646 \n",
            "Regression loss: 0.06256032735109329\n",
            "----------------\n",
            "Loss:  0.7554585784673691 \n",
            "Classification loss: 1.4085183143615723 \n",
            "Regression loss: 0.05119942128658295\n",
            "----------------\n",
            "Loss:  0.772122822701931 \n",
            "Classification loss: 1.4525506496429443 \n",
            "Regression loss: 0.04584749788045883\n",
            "----------------\n",
            "Loss:  0.7218601740896702 \n",
            "Classification loss: 1.3812165260314941 \n",
            "Regression loss: 0.03125191107392311\n",
            "----------------\n",
            "Loss:  0.7144961431622505 \n",
            "Classification loss: 1.3057665824890137 \n",
            "Regression loss: 0.06161285191774368\n",
            "----------------\n",
            "Loss:  0.66456089168787 \n",
            "Classification loss: 1.207011342048645 \n",
            "Regression loss: 0.061055220663547516\n",
            "----------------\n",
            "Loss:  0.7573289051651955 \n",
            "Classification loss: 1.4300702810287476 \n",
            "Regression loss: 0.042293764650821686\n",
            "----------------\n",
            "Loss:  0.7177532017230988 \n",
            "Classification loss: 1.332082748413086 \n",
            "Regression loss: 0.051711827516555786\n",
            "----------------\n",
            "Loss:  0.7711007185280323 \n",
            "Classification loss: 1.465865135192871 \n",
            "Regression loss: 0.038168150931596756\n",
            "----------------\n",
            "Loss:  0.727974746376276 \n",
            "Classification loss: 1.3621881008148193 \n",
            "Regression loss: 0.04688069596886635\n",
            "----------------\n",
            "Loss:  0.6566735431551933 \n",
            "Classification loss: 1.2122046947479248 \n",
            "Regression loss: 0.050571195781230927\n",
            "----------------\n",
            "Loss:  0.713748574256897 \n",
            "Classification loss: 1.3422776460647583 \n",
            "Regression loss: 0.04260975122451782\n",
            "----------------\n",
            "Loss:  0.7485751137137413 \n",
            "Classification loss: 1.3490588665008545 \n",
            "Regression loss: 0.07404568046331406\n",
            "----------------\n",
            "Loss:  0.7747143469750881 \n",
            "Classification loss: 1.4368399381637573 \n",
            "Regression loss: 0.05629437789320946\n",
            "----------------\n",
            "Loss:  0.7258548252284527 \n",
            "Classification loss: 1.344740629196167 \n",
            "Regression loss: 0.053484510630369186\n",
            "----------------\n",
            "Loss:  0.8268589936196804 \n",
            "Classification loss: 1.5611404180526733 \n",
            "Regression loss: 0.046288784593343735\n",
            "----------------\n",
            "Loss:  0.7923082783818245 \n",
            "Classification loss: 1.4231188297271729 \n",
            "Regression loss: 0.08074886351823807\n",
            "----------------\n",
            "Loss:  0.6765284352004528 \n",
            "Classification loss: 1.253204107284546 \n",
            "Regression loss: 0.049926381558179855\n",
            "----------------\n",
            "Loss:  0.6648902520537376 \n",
            "Classification loss: 1.2722752094268799 \n",
            "Regression loss: 0.0287526473402977\n",
            "----------------\n",
            "Loss:  0.6857766136527061 \n",
            "Classification loss: 1.3074170351028442 \n",
            "Regression loss: 0.03206809610128403\n",
            "----------------\n",
            "Loss:  0.744845874607563 \n",
            "Classification loss: 1.369511365890503 \n",
            "Regression loss: 0.060090191662311554\n",
            "----------------\n",
            "Loss:  0.7547434084117413 \n",
            "Classification loss: 1.4035589694976807 \n",
            "Regression loss: 0.052963923662900925\n",
            "----------------\n",
            "Loss:  0.6261905916035175 \n",
            "Classification loss: 1.2028132677078247 \n",
            "Regression loss: 0.02478395774960518\n",
            "----------------\n",
            "Loss:  0.8063566908240318 \n",
            "Classification loss: 1.4555985927581787 \n",
            "Regression loss: 0.07855739444494247\n",
            "----------------\n",
            "Loss:  0.6845237202942371 \n",
            "Classification loss: 1.2853741645812988 \n",
            "Regression loss: 0.04183663800358772\n",
            "----------------\n",
            "Loss:  0.7288727425038815 \n",
            "Classification loss: 1.361748218536377 \n",
            "Regression loss: 0.04799863323569298\n",
            "----------------\n",
            "Loss:  0.6631971001625061 \n",
            "Classification loss: 1.2065646648406982 \n",
            "Regression loss: 0.05991476774215698\n",
            "----------------\n",
            "Loss:  0.8546568378806114 \n",
            "Classification loss: 1.5583775043487549 \n",
            "Regression loss: 0.07546808570623398\n",
            "----------------\n",
            "Loss:  0.7701199986040592 \n",
            "Classification loss: 1.4244861602783203 \n",
            "Regression loss: 0.05787691846489906\n",
            "----------------\n",
            "Loss:  0.6897171065211296 \n",
            "Classification loss: 1.2953147888183594 \n",
            "Regression loss: 0.04205971211194992\n",
            "----------------\n",
            "Loss:  0.860500767827034 \n",
            "Classification loss: 1.5848445892333984 \n",
            "Regression loss: 0.06807847321033478\n",
            "----------------\n",
            "Loss:  0.6384931057691574 \n",
            "Classification loss: 1.1991157531738281 \n",
            "Regression loss: 0.03893522918224335\n",
            "----------------\n",
            "Loss:  0.795280933380127 \n",
            "Classification loss: 1.4634673595428467 \n",
            "Regression loss: 0.06354725360870361\n",
            "----------------\n",
            "Loss:  0.6751931607723236 \n",
            "Classification loss: 1.2371200323104858 \n",
            "Regression loss: 0.05663314461708069\n",
            "----------------\n",
            "Loss:  0.6813395544886589 \n",
            "Classification loss: 1.2420850992202759 \n",
            "Regression loss: 0.060297004878520966\n",
            "----------------\n",
            "Loss:  0.6698243636637926 \n",
            "Classification loss: 1.2781645059585571 \n",
            "Regression loss: 0.030742110684514046\n",
            "----------------\n",
            "Loss:  0.6783922016620636 \n",
            "Classification loss: 1.2770256996154785 \n",
            "Regression loss: 0.03987935185432434\n",
            "----------------\n",
            "Loss:  0.7384071350097656 \n",
            "Classification loss: 1.3494303226470947 \n",
            "Regression loss: 0.06369197368621826\n",
            "----------------\n",
            "Loss:  0.6770938783884048 \n",
            "Classification loss: 1.2290641069412231 \n",
            "Regression loss: 0.06256182491779327\n",
            "----------------\n",
            "Loss:  0.7679646722972393 \n",
            "Classification loss: 1.4136961698532104 \n",
            "Regression loss: 0.06111658737063408\n",
            "----------------\n",
            "Loss:  0.7581203058362007 \n",
            "Classification loss: 1.3912241458892822 \n",
            "Regression loss: 0.0625082328915596\n",
            "----------------\n",
            "Loss:  0.8051721230149269 \n",
            "Classification loss: 1.503745436668396 \n",
            "Regression loss: 0.05329940468072891\n",
            "----------------\n",
            "Loss:  0.729625791311264 \n",
            "Classification loss: 1.331675410270691 \n",
            "Regression loss: 0.06378808617591858\n",
            "----------------\n",
            "Loss:  0.7326419800519943 \n",
            "Classification loss: 1.3551667928695679 \n",
            "Regression loss: 0.05505858361721039\n",
            "----------------\n",
            "Loss:  0.8842296823859215 \n",
            "Classification loss: 1.6035560369491577 \n",
            "Regression loss: 0.08245166391134262\n",
            "----------------\n",
            "Loss:  0.8489953055977821 \n",
            "Classification loss: 1.5908260345458984 \n",
            "Regression loss: 0.053582288324832916\n",
            "----------------\n",
            "Loss:  0.7181611284613609 \n",
            "Classification loss: 1.3544073104858398 \n",
            "Regression loss: 0.04095747321844101\n",
            "----------------\n",
            "Loss:  0.7946750558912754 \n",
            "Classification loss: 1.4735300540924072 \n",
            "Regression loss: 0.05791002884507179\n",
            "----------------\n",
            "Loss:  0.6838592141866684 \n",
            "Classification loss: 1.274912714958191 \n",
            "Regression loss: 0.04640285670757294\n",
            "----------------\n",
            "Loss:  0.7847426757216454 \n",
            "Classification loss: 1.427640438079834 \n",
            "Regression loss: 0.07092245668172836\n",
            "----------------\n",
            "Loss:  0.7570698596537113 \n",
            "Classification loss: 1.4283175468444824 \n",
            "Regression loss: 0.04291108623147011\n",
            "----------------\n",
            "Loss:  0.7648423984646797 \n",
            "Classification loss: 1.4329888820648193 \n",
            "Regression loss: 0.04834795743227005\n",
            "----------------\n",
            "Loss:  0.757318452000618 \n",
            "Classification loss: 1.3917523622512817 \n",
            "Regression loss: 0.06144227087497711\n",
            "----------------\n",
            "Loss:  0.708136148750782 \n",
            "Classification loss: 1.33259916305542 \n",
            "Regression loss: 0.04183656722307205\n",
            "----------------\n",
            "Loss:  0.8202316053211689 \n",
            "Classification loss: 1.5308140516281128 \n",
            "Regression loss: 0.0548245795071125\n",
            "----------------\n",
            "Loss:  0.7495778948068619 \n",
            "Classification loss: 1.3733717203140259 \n",
            "Regression loss: 0.06289203464984894\n",
            "----------------\n",
            "Loss:  0.6599083002656698 \n",
            "Classification loss: 1.2582284212112427 \n",
            "Regression loss: 0.030794089660048485\n",
            "----------------\n",
            "Loss:  0.7721799984574318 \n",
            "Classification loss: 1.456518292427063 \n",
            "Regression loss: 0.0439208522439003\n",
            "----------------\n",
            "Loss:  0.8422561511397362 \n",
            "Classification loss: 1.5651686191558838 \n",
            "Regression loss: 0.05967184156179428\n",
            "----------------\n",
            "Loss:  0.6812834143638611 \n",
            "Classification loss: 1.2752087116241455 \n",
            "Regression loss: 0.04367905855178833\n",
            "----------------\n",
            "Loss:  0.7123918458819389 \n",
            "Classification loss: 1.3155896663665771 \n",
            "Regression loss: 0.05459701269865036\n",
            "----------------\n",
            "Loss:  0.7817729972302914 \n",
            "Classification loss: 1.448544979095459 \n",
            "Regression loss: 0.057500507682561874\n",
            "----------------\n",
            "Loss:  0.8570845127105713 \n",
            "Classification loss: 1.6119277477264404 \n",
            "Regression loss: 0.051120638847351074\n",
            "----------------\n",
            "Loss:  0.8549487963318825 \n",
            "Classification loss: 1.5579547882080078 \n",
            "Regression loss: 0.07597140222787857\n",
            "----------------\n",
            "Loss:  0.8559513166546822 \n",
            "Classification loss: 1.6104755401611328 \n",
            "Regression loss: 0.05071354657411575\n",
            "----------------\n",
            "Loss:  0.6715962812304497 \n",
            "Classification loss: 1.2671728134155273 \n",
            "Regression loss: 0.038009874522686005\n",
            "----------------\n",
            "Loss:  0.7482298165559769 \n",
            "Classification loss: 1.4149224758148193 \n",
            "Regression loss: 0.0407685786485672\n",
            "----------------\n",
            "Loss:  0.7348514720797539 \n",
            "Classification loss: 1.3954564332962036 \n",
            "Regression loss: 0.03712325543165207\n",
            "----------------\n",
            "Loss:  0.6812855117022991 \n",
            "Classification loss: 1.2889430522918701 \n",
            "Regression loss: 0.03681398555636406\n",
            "----------------\n",
            "Loss:  0.8412526957690716 \n",
            "Classification loss: 1.593087911605835 \n",
            "Regression loss: 0.0447087399661541\n",
            "----------------\n",
            "Loss:  0.6924432516098022 \n",
            "Classification loss: 1.2917826175689697 \n",
            "Regression loss: 0.04655194282531738\n",
            "----------------\n",
            "Loss:  0.8996876515448093 \n",
            "Classification loss: 1.7054216861724854 \n",
            "Regression loss: 0.046976808458566666\n",
            "----------------\n",
            "Loss:  0.7077552583068609 \n",
            "Classification loss: 1.355210781097412 \n",
            "Regression loss: 0.03014986775815487\n",
            "----------------\n",
            "Loss:  0.7169294618070126 \n",
            "Classification loss: 1.3690078258514404 \n",
            "Regression loss: 0.03242554888129234\n",
            "----------------\n",
            "Loss:  0.8258069381117821 \n",
            "Classification loss: 1.4722633361816406 \n",
            "Regression loss: 0.08967527002096176\n",
            "----------------\n",
            "Loss:  0.7499128170311451 \n",
            "Classification loss: 1.3899452686309814 \n",
            "Regression loss: 0.05494018271565437\n",
            "----------------\n",
            "Loss:  0.7246961444616318 \n",
            "Classification loss: 1.3772647380828857 \n",
            "Regression loss: 0.036063775420188904\n",
            "----------------\n",
            "Loss:  0.759276457130909 \n",
            "Classification loss: 1.3904080390930176 \n",
            "Regression loss: 0.06407243758440018\n",
            "----------------\n",
            "Loss:  0.6623609531670809 \n",
            "Classification loss: 1.2713210582733154 \n",
            "Regression loss: 0.026700424030423164\n",
            "----------------\n",
            "Loss:  0.8776564598083496 \n",
            "Classification loss: 1.624584436416626 \n",
            "Regression loss: 0.06536424160003662\n",
            "----------------\n",
            "Loss:  0.8359957784414291 \n",
            "Classification loss: 1.5012786388397217 \n",
            "Regression loss: 0.0853564590215683\n",
            "----------------\n",
            "Loss:  0.7147937752306461 \n",
            "Classification loss: 1.3694031238555908 \n",
            "Regression loss: 0.030092213302850723\n",
            "----------------\n",
            "Loss:  0.6886114627122879 \n",
            "Classification loss: 1.2637091875076294 \n",
            "Regression loss: 0.056756868958473206\n",
            "----------------\n",
            "Loss:  0.769029900431633 \n",
            "Classification loss: 1.4239799976348877 \n",
            "Regression loss: 0.05703990161418915\n",
            "----------------\n",
            "Loss:  0.755863294005394 \n",
            "Classification loss: 1.4360169172286987 \n",
            "Regression loss: 0.03785483539104462\n",
            "----------------\n",
            "Loss:  0.6354503817856312 \n",
            "Classification loss: 1.1768200397491455 \n",
            "Regression loss: 0.047040361911058426\n",
            "----------------\n",
            "Loss:  0.6264764443039894 \n",
            "Classification loss: 1.1682473421096802 \n",
            "Regression loss: 0.04235277324914932\n",
            "----------------\n",
            "Loss:  0.7925806418061256 \n",
            "Classification loss: 1.5207194089889526 \n",
            "Regression loss: 0.03222093731164932\n",
            "----------------\n",
            "Loss:  0.7428953871130943 \n",
            "Classification loss: 1.376986026763916 \n",
            "Regression loss: 0.05440237373113632\n",
            "----------------\n",
            "Loss:  0.7368421629071236 \n",
            "Classification loss: 1.3742936849594116 \n",
            "Regression loss: 0.049695320427417755\n",
            "----------------\n",
            "Loss:  0.7132228761911392 \n",
            "Classification loss: 1.2864744663238525 \n",
            "Regression loss: 0.06998564302921295\n",
            "----------------\n",
            "Loss:  0.669665552675724 \n",
            "Classification loss: 1.265910029411316 \n",
            "Regression loss: 0.03671053797006607\n",
            "----------------\n",
            "Loss:  0.7823116183280945 \n",
            "Classification loss: 1.4546438455581665 \n",
            "Regression loss: 0.05498969554901123\n",
            "----------------\n",
            "Loss:  0.6450009196996689 \n",
            "Classification loss: 1.2021594047546387 \n",
            "Regression loss: 0.04392121732234955\n",
            "----------------\n",
            "Loss:  0.7925730645656586 \n",
            "Classification loss: 1.4408032894134521 \n",
            "Regression loss: 0.0721714198589325\n",
            "----------------\n",
            "Loss:  0.7011071145534515 \n",
            "Classification loss: 1.320062518119812 \n",
            "Regression loss: 0.04107585549354553\n",
            "----------------\n",
            "Loss:  0.6910796985030174 \n",
            "Classification loss: 1.3281612396240234 \n",
            "Regression loss: 0.026999078691005707\n",
            "----------------\n",
            "Loss:  0.7310893498361111 \n",
            "Classification loss: 1.35801100730896 \n",
            "Regression loss: 0.05208384618163109\n",
            "----------------\n",
            "Loss:  0.6770570874214172 \n",
            "Classification loss: 1.2840403318405151 \n",
            "Regression loss: 0.03503692150115967\n",
            "----------------\n",
            "Loss:  0.7090525105595589 \n",
            "Classification loss: 1.3276519775390625 \n",
            "Regression loss: 0.04522652179002762\n",
            "----------------\n",
            "Loss:  0.7458521202206612 \n",
            "Classification loss: 1.4285287857055664 \n",
            "Regression loss: 0.03158772736787796\n",
            "----------------\n",
            "Loss:  0.6292121894657612 \n",
            "Classification loss: 1.202723503112793 \n",
            "Regression loss: 0.0278504379093647\n",
            "----------------\n",
            "Loss:  0.6979293264448643 \n",
            "Classification loss: 1.3331174850463867 \n",
            "Regression loss: 0.031370583921670914\n",
            "----------------\n",
            "Loss:  0.8419162109494209 \n",
            "Classification loss: 1.5432560443878174 \n",
            "Regression loss: 0.07028818875551224\n",
            "----------------\n",
            "Loss:  0.7019744291901588 \n",
            "Classification loss: 1.289974570274353 \n",
            "Regression loss: 0.05698714405298233\n",
            "----------------\n",
            "Loss:  0.7375342808663845 \n",
            "Classification loss: 1.3873181343078613 \n",
            "Regression loss: 0.04387521371245384\n",
            "----------------\n",
            "Loss:  0.6868430972099304 \n",
            "Classification loss: 1.2842063903808594 \n",
            "Regression loss: 0.04473990201950073\n",
            "----------------\n",
            "Loss:  0.5701921861618757 \n",
            "Classification loss: 1.0963139533996582 \n",
            "Regression loss: 0.022035209462046623\n",
            "----------------\n",
            "Loss:  0.7549703270196915 \n",
            "Classification loss: 1.4088847637176514 \n",
            "Regression loss: 0.050527945160865784\n",
            "----------------\n",
            "Loss:  0.7250760979950428 \n",
            "Classification loss: 1.3527346849441528 \n",
            "Regression loss: 0.048708755522966385\n",
            "----------------\n",
            "Loss:  0.7531733885407448 \n",
            "Classification loss: 1.391576886177063 \n",
            "Regression loss: 0.05738494545221329\n",
            "----------------\n",
            "Loss:  0.6915438547730446 \n",
            "Classification loss: 1.2769179344177246 \n",
            "Regression loss: 0.05308488756418228\n",
            "----------------\n",
            "Loss:  0.5953145995736122 \n",
            "Classification loss: 1.1127253770828247 \n",
            "Regression loss: 0.03895191103219986\n",
            "----------------\n",
            "Loss:  0.6053848154842854 \n",
            "Classification loss: 1.1589674949645996 \n",
            "Regression loss: 0.02590106800198555\n",
            "----------------\n",
            "Loss:  0.6401023156940937 \n",
            "Classification loss: 1.1982418298721313 \n",
            "Regression loss: 0.04098140075802803\n",
            "----------------\n",
            "Loss:  0.6728412583470345 \n",
            "Classification loss: 1.2726597785949707 \n",
            "Regression loss: 0.0365113690495491\n",
            "----------------\n",
            "Loss:  0.6297409366816282 \n",
            "Classification loss: 1.1985650062561035 \n",
            "Regression loss: 0.03045843355357647\n",
            "----------------\n",
            "Loss:  0.7055993974208832 \n",
            "Classification loss: 1.3137394189834595 \n",
            "Regression loss: 0.04872968792915344\n",
            "----------------\n",
            "Loss:  0.7215361669659615 \n",
            "Classification loss: 1.330198884010315 \n",
            "Regression loss: 0.056436724960803986\n",
            "----------------\n",
            "Loss:  0.7083620578050613 \n",
            "Classification loss: 1.2810696363449097 \n",
            "Regression loss: 0.0678272396326065\n",
            "----------------\n",
            "Loss:  0.6869034692645073 \n",
            "Classification loss: 1.277380108833313 \n",
            "Regression loss: 0.0482134148478508\n",
            "----------------\n",
            "Loss:  0.7658461928367615 \n",
            "Classification loss: 1.4149093627929688 \n",
            "Regression loss: 0.0583915114402771\n",
            "----------------\n",
            "Loss:  0.7536019906401634 \n",
            "Classification loss: 1.4167648553848267 \n",
            "Regression loss: 0.04521956294775009\n",
            "----------------\n",
            "Loss:  0.6658282466232777 \n",
            "Classification loss: 1.263533592224121 \n",
            "Regression loss: 0.03406145051121712\n",
            "----------------\n",
            "Loss:  0.5769692100584507 \n",
            "Classification loss: 1.0748865604400635 \n",
            "Regression loss: 0.03952592983841896\n",
            "----------------\n",
            "Loss:  0.6830062493681908 \n",
            "Classification loss: 1.2876776456832886 \n",
            "Regression loss: 0.03916742652654648\n",
            "----------------\n",
            "Loss:  0.722482942044735 \n",
            "Classification loss: 1.3709475994110107 \n",
            "Regression loss: 0.037009142339229584\n",
            "----------------\n",
            "Loss:  0.7163066379725933 \n",
            "Classification loss: 1.3577007055282593 \n",
            "Regression loss: 0.03745628520846367\n",
            "----------------\n",
            "Loss:  0.7743345648050308 \n",
            "Classification loss: 1.404657006263733 \n",
            "Regression loss: 0.07200606167316437\n",
            "----------------\n",
            "Loss:  0.6547766141593456 \n",
            "Classification loss: 1.2274080514907837 \n",
            "Regression loss: 0.04107258841395378\n",
            "----------------\n",
            "Loss:  0.6845861710608006 \n",
            "Classification loss: 1.2715080976486206 \n",
            "Regression loss: 0.04883212223649025\n",
            "----------------\n",
            "Loss:  0.7464321479201317 \n",
            "Classification loss: 1.3902122974395752 \n",
            "Regression loss: 0.051325999200344086\n",
            "----------------\n",
            "Loss:  0.8480565547943115 \n",
            "Classification loss: 1.5501415729522705 \n",
            "Regression loss: 0.07298576831817627\n",
            "----------------\n",
            "Loss:  0.657934982329607 \n",
            "Classification loss: 1.2248847484588623 \n",
            "Regression loss: 0.04549260810017586\n",
            "----------------\n",
            "Loss:  0.8076733872294426 \n",
            "Classification loss: 1.483520269393921 \n",
            "Regression loss: 0.06591325253248215\n",
            "----------------\n",
            "Loss:  0.6415971666574478 \n",
            "Classification loss: 1.2015352249145508 \n",
            "Regression loss: 0.040829554200172424\n",
            "----------------\n",
            "Loss:  0.7811731025576591 \n",
            "Classification loss: 1.3843400478363037 \n",
            "Regression loss: 0.0890030786395073\n",
            "----------------\n",
            "Loss:  0.7584230527281761 \n",
            "Classification loss: 1.4333360195159912 \n",
            "Regression loss: 0.04175504297018051\n",
            "----------------\n",
            "Loss:  0.7287207245826721 \n",
            "Classification loss: 1.339984655380249 \n",
            "Regression loss: 0.05872839689254761\n",
            "----------------\n",
            "Loss:  0.6343171186745167 \n",
            "Classification loss: 1.1825017929077148 \n",
            "Regression loss: 0.043066222220659256\n",
            "----------------\n",
            "Loss:  0.7294742725789547 \n",
            "Classification loss: 1.3499133586883545 \n",
            "Regression loss: 0.05451759323477745\n",
            "----------------\n",
            "Loss:  0.7541664317250252 \n",
            "Classification loss: 1.3893814086914062 \n",
            "Regression loss: 0.05947572737932205\n",
            "----------------\n",
            "Loss:  0.6994028612971306 \n",
            "Classification loss: 1.3276195526123047 \n",
            "Regression loss: 0.03559308499097824\n",
            "----------------\n",
            "Loss:  0.7368705347180367 \n",
            "Classification loss: 1.4022260904312134 \n",
            "Regression loss: 0.03575748950242996\n",
            "----------------\n",
            "Loss:  0.7350869253277779 \n",
            "Classification loss: 1.3821330070495605 \n",
            "Regression loss: 0.04402042180299759\n",
            "----------------\n",
            "Loss:  0.7139781191945076 \n",
            "Classification loss: 1.3150701522827148 \n",
            "Regression loss: 0.05644304305315018\n",
            "----------------\n",
            "Loss:  0.6425584405660629 \n",
            "Classification loss: 1.1854639053344727 \n",
            "Regression loss: 0.0498264878988266\n",
            "----------------\n",
            "Loss:  0.6355784796178341 \n",
            "Classification loss: 1.1970165967941284 \n",
            "Regression loss: 0.03707018122076988\n",
            "----------------\n",
            "Loss:  0.7529744952917099 \n",
            "Classification loss: 1.355404019355774 \n",
            "Regression loss: 0.07527248561382294\n",
            "----------------\n",
            "Loss:  0.6874416321516037 \n",
            "Classification loss: 1.2614296674728394 \n",
            "Regression loss: 0.05672679841518402\n",
            "----------------\n",
            "Loss:  0.8112424835562706 \n",
            "Classification loss: 1.519484281539917 \n",
            "Regression loss: 0.0515003427863121\n",
            "----------------\n",
            "Loss:  0.8662887215614319 \n",
            "Classification loss: 1.6478824615478516 \n",
            "Regression loss: 0.042347490787506104\n",
            "----------------\n",
            "Loss:  0.6325540691614151 \n",
            "Classification loss: 1.1917455196380615 \n",
            "Regression loss: 0.03668130934238434\n",
            "----------------\n",
            "Loss:  0.7307671867311001 \n",
            "Classification loss: 1.3565704822540283 \n",
            "Regression loss: 0.05248194560408592\n",
            "----------------\n",
            "Loss:  0.7142549082636833 \n",
            "Classification loss: 1.3330497741699219 \n",
            "Regression loss: 0.04773002117872238\n",
            "----------------\n",
            "Loss:  0.6997705772519112 \n",
            "Classification loss: 1.248169183731079 \n",
            "Regression loss: 0.07568598538637161\n",
            "----------------\n",
            "Loss:  0.7581842467188835 \n",
            "Classification loss: 1.414435863494873 \n",
            "Regression loss: 0.05096631497144699\n",
            "----------------\n",
            "Loss:  0.6985313668847084 \n",
            "Classification loss: 1.2664157152175903 \n",
            "Regression loss: 0.06532350927591324\n",
            "----------------\n",
            "Loss:  0.6362797617912292 \n",
            "Classification loss: 1.1407440900802612 \n",
            "Regression loss: 0.06590771675109863\n",
            "----------------\n",
            "Loss:  0.7359159141778946 \n",
            "Classification loss: 1.3984224796295166 \n",
            "Regression loss: 0.03670467436313629\n",
            "----------------\n",
            "Loss:  0.6366688869893551 \n",
            "Classification loss: 1.2190406322479248 \n",
            "Regression loss: 0.027148570865392685\n",
            "----------------\n",
            "Loss:  0.6904297582805157 \n",
            "Classification loss: 1.3039181232452393 \n",
            "Regression loss: 0.03847069665789604\n",
            "----------------\n",
            "Loss:  0.662280984222889 \n",
            "Classification loss: 1.2298940420150757 \n",
            "Regression loss: 0.047333963215351105\n",
            "----------------\n",
            "Loss:  0.7998192310333252 \n",
            "Classification loss: 1.4748003482818604 \n",
            "Regression loss: 0.06241905689239502\n",
            "----------------\n",
            "Loss:  0.9065658375620842 \n",
            "Classification loss: 1.6869512796401978 \n",
            "Regression loss: 0.06309019774198532\n",
            "----------------\n",
            "Loss:  0.7017029672861099 \n",
            "Classification loss: 1.2434074878692627 \n",
            "Regression loss: 0.07999922335147858\n",
            "----------------\n",
            "Loss:  0.6985767632722855 \n",
            "Classification loss: 1.3093202114105225 \n",
            "Regression loss: 0.04391665756702423\n",
            "----------------\n",
            "Loss:  0.752912849187851 \n",
            "Classification loss: 1.4390627145767212 \n",
            "Regression loss: 0.033381491899490356\n",
            "----------------\n",
            "Loss:  0.7926966361701488 \n",
            "Classification loss: 1.4791111946105957 \n",
            "Regression loss: 0.053141038864851\n",
            "----------------\n",
            "Loss:  0.7711215578019619 \n",
            "Classification loss: 1.443752646446228 \n",
            "Regression loss: 0.049245234578847885\n",
            "----------------\n",
            "Loss:  0.6276993714272976 \n",
            "Classification loss: 1.1592817306518555 \n",
            "Regression loss: 0.04805850610136986\n",
            "----------------\n",
            "Loss:  0.6619869992136955 \n",
            "Classification loss: 1.1636691093444824 \n",
            "Regression loss: 0.08015244454145432\n",
            "----------------\n",
            "Loss:  0.7041372731328011 \n",
            "Classification loss: 1.2747031450271606 \n",
            "Regression loss: 0.06678570061922073\n",
            "----------------\n",
            "Loss:  0.667852558195591 \n",
            "Classification loss: 1.2624849081039429 \n",
            "Regression loss: 0.03661010414361954\n",
            "----------------\n",
            "Loss:  0.8242802657186985 \n",
            "Classification loss: 1.5400524139404297 \n",
            "Regression loss: 0.05425405874848366\n",
            "----------------\n",
            "Loss:  0.6182998009026051 \n",
            "Classification loss: 1.165073037147522 \n",
            "Regression loss: 0.03576328232884407\n",
            "----------------\n",
            "Loss:  0.7524538934230804 \n",
            "Classification loss: 1.4219326972961426 \n",
            "Regression loss: 0.041487544775009155\n",
            "----------------\n",
            "Loss:  0.6945756375789642 \n",
            "Classification loss: 1.2365938425064087 \n",
            "Regression loss: 0.07627871632575989\n",
            "----------------\n",
            "Loss:  0.681429959833622 \n",
            "Classification loss: 1.2721614837646484 \n",
            "Regression loss: 0.04534921795129776\n",
            "----------------\n",
            "Loss:  0.5804759114980698 \n",
            "Classification loss: 1.1019906997680664 \n",
            "Regression loss: 0.02948056161403656\n",
            "----------------\n",
            "Loss:  0.8364594243466854 \n",
            "Classification loss: 1.568172574043274 \n",
            "Regression loss: 0.05237313732504845\n",
            "----------------\n",
            "Loss:  0.7014087773859501 \n",
            "Classification loss: 1.3165485858917236 \n",
            "Regression loss: 0.04313448444008827\n",
            "----------------\n",
            "Loss:  0.7656949795782566 \n",
            "Classification loss: 1.4340882301330566 \n",
            "Regression loss: 0.04865086451172829\n",
            "----------------\n",
            "Loss:  0.7677465528249741 \n",
            "Classification loss: 1.4110722541809082 \n",
            "Regression loss: 0.06221042573451996\n",
            "----------------\n",
            "Loss:  0.6970718838274479 \n",
            "Classification loss: 1.2922093868255615 \n",
            "Regression loss: 0.05096719041466713\n",
            "----------------\n",
            "Loss:  0.7633600383996964 \n",
            "Classification loss: 1.414276361465454 \n",
            "Regression loss: 0.0562218576669693\n",
            "----------------\n",
            "Loss:  0.7421929985284805 \n",
            "Classification loss: 1.3590394258499146 \n",
            "Regression loss: 0.06267328560352325\n",
            "----------------\n",
            "Loss:  0.7496876493096352 \n",
            "Classification loss: 1.4104456901550293 \n",
            "Regression loss: 0.044464804232120514\n",
            "----------------\n",
            "Loss:  0.798145204782486 \n",
            "Classification loss: 1.4740080833435059 \n",
            "Regression loss: 0.06114116311073303\n",
            "----------------\n",
            "Loss:  0.6839746534824371 \n",
            "Classification loss: 1.2732962369918823 \n",
            "Regression loss: 0.04732653498649597\n",
            "----------------\n",
            "Loss:  0.6567330248653889 \n",
            "Classification loss: 1.239640712738037 \n",
            "Regression loss: 0.036912668496370316\n",
            "----------------\n",
            "Loss:  0.7070715427398682 \n",
            "Classification loss: 1.2822868824005127 \n",
            "Regression loss: 0.06592810153961182\n",
            "----------------\n",
            "Loss:  0.7152827978134155 \n",
            "Classification loss: 1.3342597484588623 \n",
            "Regression loss: 0.048152923583984375\n",
            "----------------\n",
            "Loss:  0.7031721621751785 \n",
            "Classification loss: 1.3174983263015747 \n",
            "Regression loss: 0.044422999024391174\n",
            "----------------\n",
            "Loss:  0.7379457429051399 \n",
            "Classification loss: 1.3837251663208008 \n",
            "Regression loss: 0.04608315974473953\n",
            "----------------\n",
            "Loss:  0.8384265452623367 \n",
            "Classification loss: 1.5397149324417114 \n",
            "Regression loss: 0.06856907904148102\n",
            "----------------\n",
            "Loss:  0.6608658134937286 \n",
            "Classification loss: 1.2220990657806396 \n",
            "Regression loss: 0.049816280603408813\n",
            "----------------\n",
            "Loss:  0.7500310018658638 \n",
            "Classification loss: 1.3763877153396606 \n",
            "Regression loss: 0.06183714419603348\n",
            "----------------\n",
            "Loss:  0.679490964859724 \n",
            "Classification loss: 1.2758041620254517 \n",
            "Regression loss: 0.041588883846998215\n",
            "----------------\n",
            "Loss:  0.8513644188642502 \n",
            "Classification loss: 1.5822834968566895 \n",
            "Regression loss: 0.06022267043590546\n",
            "----------------\n",
            "Loss:  0.6710536256432533 \n",
            "Classification loss: 1.249558925628662 \n",
            "Regression loss: 0.04627416282892227\n",
            "----------------\n",
            "Loss:  0.7038588561117649 \n",
            "Classification loss: 1.316784381866455 \n",
            "Regression loss: 0.04546666517853737\n",
            "----------------\n",
            "Loss:  0.6142618060112 \n",
            "Classification loss: 1.110061526298523 \n",
            "Regression loss: 0.05923104286193848\n",
            "----------------\n",
            "Loss:  0.6665298230946064 \n",
            "Classification loss: 1.2329866886138916 \n",
            "Regression loss: 0.0500364787876606\n",
            "----------------\n",
            "Loss:  0.6852991878986359 \n",
            "Classification loss: 1.2435383796691895 \n",
            "Regression loss: 0.06352999806404114\n",
            "----------------\n",
            "Loss:  0.6718503553420305 \n",
            "Classification loss: 1.2853223085403442 \n",
            "Regression loss: 0.029189201071858406\n",
            "----------------\n",
            "Loss:  0.580447643995285 \n",
            "Classification loss: 1.0960724353790283 \n",
            "Regression loss: 0.032411426305770874\n",
            "----------------\n",
            "Loss:  0.6073252968490124 \n",
            "Classification loss: 1.1517558097839355 \n",
            "Regression loss: 0.0314473919570446\n",
            "----------------\n",
            "Loss:  0.7936692871153355 \n",
            "Classification loss: 1.5004818439483643 \n",
            "Regression loss: 0.043428365141153336\n",
            "----------------\n",
            "Loss:  0.6961678378283978 \n",
            "Classification loss: 1.326914668083191 \n",
            "Regression loss: 0.03271050378680229\n",
            "----------------\n",
            "Loss:  0.6873007752001286 \n",
            "Classification loss: 1.259907603263855 \n",
            "Regression loss: 0.057346973568201065\n",
            "----------------\n",
            "Loss:  0.7285177335143089 \n",
            "Classification loss: 1.3354740142822266 \n",
            "Regression loss: 0.06078072637319565\n",
            "----------------\n",
            "Loss:  0.6659945994615555 \n",
            "Classification loss: 1.271040916442871 \n",
            "Regression loss: 0.030474141240119934\n",
            "----------------\n",
            "Loss:  0.7120347917079926 \n",
            "Classification loss: 1.311079740524292 \n",
            "Regression loss: 0.05649492144584656\n",
            "----------------\n",
            "Loss:  0.8013014793395996 \n",
            "Classification loss: 1.48679780960083 \n",
            "Regression loss: 0.05790257453918457\n",
            "----------------\n",
            "Loss:  0.7429342903196812 \n",
            "Classification loss: 1.388880729675293 \n",
            "Regression loss: 0.04849392548203468\n",
            "----------------\n",
            "Loss:  0.6641378626227379 \n",
            "Classification loss: 1.2475069761276245 \n",
            "Regression loss: 0.04038437455892563\n",
            "----------------\n",
            "Loss:  0.6693960390985012 \n",
            "Classification loss: 1.248868703842163 \n",
            "Regression loss: 0.04496168717741966\n",
            "----------------\n",
            "Loss:  0.6097006574273109 \n",
            "Classification loss: 1.1080939769744873 \n",
            "Regression loss: 0.05565366894006729\n",
            "----------------\n",
            "Loss:  0.6803098656237125 \n",
            "Classification loss: 1.3052527904510498 \n",
            "Regression loss: 0.027683470398187637\n",
            "----------------\n",
            "Loss:  0.659047082066536 \n",
            "Classification loss: 1.2261605262756348 \n",
            "Regression loss: 0.04596681892871857\n",
            "----------------\n",
            "Loss:  0.7365879118442535 \n",
            "Classification loss: 1.3456330299377441 \n",
            "Regression loss: 0.06377139687538147\n",
            "----------------\n",
            "Loss:  0.7176157981157303 \n",
            "Classification loss: 1.3027392625808716 \n",
            "Regression loss: 0.0662461668252945\n",
            "----------------\n",
            "Loss:  0.7102122455835342 \n",
            "Classification loss: 1.3218271732330322 \n",
            "Regression loss: 0.04929865896701813\n",
            "----------------\n",
            "Loss:  0.7296195030212402 \n",
            "Classification loss: 1.340859293937683 \n",
            "Regression loss: 0.05918985605239868\n",
            "----------------\n",
            "Loss:  0.6999433971941471 \n",
            "Classification loss: 1.3061549663543701 \n",
            "Regression loss: 0.04686591401696205\n",
            "----------------\n",
            "Loss:  0.6713363267481327 \n",
            "Classification loss: 1.2641820907592773 \n",
            "Regression loss: 0.039245281368494034\n",
            "----------------\n",
            "Loss:  0.7294202819466591 \n",
            "Classification loss: 1.3652245998382568 \n",
            "Regression loss: 0.04680798202753067\n",
            "----------------\n",
            "Loss:  0.6377661935985088 \n",
            "Classification loss: 1.1751357316970825 \n",
            "Regression loss: 0.050198327749967575\n",
            "----------------\n",
            "Loss:  0.7299957610666752 \n",
            "Classification loss: 1.3553470373153687 \n",
            "Regression loss: 0.05232224240899086\n",
            "----------------\n",
            "Loss:  0.781213216483593 \n",
            "Classification loss: 1.4459706544876099 \n",
            "Regression loss: 0.058227889239788055\n",
            "----------------\n",
            "Loss:  0.7262789979577065 \n",
            "Classification loss: 1.393805742263794 \n",
            "Regression loss: 0.02937612682580948\n",
            "----------------\n",
            "Loss:  0.7895109578967094 \n",
            "Classification loss: 1.4782800674438477 \n",
            "Regression loss: 0.050370924174785614\n",
            "----------------\n",
            "Loss:  0.6986033394932747 \n",
            "Classification loss: 1.263346552848816 \n",
            "Regression loss: 0.06693006306886673\n",
            "----------------\n",
            "Loss:  0.7201003804802895 \n",
            "Classification loss: 1.3674174547195435 \n",
            "Regression loss: 0.03639165312051773\n",
            "----------------\n",
            "Loss:  0.6931653246283531 \n",
            "Classification loss: 1.2839524745941162 \n",
            "Regression loss: 0.05118908733129501\n",
            "----------------\n",
            "Loss:  0.6827823892235756 \n",
            "Classification loss: 1.2637748718261719 \n",
            "Regression loss: 0.050894953310489655\n",
            "----------------\n",
            "Loss:  0.6812455728650093 \n",
            "Classification loss: 1.2803289890289307 \n",
            "Regression loss: 0.041081078350543976\n",
            "----------------\n",
            "Loss:  0.7806169390678406 \n",
            "Classification loss: 1.4168509244918823 \n",
            "Regression loss: 0.07219147682189941\n",
            "----------------\n",
            "Loss:  0.7458182200789452 \n",
            "Classification loss: 1.3357164859771729 \n",
            "Regression loss: 0.07795997709035873\n",
            "----------------\n",
            "Loss:  0.7308287024497986 \n",
            "Classification loss: 1.3708910942077637 \n",
            "Regression loss: 0.04538315534591675\n",
            "----------------\n",
            "Loss:  0.6249634064733982 \n",
            "Classification loss: 1.1893682479858398 \n",
            "Regression loss: 0.030279282480478287\n",
            "----------------\n",
            "Loss:  0.7408545836806297 \n",
            "Classification loss: 1.3674144744873047 \n",
            "Regression loss: 0.057147346436977386\n",
            "----------------\n",
            "Loss:  0.5888971611857414 \n",
            "Classification loss: 1.1178443431854248 \n",
            "Regression loss: 0.029974989593029022\n",
            "----------------\n",
            "Loss:  0.6810430139303207 \n",
            "Classification loss: 1.2787736654281616 \n",
            "Regression loss: 0.04165618121623993\n",
            "----------------\n",
            "Loss:  0.6667860150337219 \n",
            "Classification loss: 1.203693151473999 \n",
            "Regression loss: 0.06493943929672241\n",
            "----------------\n",
            "Loss:  0.6389226466417313 \n",
            "Classification loss: 1.1839635372161865 \n",
            "Regression loss: 0.046940878033638\n",
            "----------------\n",
            "Loss:  0.7080578878521919 \n",
            "Classification loss: 1.3262417316436768 \n",
            "Regression loss: 0.044937022030353546\n",
            "----------------\n",
            "Loss:  0.799990076571703 \n",
            "Classification loss: 1.4777089357376099 \n",
            "Regression loss: 0.061135608702898026\n",
            "----------------\n",
            "Loss:  0.6756973452866077 \n",
            "Classification loss: 1.2575910091400146 \n",
            "Regression loss: 0.04690184071660042\n",
            "----------------\n",
            "Loss:  0.5886886697262526 \n",
            "Classification loss: 1.127530813217163 \n",
            "Regression loss: 0.024923263117671013\n",
            "----------------\n",
            "Loss:  0.7363397479057312 \n",
            "Classification loss: 1.3971774578094482 \n",
            "Regression loss: 0.03775101900100708\n",
            "----------------\n",
            "Loss:  0.636751152575016 \n",
            "Classification loss: 1.1912111043930054 \n",
            "Regression loss: 0.041145600378513336\n",
            "----------------\n",
            "Loss:  0.7957657910883427 \n",
            "Classification loss: 1.493015170097351 \n",
            "Regression loss: 0.04925820603966713\n",
            "----------------\n",
            "Loss:  0.6891565155237913 \n",
            "Classification loss: 1.329436182975769 \n",
            "Regression loss: 0.02443842403590679\n",
            "----------------\n",
            "Loss:  0.8303867764770985 \n",
            "Classification loss: 1.541983723640442 \n",
            "Regression loss: 0.05939491465687752\n",
            "----------------\n",
            "Loss:  0.6463236659765244 \n",
            "Classification loss: 1.1954172849655151 \n",
            "Regression loss: 0.048615023493766785\n",
            "----------------\n",
            "Loss:  0.5861941985785961 \n",
            "Classification loss: 1.0899714231491089 \n",
            "Regression loss: 0.04120848700404167\n",
            "----------------\n",
            "Loss:  0.6353062242269516 \n",
            "Classification loss: 1.1507556438446045 \n",
            "Regression loss: 0.05992840230464935\n",
            "----------------\n",
            "Loss:  0.6097572296857834 \n",
            "Classification loss: 1.139873743057251 \n",
            "Regression loss: 0.0398203581571579\n",
            "----------------\n",
            "Loss:  0.7253494821488857 \n",
            "Classification loss: 1.3817328214645386 \n",
            "Regression loss: 0.03448307141661644\n",
            "----------------\n",
            "Loss:  0.7079797498881817 \n",
            "Classification loss: 1.3572977781295776 \n",
            "Regression loss: 0.029330860823392868\n",
            "----------------\n",
            "Loss:  0.6985179223120213 \n",
            "Classification loss: 1.2890483140945435 \n",
            "Regression loss: 0.05399376526474953\n",
            "----------------\n",
            "Loss:  0.7583307176828384 \n",
            "Classification loss: 1.4111042022705078 \n",
            "Regression loss: 0.052778616547584534\n",
            "----------------\n",
            "Loss:  0.6761254221200943 \n",
            "Classification loss: 1.2677141427993774 \n",
            "Regression loss: 0.04226835072040558\n",
            "----------------\n",
            "Loss:  0.6100834086537361 \n",
            "Classification loss: 1.156891942024231 \n",
            "Regression loss: 0.031637437641620636\n",
            "----------------\n",
            "Loss:  0.8250604569911957 \n",
            "Classification loss: 1.5467932224273682 \n",
            "Regression loss: 0.0516638457775116\n",
            "----------------\n",
            "Loss:  0.7054298669099808 \n",
            "Classification loss: 1.3024868965148926 \n",
            "Regression loss: 0.054186418652534485\n",
            "----------------\n",
            "Loss:  0.7827252447605133 \n",
            "Classification loss: 1.458979845046997 \n",
            "Regression loss: 0.05323532223701477\n",
            "----------------\n",
            "Loss:  0.6731182187795639 \n",
            "Classification loss: 1.2851827144622803 \n",
            "Regression loss: 0.030526861548423767\n",
            "----------------\n",
            "Loss:  0.6404599025845528 \n",
            "Classification loss: 1.2066354751586914 \n",
            "Regression loss: 0.03714216500520706\n",
            "----------------\n",
            "Loss:  0.7321834713220596 \n",
            "Classification loss: 1.34660005569458 \n",
            "Regression loss: 0.05888344347476959\n",
            "----------------\n",
            "Loss:  0.7282971423119307 \n",
            "Classification loss: 1.4052318334579468 \n",
            "Regression loss: 0.025681225582957268\n",
            "----------------\n",
            "Loss:  0.7149884179234505 \n",
            "Classification loss: 1.304160714149475 \n",
            "Regression loss: 0.06290806084871292\n",
            "----------------\n",
            "Loss:  0.808396827429533 \n",
            "Classification loss: 1.5325418710708618 \n",
            "Regression loss: 0.0421258918941021\n",
            "----------------\n",
            "Loss:  0.7140740752220154 \n",
            "Classification loss: 1.3548023700714111 \n",
            "Regression loss: 0.036672890186309814\n",
            "----------------\n",
            "Loss:  0.6699821911752224 \n",
            "Classification loss: 1.24098801612854 \n",
            "Regression loss: 0.04948818311095238\n",
            "----------------\n",
            "Loss:  0.8210118114948273 \n",
            "Classification loss: 1.507887363433838 \n",
            "Regression loss: 0.06706812977790833\n",
            "----------------\n",
            "Loss:  0.669011402875185 \n",
            "Classification loss: 1.249916911125183 \n",
            "Regression loss: 0.04405294731259346\n",
            "----------------\n",
            "Loss:  0.6272699255496264 \n",
            "Classification loss: 1.2075886726379395 \n",
            "Regression loss: 0.023475589230656624\n",
            "----------------\n",
            "Loss:  0.6800890974700451 \n",
            "Classification loss: 1.2519192695617676 \n",
            "Regression loss: 0.0541294626891613\n",
            "----------------\n",
            "Loss:  0.698021899908781 \n",
            "Classification loss: 1.271488904953003 \n",
            "Regression loss: 0.06227744743227959\n",
            "----------------\n",
            "Loss:  0.690110556781292 \n",
            "Classification loss: 1.2871143817901611 \n",
            "Regression loss: 0.046553365886211395\n",
            "----------------\n",
            "Loss:  0.7445436269044876 \n",
            "Classification loss: 1.3710596561431885 \n",
            "Regression loss: 0.05901379883289337\n",
            "----------------\n",
            "Loss:  0.627595815807581 \n",
            "Classification loss: 1.1641042232513428 \n",
            "Regression loss: 0.04554370418190956\n",
            "----------------\n",
            "Loss:  0.7531417347490788 \n",
            "Classification loss: 1.4142818450927734 \n",
            "Regression loss: 0.04600081220269203\n",
            "----------------\n",
            "Loss:  0.7386229410767555 \n",
            "Classification loss: 1.2979156970977783 \n",
            "Regression loss: 0.08966509252786636\n",
            "----------------\n",
            "Loss:  0.6429242715239525 \n",
            "Classification loss: 1.1674906015396118 \n",
            "Regression loss: 0.059178970754146576\n",
            "----------------\n",
            "Loss:  0.7099300436675549 \n",
            "Classification loss: 1.3242098093032837 \n",
            "Regression loss: 0.04782513901591301\n",
            "----------------\n",
            "Loss:  0.694495864212513 \n",
            "Classification loss: 1.2672879695892334 \n",
            "Regression loss: 0.06085187941789627\n",
            "----------------\n",
            "Loss:  0.5974120534956455 \n",
            "Classification loss: 1.1139333248138428 \n",
            "Regression loss: 0.040445391088724136\n",
            "----------------\n",
            "Loss:  0.7059048004448414 \n",
            "Classification loss: 1.3371294736862183 \n",
            "Regression loss: 0.037340063601732254\n",
            "----------------\n",
            "Loss:  0.6411137916147709 \n",
            "Classification loss: 1.2209852933883667 \n",
            "Regression loss: 0.03062114492058754\n",
            "----------------\n",
            "Loss:  0.6288187950849533 \n",
            "Classification loss: 1.1751190423965454 \n",
            "Regression loss: 0.0412592738866806\n",
            "----------------\n",
            "Loss:  0.7206794582307339 \n",
            "Classification loss: 1.336846947669983 \n",
            "Regression loss: 0.052255984395742416\n",
            "----------------\n",
            "Loss:  0.808211125433445 \n",
            "Classification loss: 1.507910966873169 \n",
            "Regression loss: 0.054255641996860504\n",
            "----------------\n",
            "Loss:  0.8356137797236443 \n",
            "Classification loss: 1.5100528001785278 \n",
            "Regression loss: 0.08058737963438034\n",
            "----------------\n",
            "Loss:  0.821733832359314 \n",
            "Classification loss: 1.5680506229400635 \n",
            "Regression loss: 0.03770852088928223\n",
            "----------------\n",
            "Loss:  0.7365790121257305 \n",
            "Classification loss: 1.392828345298767 \n",
            "Regression loss: 0.04016483947634697\n",
            "----------------\n",
            "Loss:  0.6752673946321011 \n",
            "Classification loss: 1.290210485458374 \n",
            "Regression loss: 0.030162151902914047\n",
            "----------------\n",
            "Loss:  0.7243171185255051 \n",
            "Classification loss: 1.375151515007019 \n",
            "Regression loss: 0.036741361021995544\n",
            "----------------\n",
            "Loss:  0.6632063016295433 \n",
            "Classification loss: 1.1942174434661865 \n",
            "Regression loss: 0.06609757989645004\n",
            "----------------\n",
            "Loss:  0.8089318461716175 \n",
            "Classification loss: 1.515756368637085 \n",
            "Regression loss: 0.05105366185307503\n",
            "----------------\n",
            "Loss:  0.8172774240374565 \n",
            "Classification loss: 1.4882652759552002 \n",
            "Regression loss: 0.07314478605985641\n",
            "----------------\n",
            "Loss:  0.775008462369442 \n",
            "Classification loss: 1.4320588111877441 \n",
            "Regression loss: 0.058979056775569916\n",
            "----------------\n",
            "Loss:  0.7485208250582218 \n",
            "Classification loss: 1.4375643730163574 \n",
            "Regression loss: 0.029738638550043106\n",
            "----------------\n",
            "Loss:  0.5885259062051773 \n",
            "Classification loss: 1.0940444469451904 \n",
            "Regression loss: 0.04150368273258209\n",
            "----------------\n",
            "Loss:  0.7115400955080986 \n",
            "Classification loss: 1.3573458194732666 \n",
            "Regression loss: 0.0328671857714653\n",
            "----------------\n",
            "Loss:  0.7597622647881508 \n",
            "Classification loss: 1.4196890592575073 \n",
            "Regression loss: 0.049917735159397125\n",
            "----------------\n",
            "Loss:  0.7083320058882236 \n",
            "Classification loss: 1.3141772747039795 \n",
            "Regression loss: 0.0512433685362339\n",
            "----------------\n",
            "Loss:  0.7004358768463135 \n",
            "Classification loss: 1.3195140361785889 \n",
            "Regression loss: 0.04067885875701904\n",
            "----------------\n",
            "Loss:  0.7831464782357216 \n",
            "Classification loss: 1.4140076637268066 \n",
            "Regression loss: 0.07614264637231827\n",
            "----------------\n",
            "Loss:  0.7807223871350288 \n",
            "Classification loss: 1.4308573007583618 \n",
            "Regression loss: 0.06529373675584793\n",
            "----------------\n",
            "Loss:  0.7245496101677418 \n",
            "Classification loss: 1.3533416986465454 \n",
            "Regression loss: 0.04787876084446907\n",
            "----------------\n",
            "Loss:  0.7203046530485153 \n",
            "Classification loss: 1.347719430923462 \n",
            "Regression loss: 0.04644493758678436\n",
            "----------------\n",
            "Loss:  0.8114126473665237 \n",
            "Classification loss: 1.5306706428527832 \n",
            "Regression loss: 0.04607732594013214\n",
            "----------------\n",
            "Loss:  0.7017954848706722 \n",
            "Classification loss: 1.3023960590362549 \n",
            "Regression loss: 0.050597455352544785\n",
            "----------------\n",
            "Loss:  0.7835775762796402 \n",
            "Classification loss: 1.4445385932922363 \n",
            "Regression loss: 0.061308279633522034\n",
            "----------------\n",
            "Loss:  0.6914999857544899 \n",
            "Classification loss: 1.29771089553833 \n",
            "Regression loss: 0.04264453798532486\n",
            "----------------\n",
            "Loss:  0.7225252278149128 \n",
            "Classification loss: 1.363734245300293 \n",
            "Regression loss: 0.04065810516476631\n",
            "----------------\n",
            "Loss:  0.7225141525268555 \n",
            "Classification loss: 1.3341517448425293 \n",
            "Regression loss: 0.05543828010559082\n",
            "----------------\n",
            "Loss:  0.7827338799834251 \n",
            "Classification loss: 1.4382530450820923 \n",
            "Regression loss: 0.063607357442379\n",
            "----------------\n",
            "Loss:  0.8358331844210625 \n",
            "Classification loss: 1.5463166236877441 \n",
            "Regression loss: 0.0626748725771904\n",
            "----------------\n",
            "Loss:  0.6801631040871143 \n",
            "Classification loss: 1.2969878911972046 \n",
            "Regression loss: 0.03166915848851204\n",
            "----------------\n",
            "Loss:  0.7613511048257351 \n",
            "Classification loss: 1.4308972358703613 \n",
            "Regression loss: 0.04590248689055443\n",
            "----------------\n",
            "Loss:  0.6837364956736565 \n",
            "Classification loss: 1.2839210033416748 \n",
            "Regression loss: 0.04177599400281906\n",
            "----------------\n",
            "Loss:  0.7145091518759727 \n",
            "Classification loss: 1.3106493949890137 \n",
            "Regression loss: 0.05918445438146591\n",
            "----------------\n",
            "Loss:  0.6395619474351406 \n",
            "Classification loss: 1.2043083906173706 \n",
            "Regression loss: 0.03740775212645531\n",
            "----------------\n",
            "Loss:  0.6074332930147648 \n",
            "Classification loss: 1.104565978050232 \n",
            "Regression loss: 0.05515030398964882\n",
            "----------------\n",
            "Loss:  0.6018515154719353 \n",
            "Classification loss: 1.1434202194213867 \n",
            "Regression loss: 0.030141405761241913\n",
            "----------------\n",
            "Loss:  0.7461819648742676 \n",
            "Classification loss: 1.3638402223587036 \n",
            "Regression loss: 0.06426185369491577\n",
            "----------------\n",
            "Loss:  0.772172100841999 \n",
            "Classification loss: 1.4796934127807617 \n",
            "Regression loss: 0.032325394451618195\n",
            "----------------\n",
            "Loss:  0.716919269412756 \n",
            "Classification loss: 1.323799729347229 \n",
            "Regression loss: 0.055019404739141464\n",
            "----------------\n",
            "Loss:  0.7227057442069054 \n",
            "Classification loss: 1.337335228919983 \n",
            "Regression loss: 0.05403812974691391\n",
            "----------------\n",
            "Loss:  0.7576122619211674 \n",
            "Classification loss: 1.4130034446716309 \n",
            "Regression loss: 0.051110539585351944\n",
            "----------------\n",
            "Loss:  0.7161255516111851 \n",
            "Classification loss: 1.3488448858261108 \n",
            "Regression loss: 0.041703108698129654\n",
            "----------------\n",
            "Loss:  0.5998383983969688 \n",
            "Classification loss: 1.11400306224823 \n",
            "Regression loss: 0.04283686727285385\n",
            "----------------\n",
            "Loss:  0.7604349851608276 \n",
            "Classification loss: 1.444709062576294 \n",
            "Regression loss: 0.038080453872680664\n",
            "----------------\n",
            "Loss:  0.7033091187477112 \n",
            "Classification loss: 1.3425538539886475 \n",
            "Regression loss: 0.03203219175338745\n",
            "----------------\n",
            "Loss:  0.6987773329019547 \n",
            "Classification loss: 1.3196914196014404 \n",
            "Regression loss: 0.038931623101234436\n",
            "----------------\n",
            "Loss:  0.7333654463291168 \n",
            "Classification loss: 1.3415858745574951 \n",
            "Regression loss: 0.06257250905036926\n",
            "----------------\n",
            "Loss:  0.8372380323708057 \n",
            "Classification loss: 1.553409457206726 \n",
            "Regression loss: 0.0605333037674427\n",
            "----------------\n",
            "Loss:  0.6923198699951172 \n",
            "Classification loss: 1.318045973777771 \n",
            "Regression loss: 0.03329688310623169\n",
            "----------------\n",
            "Loss:  0.749696534126997 \n",
            "Classification loss: 1.388886570930481 \n",
            "Regression loss: 0.055253248661756516\n",
            "----------------\n",
            "Loss:  0.690943107008934 \n",
            "Classification loss: 1.2981928586959839 \n",
            "Regression loss: 0.04184667766094208\n",
            "----------------\n",
            "Loss:  0.7623346261680126 \n",
            "Classification loss: 1.4107459783554077 \n",
            "Regression loss: 0.05696163699030876\n",
            "----------------\n",
            "Loss:  0.734801571816206 \n",
            "Classification loss: 1.3534363508224487 \n",
            "Regression loss: 0.05808339640498161\n",
            "----------------\n",
            "Loss:  0.6933647356927395 \n",
            "Classification loss: 1.327716588973999 \n",
            "Regression loss: 0.029506441205739975\n",
            "----------------\n",
            "Loss:  0.7925845235586166 \n",
            "Classification loss: 1.509164810180664 \n",
            "Regression loss: 0.03800211846828461\n",
            "----------------\n",
            "Loss:  0.732457622885704 \n",
            "Classification loss: 1.3435295820236206 \n",
            "Regression loss: 0.06069283187389374\n",
            "----------------\n",
            "Loss:  0.6856077499687672 \n",
            "Classification loss: 1.3292611837387085 \n",
            "Regression loss: 0.020977158099412918\n",
            "----------------\n",
            "Loss:  0.699610248208046 \n",
            "Classification loss: 1.302980899810791 \n",
            "Regression loss: 0.04811979830265045\n",
            "----------------\n",
            "Loss:  0.7192667908966541 \n",
            "Classification loss: 1.3170788288116455 \n",
            "Regression loss: 0.060727376490831375\n",
            "----------------\n",
            "Loss:  0.7432843893766403 \n",
            "Classification loss: 1.395171880722046 \n",
            "Regression loss: 0.04569844901561737\n",
            "----------------\n",
            "Loss:  0.8262441717088223 \n",
            "Classification loss: 1.5666940212249756 \n",
            "Regression loss: 0.04289716109633446\n",
            "----------------\n",
            "Loss:  0.6274995245039463 \n",
            "Classification loss: 1.1605550050735474 \n",
            "Regression loss: 0.04722202196717262\n",
            "----------------\n",
            "Loss:  0.7383171021938324 \n",
            "Classification loss: 1.3710837364196777 \n",
            "Regression loss: 0.05277523398399353\n",
            "----------------\n",
            "Loss:  0.7545732632279396 \n",
            "Classification loss: 1.3390321731567383 \n",
            "Regression loss: 0.08505717664957047\n",
            "----------------\n",
            "Loss:  0.567135326564312 \n",
            "Classification loss: 1.0370807647705078 \n",
            "Regression loss: 0.048594944179058075\n",
            "----------------\n",
            "Loss:  0.8132420256733894 \n",
            "Classification loss: 1.5132451057434082 \n",
            "Regression loss: 0.05661947280168533\n",
            "----------------\n",
            "Loss:  0.8727370500564575 \n",
            "Classification loss: 1.6410303115844727 \n",
            "Regression loss: 0.05222189426422119\n",
            "----------------\n",
            "Loss:  0.7210273966193199 \n",
            "Classification loss: 1.3410289287567139 \n",
            "Regression loss: 0.05051293224096298\n",
            "----------------\n",
            "Loss:  0.7090810239315033 \n",
            "Classification loss: 1.360100507736206 \n",
            "Regression loss: 0.02903077006340027\n",
            "----------------\n",
            "Loss:  0.7415215373039246 \n",
            "Classification loss: 1.393025517463684 \n",
            "Regression loss: 0.04500877857208252\n",
            "----------------\n",
            "Loss:  0.6334381178021431 \n",
            "Classification loss: 1.1592037677764893 \n",
            "Regression loss: 0.05383623391389847\n",
            "----------------\n",
            "Loss:  0.6809753365814686 \n",
            "Classification loss: 1.2911797761917114 \n",
            "Regression loss: 0.03538544848561287\n",
            "----------------\n",
            "Loss:  0.6559317838400602 \n",
            "Classification loss: 1.2636113166809082 \n",
            "Regression loss: 0.024126125499606133\n",
            "----------------\n",
            "Loss:  0.6852752640843391 \n",
            "Classification loss: 1.2726120948791504 \n",
            "Regression loss: 0.04896921664476395\n",
            "----------------\n",
            "Loss:  0.6186080314218998 \n",
            "Classification loss: 1.1616978645324707 \n",
            "Regression loss: 0.037759099155664444\n",
            "----------------\n",
            "Loss:  0.7356878034770489 \n",
            "Classification loss: 1.3716905117034912 \n",
            "Regression loss: 0.04984254762530327\n",
            "----------------\n",
            "Loss:  0.6859451830387115 \n",
            "Classification loss: 1.279052972793579 \n",
            "Regression loss: 0.046418696641922\n",
            "----------------\n",
            "Loss:  0.7474190592765808 \n",
            "Classification loss: 1.3844523429870605 \n",
            "Regression loss: 0.05519288778305054\n",
            "----------------\n",
            "Loss:  0.7926905229687691 \n",
            "Classification loss: 1.5289950370788574 \n",
            "Regression loss: 0.028193004429340363\n",
            "----------------\n",
            "Loss:  0.8040194176137447 \n",
            "Classification loss: 1.5037269592285156 \n",
            "Regression loss: 0.05215593799948692\n",
            "----------------\n",
            "Loss:  0.7813355699181557 \n",
            "Classification loss: 1.47135591506958 \n",
            "Regression loss: 0.04565761238336563\n",
            "----------------\n",
            "Loss:  0.6101164072751999 \n",
            "Classification loss: 1.1234307289123535 \n",
            "Regression loss: 0.04840104281902313\n",
            "----------------\n",
            "Loss:  0.6740945652127266 \n",
            "Classification loss: 1.236971378326416 \n",
            "Regression loss: 0.055608876049518585\n",
            "----------------\n",
            "Loss:  0.7959166262298822 \n",
            "Classification loss: 1.530171513557434 \n",
            "Regression loss: 0.0308308694511652\n",
            "----------------\n",
            "Loss:  0.616221321746707 \n",
            "Classification loss: 1.1722732782363892 \n",
            "Regression loss: 0.030084682628512383\n",
            "----------------\n",
            "Loss:  0.6009724140167236 \n",
            "Classification loss: 1.1340837478637695 \n",
            "Regression loss: 0.03393054008483887\n",
            "----------------\n",
            "Loss:  0.6881117708981037 \n",
            "Classification loss: 1.261641502380371 \n",
            "Regression loss: 0.05729101970791817\n",
            "----------------\n",
            "Loss:  0.6608919091522694 \n",
            "Classification loss: 1.2357028722763062 \n",
            "Regression loss: 0.04304047301411629\n",
            "----------------\n",
            "Loss:  0.7036712989211082 \n",
            "Classification loss: 1.3249150514602661 \n",
            "Regression loss: 0.04121377319097519\n",
            "----------------\n",
            "Loss:  0.6749356165528297 \n",
            "Classification loss: 1.210263729095459 \n",
            "Regression loss: 0.06980375200510025\n",
            "----------------\n",
            "Loss:  0.6901866514235735 \n",
            "Classification loss: 1.3191436529159546 \n",
            "Regression loss: 0.0306148249655962\n",
            "----------------\n",
            "Loss:  0.8294140547513962 \n",
            "Classification loss: 1.4985685348510742 \n",
            "Regression loss: 0.08012978732585907\n",
            "----------------\n",
            "Loss:  0.664356779307127 \n",
            "Classification loss: 1.2079352140426636 \n",
            "Regression loss: 0.06038917228579521\n",
            "----------------\n",
            "Loss:  0.7673318572342396 \n",
            "Classification loss: 1.44937002658844 \n",
            "Regression loss: 0.04264684394001961\n",
            "----------------\n",
            "Loss:  0.7042955309152603 \n",
            "Classification loss: 1.3402748107910156 \n",
            "Regression loss: 0.0341581255197525\n",
            "----------------\n",
            "Loss:  0.6934699639678001 \n",
            "Classification loss: 1.2722277641296387 \n",
            "Regression loss: 0.057356081902980804\n",
            "----------------\n",
            "Loss:  0.7300986871123314 \n",
            "Classification loss: 1.3408446311950684 \n",
            "Regression loss: 0.05967637151479721\n",
            "----------------\n",
            "Loss:  0.8023247271776199 \n",
            "Classification loss: 1.435685634613037 \n",
            "Regression loss: 0.08448190987110138\n",
            "----------------\n",
            "Loss:  0.6008230485022068 \n",
            "Classification loss: 1.1203818321228027 \n",
            "Regression loss: 0.040632132440805435\n",
            "----------------\n",
            "Loss:  0.6781076043844223 \n",
            "Classification loss: 1.230191707611084 \n",
            "Regression loss: 0.06301175057888031\n",
            "----------------\n",
            "Loss:  0.7040998935699463 \n",
            "Classification loss: 1.353521704673767 \n",
            "Regression loss: 0.027339041233062744\n",
            "----------------\n",
            "Loss:  0.6108160950243473 \n",
            "Classification loss: 1.130187749862671 \n",
            "Regression loss: 0.045722220093011856\n",
            "----------------\n",
            "Loss:  0.742752194404602 \n",
            "Classification loss: 1.4027107954025269 \n",
            "Regression loss: 0.04139679670333862\n",
            "----------------\n",
            "Loss:  0.5985588394105434 \n",
            "Classification loss: 1.1291857957839966 \n",
            "Regression loss: 0.03396594151854515\n",
            "----------------\n",
            "Loss:  0.5825222339481115 \n",
            "Classification loss: 1.1130640506744385 \n",
            "Regression loss: 0.025990208610892296\n",
            "----------------\n",
            "Loss:  0.670624628663063 \n",
            "Classification loss: 1.189422607421875 \n",
            "Regression loss: 0.07591332495212555\n",
            "----------------\n",
            "Loss:  0.8010289743542671 \n",
            "Classification loss: 1.5025289058685303 \n",
            "Regression loss: 0.049764521420001984\n",
            "----------------\n",
            "Loss:  0.6392200998961926 \n",
            "Classification loss: 1.1702821254730225 \n",
            "Regression loss: 0.05407903715968132\n",
            "----------------\n",
            "Loss:  0.6394049599766731 \n",
            "Classification loss: 1.1871421337127686 \n",
            "Regression loss: 0.04583389312028885\n",
            "----------------\n",
            "Loss:  0.7037182562053204 \n",
            "Classification loss: 1.2988014221191406 \n",
            "Regression loss: 0.054317545145750046\n",
            "----------------\n",
            "Loss:  0.6954620592296124 \n",
            "Classification loss: 1.3178173303604126 \n",
            "Regression loss: 0.03655339404940605\n",
            "----------------\n",
            "Loss:  0.6883283853530884 \n",
            "Classification loss: 1.232417345046997 \n",
            "Regression loss: 0.07211971282958984\n",
            "----------------\n",
            "Loss:  0.860513474792242 \n",
            "Classification loss: 1.6091697216033936 \n",
            "Regression loss: 0.05592861399054527\n",
            "----------------\n",
            "Loss:  0.7624438405036926 \n",
            "Classification loss: 1.394949197769165 \n",
            "Regression loss: 0.06496924161911011\n",
            "----------------\n",
            "Loss:  0.5919926539063454 \n",
            "Classification loss: 1.1021957397460938 \n",
            "Regression loss: 0.04089478403329849\n",
            "----------------\n",
            "Loss:  0.685791227966547 \n",
            "Classification loss: 1.269763469696045 \n",
            "Regression loss: 0.05090949311852455\n",
            "----------------\n",
            "Loss:  0.8567785546183586 \n",
            "Classification loss: 1.6260261535644531 \n",
            "Regression loss: 0.04376547783613205\n",
            "----------------\n",
            "Loss:  0.7086416073143482 \n",
            "Classification loss: 1.3382337093353271 \n",
            "Regression loss: 0.03952475264668465\n",
            "----------------\n",
            "Loss:  0.7699953243136406 \n",
            "Classification loss: 1.4547392129898071 \n",
            "Regression loss: 0.04262571781873703\n",
            "----------------\n",
            "Loss:  0.6183706223964691 \n",
            "Classification loss: 1.09479820728302 \n",
            "Regression loss: 0.0709715187549591\n",
            "----------------\n",
            "Loss:  0.6687903553247452 \n",
            "Classification loss: 1.2426080703735352 \n",
            "Regression loss: 0.0474863201379776\n",
            "----------------\n",
            "Loss:  0.706644382327795 \n",
            "Classification loss: 1.2891478538513184 \n",
            "Regression loss: 0.06207045540213585\n",
            "----------------\n",
            "Loss:  0.707311175763607 \n",
            "Classification loss: 1.3697255849838257 \n",
            "Regression loss: 0.022448383271694183\n",
            "----------------\n",
            "Loss:  0.8126406520605087 \n",
            "Classification loss: 1.5227439403533936 \n",
            "Regression loss: 0.05126868188381195\n",
            "----------------\n",
            "Loss:  0.6624122597277164 \n",
            "Classification loss: 1.2451810836791992 \n",
            "Regression loss: 0.03982171788811684\n",
            "----------------\n",
            "Loss:  0.7081047147512436 \n",
            "Classification loss: 1.3223977088928223 \n",
            "Regression loss: 0.04690586030483246\n",
            "----------------\n",
            "Loss:  0.7387741282582283 \n",
            "Classification loss: 1.3718396425247192 \n",
            "Regression loss: 0.05285430699586868\n",
            "----------------\n",
            "Loss:  0.7665214873850346 \n",
            "Classification loss: 1.4408220052719116 \n",
            "Regression loss: 0.04611048474907875\n",
            "----------------\n",
            "Loss:  0.6141218431293964 \n",
            "Classification loss: 1.148782730102539 \n",
            "Regression loss: 0.03973047807812691\n",
            "----------------\n",
            "Loss:  0.6574264168739319 \n",
            "Classification loss: 1.2114886045455933 \n",
            "Regression loss: 0.051682114601135254\n",
            "----------------\n",
            "Loss:  0.7587767094373703 \n",
            "Classification loss: 1.3897737264633179 \n",
            "Regression loss: 0.06388984620571136\n",
            "----------------\n",
            "Loss:  0.6611032374203205 \n",
            "Classification loss: 1.2639796733856201 \n",
            "Regression loss: 0.029113400727510452\n",
            "----------------\n",
            "Loss:  0.6939709261059761 \n",
            "Classification loss: 1.3228158950805664 \n",
            "Regression loss: 0.0325629785656929\n",
            "----------------\n",
            "Loss:  0.7117609456181526 \n",
            "Classification loss: 1.342958688735962 \n",
            "Regression loss: 0.04028160125017166\n",
            "----------------\n",
            "Loss:  0.6075940728187561 \n",
            "Classification loss: 1.1687755584716797 \n",
            "Regression loss: 0.02320629358291626\n",
            "----------------\n",
            "Loss:  0.6533535905182362 \n",
            "Classification loss: 1.2093249559402466 \n",
            "Regression loss: 0.04869111254811287\n",
            "----------------\n",
            "Loss:  0.6834330558776855 \n",
            "Classification loss: 1.267337441444397 \n",
            "Regression loss: 0.04976433515548706\n",
            "----------------\n",
            "Loss:  0.7642957791686058 \n",
            "Classification loss: 1.444598913192749 \n",
            "Regression loss: 0.04199632257223129\n",
            "----------------\n",
            "Loss:  0.7314511723816395 \n",
            "Classification loss: 1.4062199592590332 \n",
            "Regression loss: 0.02834119275212288\n",
            "----------------\n",
            "Loss:  0.6838703118264675 \n",
            "Classification loss: 1.265575885772705 \n",
            "Regression loss: 0.051082368940114975\n",
            "----------------\n",
            "Loss:  0.7416735365986824 \n",
            "Classification loss: 1.3809601068496704 \n",
            "Regression loss: 0.0511934831738472\n",
            "----------------\n",
            "Loss:  0.67661939188838 \n",
            "Classification loss: 1.2596780061721802 \n",
            "Regression loss: 0.04678038880228996\n",
            "----------------\n",
            "Loss:  0.6573059447109699 \n",
            "Classification loss: 1.2404146194458008 \n",
            "Regression loss: 0.037098634988069534\n",
            "----------------\n",
            "Loss:  0.6455222740769386 \n",
            "Classification loss: 1.1834914684295654 \n",
            "Regression loss: 0.053776539862155914\n",
            "----------------\n",
            "Loss:  0.6257801875472069 \n",
            "Classification loss: 1.2110891342163086 \n",
            "Regression loss: 0.020235620439052582\n",
            "----------------\n",
            "Loss:  0.6122424304485321 \n",
            "Classification loss: 1.1029372215270996 \n",
            "Regression loss: 0.0607738196849823\n",
            "----------------\n",
            "Loss:  0.7012060023844242 \n",
            "Classification loss: 1.3323384523391724 \n",
            "Regression loss: 0.03503677621483803\n",
            "----------------\n",
            "Loss:  0.5997105836868286 \n",
            "Classification loss: 1.1548256874084473 \n",
            "Regression loss: 0.02229773998260498\n",
            "----------------\n",
            "Loss:  0.6232861168682575 \n",
            "Classification loss: 1.1644803285598755 \n",
            "Regression loss: 0.04104595258831978\n",
            "----------------\n",
            "Loss:  0.5855274051427841 \n",
            "Classification loss: 1.0900845527648926 \n",
            "Regression loss: 0.04048512876033783\n",
            "----------------\n",
            "Loss:  0.7273166179656982 \n",
            "Classification loss: 1.3494110107421875 \n",
            "Regression loss: 0.05261111259460449\n",
            "----------------\n",
            "Loss:  0.7371253557503223 \n",
            "Classification loss: 1.4264404773712158 \n",
            "Regression loss: 0.023905117064714432\n",
            "----------------\n",
            "Loss:  0.7214518040418625 \n",
            "Classification loss: 1.3303842544555664 \n",
            "Regression loss: 0.056259676814079285\n",
            "----------------\n",
            "Loss:  0.6833365075290203 \n",
            "Classification loss: 1.2635619640350342 \n",
            "Regression loss: 0.05155552551150322\n",
            "----------------\n",
            "Loss:  0.6298905164003372 \n",
            "Classification loss: 1.1448860168457031 \n",
            "Regression loss: 0.05744750797748566\n",
            "----------------\n",
            "Loss:  0.8115581423044205 \n",
            "Classification loss: 1.4971096515655518 \n",
            "Regression loss: 0.06300331652164459\n",
            "----------------\n",
            "Loss:  0.7191364914178848 \n",
            "Classification loss: 1.3576838970184326 \n",
            "Regression loss: 0.04029454290866852\n",
            "----------------\n",
            "Loss:  0.6057951040565968 \n",
            "Classification loss: 1.162858247756958 \n",
            "Regression loss: 0.024365980178117752\n",
            "----------------\n",
            "Loss:  0.6339648552238941 \n",
            "Classification loss: 1.1536433696746826 \n",
            "Regression loss: 0.05714317038655281\n",
            "----------------\n",
            "Loss:  0.705868948251009 \n",
            "Classification loss: 1.317718267440796 \n",
            "Regression loss: 0.04700981453061104\n",
            "----------------\n",
            "Loss:  0.878411203622818 \n",
            "Classification loss: 1.6620171070098877 \n",
            "Regression loss: 0.047402650117874146\n",
            "----------------\n",
            "Loss:  0.6824727542698383 \n",
            "Classification loss: 1.269990086555481 \n",
            "Regression loss: 0.047477710992097855\n",
            "----------------\n",
            "Loss:  0.7340714856982231 \n",
            "Classification loss: 1.3721232414245605 \n",
            "Regression loss: 0.04800986498594284\n",
            "----------------\n",
            "Loss:  0.657400831580162 \n",
            "Classification loss: 1.2423604726791382 \n",
            "Regression loss: 0.03622059524059296\n",
            "----------------\n",
            "Loss:  0.6814190801233053 \n",
            "Classification loss: 1.318595051765442 \n",
            "Regression loss: 0.022121554240584373\n",
            "----------------\n",
            "Loss:  0.7045797929167747 \n",
            "Classification loss: 1.3250083923339844 \n",
            "Regression loss: 0.04207559674978256\n",
            "----------------\n",
            "Loss:  0.7755949348211288 \n",
            "Classification loss: 1.4394022226333618 \n",
            "Regression loss: 0.05589382350444794\n",
            "----------------\n",
            "Loss:  0.6715568415820599 \n",
            "Classification loss: 1.2712498903274536 \n",
            "Regression loss: 0.035931896418333054\n",
            "----------------\n",
            "Loss:  0.7933660484850407 \n",
            "Classification loss: 1.498130202293396 \n",
            "Regression loss: 0.04430094733834267\n",
            "----------------\n",
            "Loss:  0.6785018555819988 \n",
            "Classification loss: 1.3052623271942139 \n",
            "Regression loss: 0.02587069198489189\n",
            "----------------\n",
            "Loss:  0.8089338019490242 \n",
            "Classification loss: 1.43361234664917 \n",
            "Regression loss: 0.09212762862443924\n",
            "----------------\n",
            "Loss:  0.6829867660999298 \n",
            "Classification loss: 1.2706775665283203 \n",
            "Regression loss: 0.04764798283576965\n",
            "----------------\n",
            "Loss:  0.6985081396996975 \n",
            "Classification loss: 1.2896230220794678 \n",
            "Regression loss: 0.05369662865996361\n",
            "----------------\n",
            "Loss:  0.8676321543753147 \n",
            "Classification loss: 1.653632640838623 \n",
            "Regression loss: 0.04081583395600319\n",
            "----------------\n",
            "Loss:  0.7702015414834023 \n",
            "Classification loss: 1.4204421043395996 \n",
            "Regression loss: 0.05998048931360245\n",
            "----------------\n",
            "Loss:  0.7984515801072121 \n",
            "Classification loss: 1.4475808143615723 \n",
            "Regression loss: 0.07466117292642593\n",
            "----------------\n",
            "Loss:  0.7501901984214783 \n",
            "Classification loss: 1.3891870975494385 \n",
            "Regression loss: 0.05559664964675903\n",
            "----------------\n",
            "Loss:  0.7110693976283073 \n",
            "Classification loss: 1.3413244485855103 \n",
            "Regression loss: 0.040407173335552216\n",
            "----------------\n",
            "Loss:  0.6443102397024632 \n",
            "Classification loss: 1.1912071704864502 \n",
            "Regression loss: 0.04870665445923805\n",
            "----------------\n",
            "Loss:  0.7886591218411922 \n",
            "Classification loss: 1.4905260801315308 \n",
            "Regression loss: 0.043396081775426865\n",
            "----------------\n",
            "Loss:  0.6860129870474339 \n",
            "Classification loss: 1.3000658750534058 \n",
            "Regression loss: 0.03598004952073097\n",
            "----------------\n",
            "Loss:  0.7550943270325661 \n",
            "Classification loss: 1.410083293914795 \n",
            "Regression loss: 0.05005268007516861\n",
            "----------------\n",
            "Loss:  0.5868520401418209 \n",
            "Classification loss: 1.1029975414276123 \n",
            "Regression loss: 0.035353269428014755\n",
            "----------------\n",
            "Loss:  0.706230852752924 \n",
            "Classification loss: 1.3365492820739746 \n",
            "Regression loss: 0.03795621171593666\n",
            "----------------\n",
            "Loss:  0.6560316905379295 \n",
            "Classification loss: 1.2004667520523071 \n",
            "Regression loss: 0.05579831451177597\n",
            "----------------\n",
            "Loss:  0.7357219718396664 \n",
            "Classification loss: 1.3906444311141968 \n",
            "Regression loss: 0.04039975628256798\n",
            "----------------\n",
            "Loss:  0.7596710585057735 \n",
            "Classification loss: 1.4088871479034424 \n",
            "Regression loss: 0.05522748455405235\n",
            "----------------\n",
            "Loss:  0.6662830039858818 \n",
            "Classification loss: 1.2209514379501343 \n",
            "Regression loss: 0.05580728501081467\n",
            "----------------\n",
            "Loss:  0.6429087929427624 \n",
            "Classification loss: 1.2211039066314697 \n",
            "Regression loss: 0.03235683962702751\n",
            "----------------\n",
            "Loss:  0.7245914116501808 \n",
            "Classification loss: 1.3426636457443237 \n",
            "Regression loss: 0.05325958877801895\n",
            "----------------\n",
            "Loss:  0.7000658884644508 \n",
            "Classification loss: 1.3321422338485718 \n",
            "Regression loss: 0.03399477154016495\n",
            "----------------\n",
            "Loss:  0.7141457311809063 \n",
            "Classification loss: 1.3422377109527588 \n",
            "Regression loss: 0.0430268757045269\n",
            "----------------\n",
            "Loss:  0.7782862633466721 \n",
            "Classification loss: 1.465477466583252 \n",
            "Regression loss: 0.04554753005504608\n",
            "----------------\n",
            "Loss:  0.7241329699754715 \n",
            "Classification loss: 1.3665519952774048 \n",
            "Regression loss: 0.040856972336769104\n",
            "----------------\n",
            "Loss:  0.684980072081089 \n",
            "Classification loss: 1.2897331714630127 \n",
            "Regression loss: 0.04011348634958267\n",
            "----------------\n",
            "Loss:  0.6940808296203613 \n",
            "Classification loss: 1.2738572359085083 \n",
            "Regression loss: 0.05715221166610718\n",
            "----------------\n",
            "Loss:  0.7334255464375019 \n",
            "Classification loss: 1.3865199089050293 \n",
            "Regression loss: 0.04016559198498726\n",
            "----------------\n",
            "Loss:  0.6454417705535889 \n",
            "Classification loss: 1.2056386470794678 \n",
            "Regression loss: 0.04262244701385498\n",
            "----------------\n",
            "Loss:  0.5991930551826954 \n",
            "Classification loss: 1.1322131156921387 \n",
            "Regression loss: 0.03308649733662605\n",
            "----------------\n",
            "Loss:  0.7454430758953094 \n",
            "Classification loss: 1.3614625930786133 \n",
            "Regression loss: 0.06471177935600281\n",
            "----------------\n",
            "Loss:  0.610545426607132 \n",
            "Classification loss: 1.1362693309783936 \n",
            "Regression loss: 0.04241076111793518\n",
            "----------------\n",
            "Loss:  0.7623844370245934 \n",
            "Classification loss: 1.4469490051269531 \n",
            "Regression loss: 0.03890993446111679\n",
            "----------------\n",
            "Loss:  0.6719556897878647 \n",
            "Classification loss: 1.238015055656433 \n",
            "Regression loss: 0.05294816195964813\n",
            "----------------\n",
            "Loss:  0.7863785400986671 \n",
            "Classification loss: 1.4679632186889648 \n",
            "Regression loss: 0.05239693075418472\n",
            "----------------\n",
            "Loss:  0.6278763711452484 \n",
            "Classification loss: 1.1906800270080566 \n",
            "Regression loss: 0.03253635764122009\n",
            "----------------\n",
            "Loss:  0.6559420377016068 \n",
            "Classification loss: 1.1876648664474487 \n",
            "Regression loss: 0.062109604477882385\n",
            "----------------\n",
            "Loss:  0.6583545133471489 \n",
            "Classification loss: 1.1816041469573975 \n",
            "Regression loss: 0.06755243986845016\n",
            "----------------\n",
            "Loss:  0.8288690112531185 \n",
            "Classification loss: 1.5749902725219727 \n",
            "Regression loss: 0.04137387499213219\n",
            "----------------\n",
            "Loss:  0.6207787692546844 \n",
            "Classification loss: 1.1376521587371826 \n",
            "Regression loss: 0.05195268988609314\n",
            "----------------\n",
            "Loss:  0.6506968773901463 \n",
            "Classification loss: 1.1907517910003662 \n",
            "Regression loss: 0.05532098188996315\n",
            "----------------\n",
            "Loss:  0.7267647609114647 \n",
            "Classification loss: 1.359195590019226 \n",
            "Regression loss: 0.047166965901851654\n",
            "----------------\n",
            "Loss:  0.6760995239019394 \n",
            "Classification loss: 1.2482233047485352 \n",
            "Regression loss: 0.051987871527671814\n",
            "----------------\n",
            "Loss:  0.6026983931660652 \n",
            "Classification loss: 1.1288961172103882 \n",
            "Regression loss: 0.038250334560871124\n",
            "----------------\n",
            "Loss:  0.6715081110596657 \n",
            "Classification loss: 1.2470993995666504 \n",
            "Regression loss: 0.047958411276340485\n",
            "----------------\n",
            "Loss:  0.7236646413803101 \n",
            "Classification loss: 1.3652750253677368 \n",
            "Regression loss: 0.04102712869644165\n",
            "----------------\n",
            "Loss:  0.6289326548576355 \n",
            "Classification loss: 1.1661443710327148 \n",
            "Regression loss: 0.045860469341278076\n",
            "----------------\n",
            "Loss:  0.6702501066029072 \n",
            "Classification loss: 1.2249085903167725 \n",
            "Regression loss: 0.05779581144452095\n",
            "----------------\n",
            "Loss:  0.7526079602539539 \n",
            "Classification loss: 1.4256348609924316 \n",
            "Regression loss: 0.03979052975773811\n",
            "----------------\n",
            "Loss:  0.7099611237645149 \n",
            "Classification loss: 1.3450716733932495 \n",
            "Regression loss: 0.03742528706789017\n",
            "----------------\n",
            "Loss:  0.6545352786779404 \n",
            "Classification loss: 1.234623670578003 \n",
            "Regression loss: 0.037223443388938904\n",
            "----------------\n",
            "Loss:  0.573916845023632 \n",
            "Classification loss: 1.0698800086975098 \n",
            "Regression loss: 0.03897684067487717\n",
            "----------------\n",
            "Loss:  0.6879986897110939 \n",
            "Classification loss: 1.3193652629852295 \n",
            "Regression loss: 0.028316058218479156\n",
            "----------------\n",
            "Loss:  0.7353317439556122 \n",
            "Classification loss: 1.3610996007919312 \n",
            "Regression loss: 0.054781943559646606\n",
            "----------------\n",
            "Loss:  0.7346808314323425 \n",
            "Classification loss: 1.367753505706787 \n",
            "Regression loss: 0.050804078578948975\n",
            "----------------\n",
            "Loss:  0.6324432641267776 \n",
            "Classification loss: 1.1587854623794556 \n",
            "Regression loss: 0.053050532937049866\n",
            "----------------\n",
            "Loss:  0.800366960465908 \n",
            "Classification loss: 1.4872689247131348 \n",
            "Regression loss: 0.05673249810934067\n",
            "----------------\n",
            "Loss:  0.6634257435798645 \n",
            "Classification loss: 1.2316781282424927 \n",
            "Regression loss: 0.047586679458618164\n",
            "----------------\n",
            "Loss:  0.6178441271185875 \n",
            "Classification loss: 1.1537926197052002 \n",
            "Regression loss: 0.040947817265987396\n",
            "----------------\n",
            "Loss:  0.7503367513418198 \n",
            "Classification loss: 1.39469575881958 \n",
            "Regression loss: 0.052988871932029724\n",
            "----------------\n",
            "Loss:  0.7542182579636574 \n",
            "Classification loss: 1.4095170497894287 \n",
            "Regression loss: 0.049459733068943024\n",
            "----------------\n",
            "Loss:  0.6634834110736847 \n",
            "Classification loss: 1.1943318843841553 \n",
            "Regression loss: 0.06631746888160706\n",
            "----------------\n",
            "Loss:  0.7050984613597393 \n",
            "Classification loss: 1.304936408996582 \n",
            "Regression loss: 0.05263025686144829\n",
            "----------------\n",
            "Loss:  0.6998343616724014 \n",
            "Classification loss: 1.2644115686416626 \n",
            "Regression loss: 0.06762857735157013\n",
            "----------------\n",
            "Loss:  0.6138385310769081 \n",
            "Classification loss: 1.114530086517334 \n",
            "Regression loss: 0.05657348781824112\n",
            "----------------\n",
            "Loss:  0.641242042183876 \n",
            "Classification loss: 1.2144556045532227 \n",
            "Regression loss: 0.03401423990726471\n",
            "----------------\n",
            "Loss:  0.6399203203618526 \n",
            "Classification loss: 1.2118780612945557 \n",
            "Regression loss: 0.033981289714574814\n",
            "----------------\n",
            "Loss:  0.8314043506979942 \n",
            "Classification loss: 1.5496197938919067 \n",
            "Regression loss: 0.05659445375204086\n",
            "----------------\n",
            "Loss:  0.5539360847324133 \n",
            "Classification loss: 1.0616190433502197 \n",
            "Regression loss: 0.02312656305730343\n",
            "----------------\n",
            "Loss:  0.7585468143224716 \n",
            "Classification loss: 1.417820692062378 \n",
            "Regression loss: 0.049636468291282654\n",
            "----------------\n",
            "Loss:  0.7292083129286766 \n",
            "Classification loss: 1.3401106595993042 \n",
            "Regression loss: 0.059152983129024506\n",
            "----------------\n",
            "Loss:  0.7390260323882103 \n",
            "Classification loss: 1.4292954206466675 \n",
            "Regression loss: 0.024378322064876556\n",
            "----------------\n",
            "Loss:  0.682261161506176 \n",
            "Classification loss: 1.2955138683319092 \n",
            "Regression loss: 0.034504227340221405\n",
            "----------------\n",
            "Loss:  0.6738832332193851 \n",
            "Classification loss: 1.2661160230636597 \n",
            "Regression loss: 0.04082522168755531\n",
            "----------------\n",
            "Loss:  0.5966852679848671 \n",
            "Classification loss: 1.0985684394836426 \n",
            "Regression loss: 0.04740104824304581\n",
            "----------------\n",
            "Loss:  0.727935504168272 \n",
            "Classification loss: 1.3486589193344116 \n",
            "Regression loss: 0.05360604450106621\n",
            "----------------\n",
            "Loss:  0.7269630581140518 \n",
            "Classification loss: 1.3534421920776367 \n",
            "Regression loss: 0.05024196207523346\n",
            "----------------\n",
            "Loss:  0.7034306637942791 \n",
            "Classification loss: 1.3052951097488403 \n",
            "Regression loss: 0.05078310891985893\n",
            "----------------\n",
            "Loss:  0.667787604033947 \n",
            "Classification loss: 1.255547046661377 \n",
            "Regression loss: 0.040014080703258514\n",
            "----------------\n",
            "Loss:  0.8373734131455421 \n",
            "Classification loss: 1.5882062911987305 \n",
            "Regression loss: 0.04327026754617691\n",
            "----------------\n",
            "Loss:  0.6895024925470352 \n",
            "Classification loss: 1.2811307907104492 \n",
            "Regression loss: 0.04893709719181061\n",
            "----------------\n",
            "Loss:  0.7389689609408379 \n",
            "Classification loss: 1.3673216104507446 \n",
            "Regression loss: 0.055308155715465546\n",
            "----------------\n",
            "Loss:  0.7255997359752655 \n",
            "Classification loss: 1.362536907196045 \n",
            "Regression loss: 0.04433128237724304\n",
            "----------------\n",
            "Loss:  0.5794715099036694 \n",
            "Classification loss: 1.0837146043777466 \n",
            "Regression loss: 0.037614207714796066\n",
            "----------------\n",
            "Loss:  0.7206119671463966 \n",
            "Classification loss: 1.33010733127594 \n",
            "Regression loss: 0.055558301508426666\n",
            "----------------\n",
            "Loss:  0.567821815609932 \n",
            "Classification loss: 1.0790636539459229 \n",
            "Regression loss: 0.02828998863697052\n",
            "----------------\n",
            "Loss:  0.6336297318339348 \n",
            "Classification loss: 1.2056282758712769 \n",
            "Regression loss: 0.030815593898296356\n",
            "----------------\n",
            "Loss:  0.7271132990717888 \n",
            "Classification loss: 1.3380569219589233 \n",
            "Regression loss: 0.05808483809232712\n",
            "----------------\n",
            "Loss:  0.7374691069126129 \n",
            "Classification loss: 1.4347257614135742 \n",
            "Regression loss: 0.020106226205825806\n",
            "----------------\n",
            "Loss:  0.7035813592374325 \n",
            "Classification loss: 1.2988338470458984 \n",
            "Regression loss: 0.05416443571448326\n",
            "----------------\n",
            "Loss:  0.750972256064415 \n",
            "Classification loss: 1.4061561822891235 \n",
            "Regression loss: 0.04789416491985321\n",
            "----------------\n",
            "Loss:  0.7733348123729229 \n",
            "Classification loss: 1.456813097000122 \n",
            "Regression loss: 0.04492826387286186\n",
            "----------------\n",
            "Loss:  0.6841871067881584 \n",
            "Classification loss: 1.272484540939331 \n",
            "Regression loss: 0.04794483631849289\n",
            "----------------\n",
            "Loss:  0.6780488938093185 \n",
            "Classification loss: 1.276879072189331 \n",
            "Regression loss: 0.039609357714653015\n",
            "----------------\n",
            "Loss:  0.8339044824242592 \n",
            "Classification loss: 1.521320104598999 \n",
            "Regression loss: 0.07324443012475967\n",
            "----------------\n",
            "Loss:  0.629152063280344 \n",
            "Classification loss: 1.1664035320281982 \n",
            "Regression loss: 0.04595029726624489\n",
            "----------------\n",
            "Loss:  0.8625673651695251 \n",
            "Classification loss: 1.6015866994857788 \n",
            "Regression loss: 0.06177401542663574\n",
            "----------------\n",
            "Loss:  0.7702471408993006 \n",
            "Classification loss: 1.4821302890777588 \n",
            "Regression loss: 0.02918199636042118\n",
            "----------------\n",
            "Loss:  0.6769973933696747 \n",
            "Classification loss: 1.2020478248596191 \n",
            "Regression loss: 0.07597348093986511\n",
            "----------------\n",
            "Loss:  0.6641434915363789 \n",
            "Classification loss: 1.2788240909576416 \n",
            "Regression loss: 0.02473144605755806\n",
            "----------------\n",
            "Loss:  0.760924156755209 \n",
            "Classification loss: 1.433881163597107 \n",
            "Regression loss: 0.0439835749566555\n",
            "----------------\n",
            "Loss:  0.6357944309711456 \n",
            "Classification loss: 1.2154030799865723 \n",
            "Regression loss: 0.028092890977859497\n",
            "----------------\n",
            "Loss:  0.5773055944591761 \n",
            "Classification loss: 1.0956330299377441 \n",
            "Regression loss: 0.029489079490303993\n",
            "----------------\n",
            "Loss:  0.6408249028027058 \n",
            "Classification loss: 1.2016780376434326 \n",
            "Regression loss: 0.039985883980989456\n",
            "----------------\n",
            "Loss:  0.6379629522562027 \n",
            "Classification loss: 1.160070538520813 \n",
            "Regression loss: 0.057927682995796204\n",
            "----------------\n",
            "Loss:  0.5686164516955614 \n",
            "Classification loss: 1.0849086046218872 \n",
            "Regression loss: 0.026162149384617805\n",
            "----------------\n",
            "Loss:  0.6649471633136272 \n",
            "Classification loss: 1.280181646347046 \n",
            "Regression loss: 0.024856340140104294\n",
            "----------------\n",
            "Loss:  0.701833613216877 \n",
            "Classification loss: 1.3143651485443115 \n",
            "Regression loss: 0.04465103894472122\n",
            "----------------\n",
            "Loss:  0.608015313744545 \n",
            "Classification loss: 1.1230918169021606 \n",
            "Regression loss: 0.04646940529346466\n",
            "----------------\n",
            "Loss:  0.7397125661373138 \n",
            "Classification loss: 1.3629109859466553 \n",
            "Regression loss: 0.058257073163986206\n",
            "----------------\n",
            "Loss:  0.6449934653937817 \n",
            "Classification loss: 1.2050426006317139 \n",
            "Regression loss: 0.04247216507792473\n",
            "----------------\n",
            "Loss:  0.6477469950914383 \n",
            "Classification loss: 1.2209866046905518 \n",
            "Regression loss: 0.037253692746162415\n",
            "----------------\n",
            "Loss:  0.6911889687180519 \n",
            "Classification loss: 1.254202127456665 \n",
            "Regression loss: 0.06408790498971939\n",
            "----------------\n",
            "Loss:  0.6452813297510147 \n",
            "Classification loss: 1.162898063659668 \n",
            "Regression loss: 0.06383229792118073\n",
            "----------------\n",
            "Loss:  0.7309612706303596 \n",
            "Classification loss: 1.382854700088501 \n",
            "Regression loss: 0.03953392058610916\n",
            "----------------\n",
            "Loss:  0.6392502523958683 \n",
            "Classification loss: 1.2137020826339722 \n",
            "Regression loss: 0.03239921107888222\n",
            "----------------\n",
            "Loss:  0.6201672330498695 \n",
            "Classification loss: 1.1698808670043945 \n",
            "Regression loss: 0.03522679954767227\n",
            "----------------\n",
            "Loss:  0.6620525419712067 \n",
            "Classification loss: 1.227963924407959 \n",
            "Regression loss: 0.04807057976722717\n",
            "----------------\n",
            "Loss:  0.6375042200088501 \n",
            "Classification loss: 1.1750915050506592 \n",
            "Regression loss: 0.04995846748352051\n",
            "----------------\n",
            "Loss:  0.707306332886219 \n",
            "Classification loss: 1.3194224834442139 \n",
            "Regression loss: 0.04759509116411209\n",
            "----------------\n",
            "Loss:  0.7066750973463058 \n",
            "Classification loss: 1.2860527038574219 \n",
            "Regression loss: 0.06364874541759491\n",
            "----------------\n",
            "Loss:  0.6228900253772736 \n",
            "Classification loss: 1.1268596649169922 \n",
            "Regression loss: 0.059460192918777466\n",
            "----------------\n",
            "Loss:  0.8538657240569592 \n",
            "Classification loss: 1.6036499738693237 \n",
            "Regression loss: 0.05204073712229729\n",
            "----------------\n",
            "Loss:  0.7816094495356083 \n",
            "Classification loss: 1.4450819492340088 \n",
            "Regression loss: 0.0590684749186039\n",
            "----------------\n",
            "Loss:  0.7777292020618916 \n",
            "Classification loss: 1.4347283840179443 \n",
            "Regression loss: 0.06036501005291939\n",
            "----------------\n",
            "Loss:  0.6466459222137928 \n",
            "Classification loss: 1.2165515422821045 \n",
            "Regression loss: 0.038370151072740555\n",
            "----------------\n",
            "Loss:  0.6137619875371456 \n",
            "Classification loss: 1.1715240478515625 \n",
            "Regression loss: 0.027999963611364365\n",
            "----------------\n",
            "Loss:  0.693420983850956 \n",
            "Classification loss: 1.2712042331695557 \n",
            "Regression loss: 0.05781886726617813\n",
            "----------------\n",
            "Loss:  0.7109502553939819 \n",
            "Classification loss: 1.2975103855133057 \n",
            "Regression loss: 0.0621950626373291\n",
            "----------------\n",
            "Loss:  0.639752421528101 \n",
            "Classification loss: 1.1632450819015503 \n",
            "Regression loss: 0.05812988057732582\n",
            "----------------\n",
            "Loss:  0.737196534872055 \n",
            "Classification loss: 1.3041582107543945 \n",
            "Regression loss: 0.08511742949485779\n",
            "----------------\n",
            "Loss:  0.7303961366415024 \n",
            "Classification loss: 1.3617452383041382 \n",
            "Regression loss: 0.04952351748943329\n",
            "----------------\n",
            "Loss:  0.619331419467926 \n",
            "Classification loss: 1.1625128984451294 \n",
            "Regression loss: 0.03807497024536133\n",
            "----------------\n",
            "Loss:  0.6987392641603947 \n",
            "Classification loss: 1.3217480182647705 \n",
            "Regression loss: 0.037865255028009415\n",
            "----------------\n",
            "Loss:  0.7096462920308113 \n",
            "Classification loss: 1.3049695491790771 \n",
            "Regression loss: 0.057161517441272736\n",
            "----------------\n",
            "Loss:  0.8053558021783829 \n",
            "Classification loss: 1.4641474485397339 \n",
            "Regression loss: 0.07328207790851593\n",
            "----------------\n",
            "Loss:  0.7113403417170048 \n",
            "Classification loss: 1.322026252746582 \n",
            "Regression loss: 0.05032721534371376\n",
            "----------------\n",
            "Loss:  0.5905197560787201 \n",
            "Classification loss: 1.0908966064453125 \n",
            "Regression loss: 0.04507145285606384\n",
            "----------------\n",
            "Loss:  0.7690838761627674 \n",
            "Classification loss: 1.4282653331756592 \n",
            "Regression loss: 0.05495120957493782\n",
            "----------------\n",
            "Loss:  0.7754119038581848 \n",
            "Classification loss: 1.4218292236328125 \n",
            "Regression loss: 0.06449729204177856\n",
            "----------------\n",
            "Loss:  0.6550096720457077 \n",
            "Classification loss: 1.188590168952942 \n",
            "Regression loss: 0.060714587569236755\n",
            "----------------\n",
            "Loss:  0.7117716819047928 \n",
            "Classification loss: 1.2928826808929443 \n",
            "Regression loss: 0.06533034145832062\n",
            "----------------\n",
            "Loss:  0.7666165344417095 \n",
            "Classification loss: 1.4233933687210083 \n",
            "Regression loss: 0.05491985008120537\n",
            "----------------\n",
            "Loss:  0.9091965854167938 \n",
            "Classification loss: 1.6702823638916016 \n",
            "Regression loss: 0.07405540347099304\n",
            "----------------\n",
            "Loss:  0.7034177184104919 \n",
            "Classification loss: 1.3189475536346436 \n",
            "Regression loss: 0.043943941593170166\n",
            "----------------\n",
            "Loss:  0.5853172764182091 \n",
            "Classification loss: 1.0870815515518188 \n",
            "Regression loss: 0.04177650064229965\n",
            "----------------\n",
            "Loss:  0.6842329278588295 \n",
            "Classification loss: 1.3010287284851074 \n",
            "Regression loss: 0.03371856361627579\n",
            "----------------\n",
            "Loss:  0.7010642625391483 \n",
            "Classification loss: 1.307478666305542 \n",
            "Regression loss: 0.047324929386377335\n",
            "----------------\n",
            "Loss:  0.6705523878335953 \n",
            "Classification loss: 1.2689282894134521 \n",
            "Regression loss: 0.0360882431268692\n",
            "----------------\n",
            "Loss:  0.662855226546526 \n",
            "Classification loss: 1.2796618938446045 \n",
            "Regression loss: 0.02302427962422371\n",
            "----------------\n",
            "Loss:  0.573565311729908 \n",
            "Classification loss: 1.0717272758483887 \n",
            "Regression loss: 0.037701673805713654\n",
            "----------------\n",
            "Loss:  0.5837997943162918 \n",
            "Classification loss: 1.0363903045654297 \n",
            "Regression loss: 0.06560464203357697\n",
            "----------------\n",
            "Loss:  0.7064633220434189 \n",
            "Classification loss: 1.329949975013733 \n",
            "Regression loss: 0.04148833453655243\n",
            "----------------\n",
            "Loss:  0.797196377068758 \n",
            "Classification loss: 1.489438533782959 \n",
            "Regression loss: 0.05247711017727852\n",
            "----------------\n",
            "Loss:  0.7140431255102158 \n",
            "Classification loss: 1.323864459991455 \n",
            "Regression loss: 0.05211089551448822\n",
            "----------------\n",
            "Loss:  0.650690171867609 \n",
            "Classification loss: 1.248579978942871 \n",
            "Regression loss: 0.026400182396173477\n",
            "----------------\n",
            "Loss:  0.6577620804309845 \n",
            "Classification loss: 1.1757233142852783 \n",
            "Regression loss: 0.06990042328834534\n",
            "----------------\n",
            "Loss:  0.6497441530227661 \n",
            "Classification loss: 1.1765714883804321 \n",
            "Regression loss: 0.06145840883255005\n",
            "----------------\n",
            "Loss:  0.6665355488657951 \n",
            "Classification loss: 1.2346243858337402 \n",
            "Regression loss: 0.04922335594892502\n",
            "----------------\n",
            "Loss:  0.7105612456798553 \n",
            "Classification loss: 1.3446648120880127 \n",
            "Regression loss: 0.038228839635849\n",
            "----------------\n",
            "Loss:  0.7380826026201248 \n",
            "Classification loss: 1.3718311786651611 \n",
            "Regression loss: 0.05216701328754425\n",
            "----------------\n",
            "Loss:  0.6441203877329826 \n",
            "Classification loss: 1.2083306312561035 \n",
            "Regression loss: 0.03995507210493088\n",
            "----------------\n",
            "Loss:  0.6246872171759605 \n",
            "Classification loss: 1.1649342775344849 \n",
            "Regression loss: 0.04222007840871811\n",
            "----------------\n",
            "Loss:  0.6808463111519814 \n",
            "Classification loss: 1.2529710531234741 \n",
            "Regression loss: 0.05436078459024429\n",
            "----------------\n",
            "Loss:  0.6295994743704796 \n",
            "Classification loss: 1.1732808351516724 \n",
            "Regression loss: 0.0429590567946434\n",
            "----------------\n",
            "Loss:  0.6459633372724056 \n",
            "Classification loss: 1.191903829574585 \n",
            "Regression loss: 0.050011422485113144\n",
            "----------------\n",
            "Loss:  0.7017759010195732 \n",
            "Classification loss: 1.3095942735671997 \n",
            "Regression loss: 0.04697876423597336\n",
            "----------------\n",
            "Loss:  0.5914760902523994 \n",
            "Classification loss: 1.0786571502685547 \n",
            "Regression loss: 0.0521475151181221\n",
            "----------------\n",
            "Loss:  0.7353039979934692 \n",
            "Classification loss: 1.3330128192901611 \n",
            "Regression loss: 0.06879758834838867\n",
            "----------------\n",
            "Loss:  0.7319386601448059 \n",
            "Classification loss: 1.3470826148986816 \n",
            "Regression loss: 0.05839735269546509\n",
            "----------------\n",
            "Loss:  0.7431738115847111 \n",
            "Classification loss: 1.3931732177734375 \n",
            "Regression loss: 0.046587202697992325\n",
            "----------------\n",
            "Loss:  0.6138466373085976 \n",
            "Classification loss: 1.1506211757659912 \n",
            "Regression loss: 0.03853604942560196\n",
            "----------------\n",
            "Loss:  0.6217841170728207 \n",
            "Classification loss: 1.1666147708892822 \n",
            "Regression loss: 0.03847673162817955\n",
            "----------------\n",
            "Loss:  0.73250912129879 \n",
            "Classification loss: 1.3709666728973389 \n",
            "Regression loss: 0.047025784850120544\n",
            "----------------\n",
            "Loss:  0.8529390841722488 \n",
            "Classification loss: 1.5971165895462036 \n",
            "Regression loss: 0.054380789399147034\n",
            "----------------\n",
            "Loss:  0.5958239585161209 \n",
            "Classification loss: 1.1124577522277832 \n",
            "Regression loss: 0.03959508240222931\n",
            "----------------\n",
            "Loss:  0.6398372873663902 \n",
            "Classification loss: 1.1951751708984375 \n",
            "Regression loss: 0.04224970191717148\n",
            "----------------\n",
            "Loss:  0.6566853746771812 \n",
            "Classification loss: 1.2354950904846191 \n",
            "Regression loss: 0.038937829434871674\n",
            "----------------\n",
            "Loss:  0.7678751684725285 \n",
            "Classification loss: 1.460526943206787 \n",
            "Regression loss: 0.0376116968691349\n",
            "----------------\n",
            "Loss:  0.6406068876385689 \n",
            "Classification loss: 1.1718451976776123 \n",
            "Regression loss: 0.054684288799762726\n",
            "----------------\n",
            "Loss:  0.6033229306340218 \n",
            "Classification loss: 1.0790835618972778 \n",
            "Regression loss: 0.06378114968538284\n",
            "----------------\n",
            "Loss:  0.7003779709339142 \n",
            "Classification loss: 1.2684483528137207 \n",
            "Regression loss: 0.06615379452705383\n",
            "----------------\n",
            "Loss:  0.6929703429341316 \n",
            "Classification loss: 1.3340651988983154 \n",
            "Regression loss: 0.025937743484973907\n",
            "----------------\n",
            "Loss:  0.6425592787563801 \n",
            "Classification loss: 1.1783779859542847 \n",
            "Regression loss: 0.05337028577923775\n",
            "----------------\n",
            "Loss:  0.7546068876981735 \n",
            "Classification loss: 1.3864699602127075 \n",
            "Regression loss: 0.06137190759181976\n",
            "----------------\n",
            "Loss:  0.6915652230381966 \n",
            "Classification loss: 1.2690457105636597 \n",
            "Regression loss: 0.05704236775636673\n",
            "----------------\n",
            "Loss:  0.5981551185250282 \n",
            "Classification loss: 1.134719967842102 \n",
            "Regression loss: 0.030795134603977203\n",
            "----------------\n",
            "Loss:  0.7210015878081322 \n",
            "Classification loss: 1.307323932647705 \n",
            "Regression loss: 0.06733962148427963\n",
            "----------------\n",
            "Loss:  0.6088391058146954 \n",
            "Classification loss: 1.1633814573287964 \n",
            "Regression loss: 0.027148377150297165\n",
            "----------------\n",
            "Loss:  0.7093575373291969 \n",
            "Classification loss: 1.3359545469284058 \n",
            "Regression loss: 0.04138026386499405\n",
            "----------------\n",
            "Loss:  0.8625733852386475 \n",
            "Classification loss: 1.596077561378479 \n",
            "Regression loss: 0.06453460454940796\n",
            "----------------\n",
            "Loss:  0.7260663509368896 \n",
            "Classification loss: 1.338953971862793 \n",
            "Regression loss: 0.056589365005493164\n",
            "----------------\n",
            "Loss:  0.6641384735703468 \n",
            "Classification loss: 1.2280552387237549 \n",
            "Regression loss: 0.05011085420846939\n",
            "----------------\n",
            "Loss:  0.7261105477809906 \n",
            "Classification loss: 1.3810548782348633 \n",
            "Regression loss: 0.03558310866355896\n",
            "----------------\n",
            "Loss:  0.6398118957877159 \n",
            "Classification loss: 1.2096283435821533 \n",
            "Regression loss: 0.03499772399663925\n",
            "----------------\n",
            "Loss:  0.7495156228542328 \n",
            "Classification loss: 1.4077694416046143 \n",
            "Regression loss: 0.04563090205192566\n",
            "----------------\n",
            "Loss:  0.6751306876540184 \n",
            "Classification loss: 1.2555320262908936 \n",
            "Regression loss: 0.047364674508571625\n",
            "----------------\n",
            "Loss:  0.5709313899278641 \n",
            "Classification loss: 1.0737011432647705 \n",
            "Regression loss: 0.03408081829547882\n",
            "----------------\n",
            "Loss:  0.747674111276865 \n",
            "Classification loss: 1.4219932556152344 \n",
            "Regression loss: 0.03667748346924782\n",
            "----------------\n",
            "Loss:  0.7893095314502716 \n",
            "Classification loss: 1.4593582153320312 \n",
            "Regression loss: 0.05963042378425598\n",
            "----------------\n",
            "Loss:  0.7010413333773613 \n",
            "Classification loss: 1.2576950788497925 \n",
            "Regression loss: 0.07219379395246506\n",
            "----------------\n",
            "Loss:  0.5876845866441727 \n",
            "Classification loss: 1.0722124576568604 \n",
            "Regression loss: 0.05157835781574249\n",
            "----------------\n",
            "Loss:  0.7989564128220081 \n",
            "Classification loss: 1.4787325859069824 \n",
            "Regression loss: 0.05959011986851692\n",
            "----------------\n",
            "Loss:  0.6921151541173458 \n",
            "Classification loss: 1.2731437683105469 \n",
            "Regression loss: 0.05554326996207237\n",
            "----------------\n",
            "Loss:  0.6736328676342964 \n",
            "Classification loss: 1.2462573051452637 \n",
            "Regression loss: 0.05050421506166458\n",
            "----------------\n",
            "Loss:  0.6941883228719234 \n",
            "Classification loss: 1.3021793365478516 \n",
            "Regression loss: 0.043098654597997665\n",
            "----------------\n",
            "Loss:  0.7186195962131023 \n",
            "Classification loss: 1.3201769590377808 \n",
            "Regression loss: 0.05853111669421196\n",
            "----------------\n",
            "Loss:  0.7988648004829884 \n",
            "Classification loss: 1.4843538999557495 \n",
            "Regression loss: 0.0566878505051136\n",
            "----------------\n",
            "Loss:  0.753963328897953 \n",
            "Classification loss: 1.4238698482513428 \n",
            "Regression loss: 0.04202840477228165\n",
            "----------------\n",
            "Loss:  0.5430852640420198 \n",
            "Classification loss: 1.0346897840499878 \n",
            "Regression loss: 0.025740372017025948\n",
            "----------------\n",
            "Loss:  0.7438633255660534 \n",
            "Classification loss: 1.4258003234863281 \n",
            "Regression loss: 0.030963163822889328\n",
            "----------------\n",
            "Loss:  0.8487516492605209 \n",
            "Classification loss: 1.578521490097046 \n",
            "Regression loss: 0.059490904211997986\n",
            "----------------\n",
            "Loss:  0.6555003635585308 \n",
            "Classification loss: 1.1963388919830322 \n",
            "Regression loss: 0.057330917567014694\n",
            "----------------\n",
            "Loss:  0.7507393136620522 \n",
            "Classification loss: 1.3339085578918457 \n",
            "Regression loss: 0.0837850347161293\n",
            "----------------\n",
            "Loss:  0.6366295889019966 \n",
            "Classification loss: 1.2033019065856934 \n",
            "Regression loss: 0.03497863560914993\n",
            "----------------\n",
            "Loss:  0.6942100524902344 \n",
            "Classification loss: 1.2812931537628174 \n",
            "Regression loss: 0.053563475608825684\n",
            "----------------\n",
            "Loss:  0.6325063370168209 \n",
            "Classification loss: 1.1553488969802856 \n",
            "Regression loss: 0.054831888526678085\n",
            "----------------\n",
            "Loss:  0.6577405147254467 \n",
            "Classification loss: 1.2767916917800903 \n",
            "Regression loss: 0.019344668835401535\n",
            "----------------\n",
            "Loss:  0.6938147824257612 \n",
            "Classification loss: 1.3319146633148193 \n",
            "Regression loss: 0.027857450768351555\n",
            "----------------\n",
            "Loss:  0.7231220118701458 \n",
            "Classification loss: 1.3589720726013184 \n",
            "Regression loss: 0.04363597556948662\n",
            "----------------\n",
            "Loss:  0.6437826380133629 \n",
            "Classification loss: 1.2228405475616455 \n",
            "Regression loss: 0.03236236423254013\n",
            "----------------\n",
            "Loss:  0.6380048841238022 \n",
            "Classification loss: 1.1778123378753662 \n",
            "Regression loss: 0.04909871518611908\n",
            "----------------\n",
            "Loss:  0.7620154432952404 \n",
            "Classification loss: 1.4188358783721924 \n",
            "Regression loss: 0.05259750410914421\n",
            "----------------\n",
            "Loss:  0.8162713013589382 \n",
            "Classification loss: 1.5174674987792969 \n",
            "Regression loss: 0.05753755196928978\n",
            "----------------\n",
            "Loss:  0.6734565906226635 \n",
            "Classification loss: 1.2550092935562134 \n",
            "Regression loss: 0.04595194384455681\n",
            "----------------\n",
            "Loss:  0.6967930570244789 \n",
            "Classification loss: 1.2905031442642212 \n",
            "Regression loss: 0.05154148489236832\n",
            "----------------\n",
            "Loss:  0.697564821690321 \n",
            "Classification loss: 1.320207118988037 \n",
            "Regression loss: 0.037461262196302414\n",
            "----------------\n",
            "Loss:  0.6830740198493004 \n",
            "Classification loss: 1.2827215194702148 \n",
            "Regression loss: 0.04171326011419296\n",
            "----------------\n",
            "Loss:  0.7484583668410778 \n",
            "Classification loss: 1.4215807914733887 \n",
            "Regression loss: 0.03766797110438347\n",
            "----------------\n",
            "Loss:  0.7115100920200348 \n",
            "Classification loss: 1.3346703052520752 \n",
            "Regression loss: 0.04417493939399719\n",
            "----------------\n",
            "Loss:  0.6760674305260181 \n",
            "Classification loss: 1.2625224590301514 \n",
            "Regression loss: 0.04480620101094246\n",
            "----------------\n",
            "Loss:  0.6664718016982079 \n",
            "Classification loss: 1.2254459857940674 \n",
            "Regression loss: 0.053748808801174164\n",
            "----------------\n",
            "Loss:  0.7142023146152496 \n",
            "Classification loss: 1.277879238128662 \n",
            "Regression loss: 0.07526269555091858\n",
            "----------------\n",
            "Loss:  0.7450566217303276 \n",
            "Classification loss: 1.3194338083267212 \n",
            "Regression loss: 0.08533971756696701\n",
            "----------------\n",
            "Loss:  0.6951420865952969 \n",
            "Classification loss: 1.2653182744979858 \n",
            "Regression loss: 0.06248294934630394\n",
            "----------------\n",
            "Loss:  0.6867650821805 \n",
            "Classification loss: 1.2522194385528564 \n",
            "Regression loss: 0.06065536290407181\n",
            "----------------\n",
            "Loss:  0.5765139311552048 \n",
            "Classification loss: 1.0876597166061401 \n",
            "Regression loss: 0.032684072852134705\n",
            "----------------\n",
            "Loss:  0.703413013368845 \n",
            "Classification loss: 1.3010547161102295 \n",
            "Regression loss: 0.05288565531373024\n",
            "----------------\n",
            "Loss:  0.665418341755867 \n",
            "Classification loss: 1.2579251527786255 \n",
            "Regression loss: 0.03645576536655426\n",
            "----------------\n",
            "Loss:  0.7108730785548687 \n",
            "Classification loss: 1.3442456722259521 \n",
            "Regression loss: 0.038750242441892624\n",
            "----------------\n",
            "Loss:  0.7289527282118797 \n",
            "Classification loss: 1.3056066036224365 \n",
            "Regression loss: 0.07614942640066147\n",
            "----------------\n",
            "Loss:  0.5640327073633671 \n",
            "Classification loss: 1.0231633186340332 \n",
            "Regression loss: 0.05245104804635048\n",
            "----------------\n",
            "Loss:  0.8960652947425842 \n",
            "Classification loss: 1.654405117034912 \n",
            "Regression loss: 0.06886273622512817\n",
            "----------------\n",
            "Loss:  0.7170134373009205 \n",
            "Classification loss: 1.338947057723999 \n",
            "Regression loss: 0.047539908438920975\n",
            "----------------\n",
            "Loss:  0.622483029961586 \n",
            "Classification loss: 1.126368761062622 \n",
            "Regression loss: 0.05929864943027496\n",
            "----------------\n",
            "Loss:  0.6877862364053726 \n",
            "Classification loss: 1.2819747924804688 \n",
            "Regression loss: 0.046798840165138245\n",
            "----------------\n",
            "Loss:  0.6770283468067646 \n",
            "Classification loss: 1.2670676708221436 \n",
            "Regression loss: 0.043494511395692825\n",
            "----------------\n",
            "Loss:  0.6431910805404186 \n",
            "Classification loss: 1.2144920825958252 \n",
            "Regression loss: 0.03594503924250603\n",
            "----------------\n",
            "Loss:  0.5665820203721523 \n",
            "Classification loss: 1.0514415502548218 \n",
            "Regression loss: 0.04086124524474144\n",
            "----------------\n",
            "Loss:  0.6565160490572453 \n",
            "Classification loss: 1.2349274158477783 \n",
            "Regression loss: 0.039052341133356094\n",
            "----------------\n",
            "Loss:  0.7656723707914352 \n",
            "Classification loss: 1.395598292350769 \n",
            "Regression loss: 0.06787322461605072\n",
            "----------------\n",
            "Loss:  0.7298615425825119 \n",
            "Classification loss: 1.3509840965270996 \n",
            "Regression loss: 0.0543694943189621\n",
            "----------------\n",
            "Loss:  0.6262788847088814 \n",
            "Classification loss: 1.1912176609039307 \n",
            "Regression loss: 0.030670054256916046\n",
            "----------------\n",
            "Loss:  0.7854652553796768 \n",
            "Classification loss: 1.414094090461731 \n",
            "Regression loss: 0.07841821014881134\n",
            "----------------\n",
            "Loss:  0.6159256137907505 \n",
            "Classification loss: 1.1485095024108887 \n",
            "Regression loss: 0.04167086258530617\n",
            "----------------\n",
            "Loss:  0.7209740355610847 \n",
            "Classification loss: 1.3213931322097778 \n",
            "Regression loss: 0.06027746945619583\n",
            "----------------\n",
            "Loss:  0.6743127889931202 \n",
            "Classification loss: 1.2683095932006836 \n",
            "Regression loss: 0.0401579923927784\n",
            "----------------\n",
            "Loss:  0.5333670601248741 \n",
            "Classification loss: 0.9766323566436768 \n",
            "Regression loss: 0.045050881803035736\n",
            "----------------\n",
            "Loss:  0.6926365531980991 \n",
            "Classification loss: 1.2854297161102295 \n",
            "Regression loss: 0.04992169514298439\n",
            "----------------\n",
            "Loss:  0.6969677656888962 \n",
            "Classification loss: 1.311293363571167 \n",
            "Regression loss: 0.04132108390331268\n",
            "----------------\n",
            "Loss:  0.654970109462738 \n",
            "Classification loss: 1.1890945434570312 \n",
            "Regression loss: 0.06042283773422241\n",
            "----------------\n",
            "Loss:  0.8494071364402771 \n",
            "Classification loss: 1.5315529108047485 \n",
            "Regression loss: 0.08363068103790283\n",
            "----------------\n",
            "Loss:  0.7133469134569168 \n",
            "Classification loss: 1.2675762176513672 \n",
            "Regression loss: 0.07955880463123322\n",
            "----------------\n",
            "Loss:  0.6659315079450607 \n",
            "Classification loss: 1.2427693605422974 \n",
            "Regression loss: 0.04454682767391205\n",
            "----------------\n",
            "Loss:  0.7272907644510269 \n",
            "Classification loss: 1.3239742517471313 \n",
            "Regression loss: 0.06530363857746124\n",
            "----------------\n",
            "Loss:  0.6769868433475494 \n",
            "Classification loss: 1.2375694513320923 \n",
            "Regression loss: 0.058202117681503296\n",
            "----------------\n",
            "Loss:  0.8269504234194756 \n",
            "Classification loss: 1.53443443775177 \n",
            "Regression loss: 0.059733204543590546\n",
            "----------------\n",
            "Loss:  0.6670794412493706 \n",
            "Classification loss: 1.241390585899353 \n",
            "Regression loss: 0.04638414829969406\n",
            "----------------\n",
            "Loss:  0.7493675947189331 \n",
            "Classification loss: 1.4253854751586914 \n",
            "Regression loss: 0.0366748571395874\n",
            "----------------\n",
            "Loss:  0.7396694868803024 \n",
            "Classification loss: 1.3648335933685303 \n",
            "Regression loss: 0.05725269019603729\n",
            "----------------\n",
            "Loss:  0.7078616321086884 \n",
            "Classification loss: 1.345881700515747 \n",
            "Regression loss: 0.03492078185081482\n",
            "----------------\n",
            "Loss:  0.762230396270752 \n",
            "Classification loss: 1.377878189086914 \n",
            "Regression loss: 0.07329130172729492\n",
            "----------------\n",
            "Loss:  0.6538368761539459 \n",
            "Classification loss: 1.2291450500488281 \n",
            "Regression loss: 0.03926435112953186\n",
            "----------------\n",
            "Loss:  0.6046460941433907 \n",
            "Classification loss: 1.091083288192749 \n",
            "Regression loss: 0.059104450047016144\n",
            "----------------\n",
            "Loss:  0.7148803994059563 \n",
            "Classification loss: 1.3333678245544434 \n",
            "Regression loss: 0.04819648712873459\n",
            "----------------\n",
            "Loss:  0.6662615388631821 \n",
            "Classification loss: 1.225817084312439 \n",
            "Regression loss: 0.053352996706962585\n",
            "----------------\n",
            "Loss:  0.5930064171552658 \n",
            "Classification loss: 1.09786057472229 \n",
            "Regression loss: 0.04407612979412079\n",
            "----------------\n",
            "Loss:  0.7738152965903282 \n",
            "Classification loss: 1.402648687362671 \n",
            "Regression loss: 0.07249095290899277\n",
            "----------------\n",
            "Loss:  0.6160504668951035 \n",
            "Classification loss: 1.0855780839920044 \n",
            "Regression loss: 0.07326142489910126\n",
            "----------------\n",
            "Loss:  0.7015910148620605 \n",
            "Classification loss: 1.2653937339782715 \n",
            "Regression loss: 0.0688941478729248\n",
            "----------------\n",
            "Loss:  0.7040315847843885 \n",
            "Classification loss: 1.3455873727798462 \n",
            "Regression loss: 0.031237898394465446\n",
            "----------------\n",
            "Loss:  0.8235706984996796 \n",
            "Classification loss: 1.5263665914535522 \n",
            "Regression loss: 0.06038740277290344\n",
            "----------------\n",
            "Loss:  0.8438325002789497 \n",
            "Classification loss: 1.564943790435791 \n",
            "Regression loss: 0.06136060506105423\n",
            "----------------\n",
            "Loss:  0.5673803277313709 \n",
            "Classification loss: 1.0580013990402222 \n",
            "Regression loss: 0.03837962821125984\n",
            "----------------\n",
            "Loss:  0.7453174814581871 \n",
            "Classification loss: 1.3898208141326904 \n",
            "Regression loss: 0.05040707439184189\n",
            "----------------\n",
            "Loss:  0.6845693215727806 \n",
            "Classification loss: 1.2787103652954102 \n",
            "Regression loss: 0.04521413892507553\n",
            "----------------\n",
            "Loss:  0.6482966281473637 \n",
            "Classification loss: 1.237088918685913 \n",
            "Regression loss: 0.02975216880440712\n",
            "----------------\n",
            "Loss:  0.5708761587738991 \n",
            "Classification loss: 1.0810210704803467 \n",
            "Regression loss: 0.03036562353372574\n",
            "----------------\n",
            "Loss:  0.7150143533945084 \n",
            "Classification loss: 1.366510272026062 \n",
            "Regression loss: 0.031759217381477356\n",
            "----------------\n",
            "Loss:  0.659847304224968 \n",
            "Classification loss: 1.2194994688034058 \n",
            "Regression loss: 0.050097569823265076\n",
            "----------------\n",
            "Loss:  0.5997914634644985 \n",
            "Classification loss: 1.081929326057434 \n",
            "Regression loss: 0.05882680043578148\n",
            "----------------\n",
            "Loss:  0.714530024677515 \n",
            "Classification loss: 1.3543336391448975 \n",
            "Regression loss: 0.0373632051050663\n",
            "----------------\n",
            "Loss:  0.6334932260215282 \n",
            "Classification loss: 1.166345238685608 \n",
            "Regression loss: 0.05032060667872429\n",
            "----------------\n",
            "Loss:  0.7358576990664005 \n",
            "Classification loss: 1.384683609008789 \n",
            "Regression loss: 0.043515894562006\n",
            "----------------\n",
            "Loss:  0.7242174353450537 \n",
            "Classification loss: 1.3867062330245972 \n",
            "Regression loss: 0.03086431883275509\n",
            "----------------\n",
            "Loss:  0.6174208708107471 \n",
            "Classification loss: 1.1457011699676514 \n",
            "Regression loss: 0.04457028582692146\n",
            "----------------\n",
            "Loss:  0.5961531531065702 \n",
            "Classification loss: 1.1540896892547607 \n",
            "Regression loss: 0.019108308479189873\n",
            "----------------\n",
            "Loss:  0.7124304100871086 \n",
            "Classification loss: 1.3174679279327393 \n",
            "Regression loss: 0.05369644612073898\n",
            "----------------\n",
            "Loss:  0.814019650220871 \n",
            "Classification loss: 1.484045386314392 \n",
            "Regression loss: 0.07199695706367493\n",
            "----------------\n",
            "Loss:  0.49571021646261215 \n",
            "Classification loss: 0.9100337624549866 \n",
            "Regression loss: 0.040693335235118866\n",
            "----------------\n",
            "Loss:  0.683350071310997 \n",
            "Classification loss: 1.268213152885437 \n",
            "Regression loss: 0.0492434948682785\n",
            "----------------\n",
            "Loss:  0.6839736960828304 \n",
            "Classification loss: 1.2803165912628174 \n",
            "Regression loss: 0.04381540045142174\n",
            "----------------\n",
            "Loss:  0.728482574224472 \n",
            "Classification loss: 1.318833827972412 \n",
            "Regression loss: 0.06906566023826599\n",
            "----------------\n",
            "Loss:  0.5783026553690434 \n",
            "Classification loss: 1.0842710733413696 \n",
            "Regression loss: 0.036167118698358536\n",
            "----------------\n",
            "Loss:  0.7024490162730217 \n",
            "Classification loss: 1.2967681884765625 \n",
            "Regression loss: 0.05406492203474045\n",
            "----------------\n",
            "Loss:  0.6907687336206436 \n",
            "Classification loss: 1.299235224723816 \n",
            "Regression loss: 0.04115112125873566\n",
            "----------------\n",
            "Loss:  0.6200966872274876 \n",
            "Classification loss: 1.13883376121521 \n",
            "Regression loss: 0.050679806619882584\n",
            "----------------\n",
            "Loss:  0.6779941096901894 \n",
            "Classification loss: 1.221097707748413 \n",
            "Regression loss: 0.06744525581598282\n",
            "----------------\n",
            "Loss:  0.7608346194028854 \n",
            "Classification loss: 1.4283703565597534 \n",
            "Regression loss: 0.04664944112300873\n",
            "----------------\n",
            "Loss:  0.8084364160895348 \n",
            "Classification loss: 1.467857837677002 \n",
            "Regression loss: 0.07450749725103378\n",
            "----------------\n",
            "Loss:  0.5151762031018734 \n",
            "Classification loss: 0.9653766751289368 \n",
            "Regression loss: 0.032487865537405014\n",
            "----------------\n",
            "Loss:  0.6388480477035046 \n",
            "Classification loss: 1.1650769710540771 \n",
            "Regression loss: 0.05630956217646599\n",
            "----------------\n",
            "Loss:  0.7201798092573881 \n",
            "Classification loss: 1.382094383239746 \n",
            "Regression loss: 0.029132617637515068\n",
            "----------------\n",
            "Loss:  0.71034986525774 \n",
            "Classification loss: 1.3012360334396362 \n",
            "Regression loss: 0.059731848537921906\n",
            "----------------\n",
            "Loss:  0.7666224874556065 \n",
            "Classification loss: 1.4337608814239502 \n",
            "Regression loss: 0.04974204674363136\n",
            "----------------\n",
            "Loss:  0.5725172869861126 \n",
            "Classification loss: 1.0711655616760254 \n",
            "Regression loss: 0.0369345061480999\n",
            "----------------\n",
            "Loss:  0.6759240254759789 \n",
            "Classification loss: 1.3053004741668701 \n",
            "Regression loss: 0.023273788392543793\n",
            "----------------\n",
            "Loss:  0.6509645208716393 \n",
            "Classification loss: 1.2270935773849487 \n",
            "Regression loss: 0.037417732179164886\n",
            "----------------\n",
            "Loss:  0.6636766940355301 \n",
            "Classification loss: 1.2367587089538574 \n",
            "Regression loss: 0.04529733955860138\n",
            "----------------\n",
            "Loss:  0.7222995087504387 \n",
            "Classification loss: 1.3210630416870117 \n",
            "Regression loss: 0.06176798790693283\n",
            "----------------\n",
            "Loss:  0.6496650204062462 \n",
            "Classification loss: 1.2159041166305542 \n",
            "Regression loss: 0.041712962090969086\n",
            "----------------\n",
            "Loss:  0.8291865289211273 \n",
            "Classification loss: 1.5871120691299438 \n",
            "Regression loss: 0.035630494356155396\n",
            "----------------\n",
            "Loss:  0.690287634730339 \n",
            "Classification loss: 1.2965683937072754 \n",
            "Regression loss: 0.042003437876701355\n",
            "----------------\n",
            "Loss:  0.6125251427292824 \n",
            "Classification loss: 1.133138656616211 \n",
            "Regression loss: 0.04595581442117691\n",
            "----------------\n",
            "Loss:  0.6403036657720804 \n",
            "Classification loss: 1.2261130809783936 \n",
            "Regression loss: 0.027247125282883644\n",
            "----------------\n",
            "Loss:  0.7701174691319466 \n",
            "Classification loss: 1.4581823348999023 \n",
            "Regression loss: 0.04102630168199539\n",
            "----------------\n",
            "Loss:  0.7042418122291565 \n",
            "Classification loss: 1.312283992767334 \n",
            "Regression loss: 0.0480998158454895\n",
            "----------------\n",
            "Loss:  0.7051308527588844 \n",
            "Classification loss: 1.308011531829834 \n",
            "Regression loss: 0.05112508684396744\n",
            "----------------\n",
            "Loss:  0.7676628977060318 \n",
            "Classification loss: 1.494219422340393 \n",
            "Regression loss: 0.020553186535835266\n",
            "----------------\n",
            "Loss:  0.7871463298797607 \n",
            "Classification loss: 1.4982225894927979 \n",
            "Regression loss: 0.038035035133361816\n",
            "----------------\n",
            "Loss:  0.6388976536691189 \n",
            "Classification loss: 1.1762597560882568 \n",
            "Regression loss: 0.05076777562499046\n",
            "----------------\n",
            "Loss:  0.7301645949482918 \n",
            "Classification loss: 1.3909716606140137 \n",
            "Regression loss: 0.03467876464128494\n",
            "----------------\n",
            "Loss:  0.7486773952841759 \n",
            "Classification loss: 1.4205361604690552 \n",
            "Regression loss: 0.038409315049648285\n",
            "----------------\n",
            "Loss:  0.7138217650353909 \n",
            "Classification loss: 1.3194735050201416 \n",
            "Regression loss: 0.05408501252532005\n",
            "----------------\n",
            "Loss:  0.6200832240283489 \n",
            "Classification loss: 1.1934576034545898 \n",
            "Regression loss: 0.023354422301054\n",
            "----------------\n",
            "Loss:  0.6425680369138718 \n",
            "Classification loss: 1.1987128257751465 \n",
            "Regression loss: 0.04321162402629852\n",
            "----------------\n",
            "Loss:  0.7359086498618126 \n",
            "Classification loss: 1.3164167404174805 \n",
            "Regression loss: 0.07770027965307236\n",
            "----------------\n",
            "Loss:  0.730153176933527 \n",
            "Classification loss: 1.3471684455871582 \n",
            "Regression loss: 0.05656895413994789\n",
            "----------------\n",
            "Loss:  0.6930057220160961 \n",
            "Classification loss: 1.3037775754928589 \n",
            "Regression loss: 0.04111693426966667\n",
            "----------------\n",
            "Loss:  0.6782648544758558 \n",
            "Classification loss: 1.315633773803711 \n",
            "Regression loss: 0.02044796757400036\n",
            "----------------\n",
            "Loss:  0.6808750741183758 \n",
            "Classification loss: 1.2990670204162598 \n",
            "Regression loss: 0.031341563910245895\n",
            "----------------\n",
            "Loss:  0.6545750200748444 \n",
            "Classification loss: 1.2511565685272217 \n",
            "Regression loss: 0.02899673581123352\n",
            "----------------\n",
            "Loss:  0.6927261725068092 \n",
            "Classification loss: 1.3147878646850586 \n",
            "Regression loss: 0.03533224016427994\n",
            "----------------\n",
            "Loss:  0.8012607395648956 \n",
            "Classification loss: 1.433736801147461 \n",
            "Regression loss: 0.08439233899116516\n",
            "----------------\n",
            "Loss:  0.7131417505443096 \n",
            "Classification loss: 1.3415420055389404 \n",
            "Regression loss: 0.0423707477748394\n",
            "----------------\n",
            "Loss:  0.6371280886232853 \n",
            "Classification loss: 1.2190101146697998 \n",
            "Regression loss: 0.02762303128838539\n",
            "----------------\n",
            "Loss:  0.6723284237086773 \n",
            "Classification loss: 1.274247407913208 \n",
            "Regression loss: 0.03520471975207329\n",
            "----------------\n",
            "Loss:  0.5917556472122669 \n",
            "Classification loss: 1.1155396699905396 \n",
            "Regression loss: 0.03398581221699715\n",
            "----------------\n",
            "Loss:  0.6674178242683411 \n",
            "Classification loss: 1.244978427886963 \n",
            "Regression loss: 0.04492861032485962\n",
            "----------------\n",
            "Loss:  0.7325032129883766 \n",
            "Classification loss: 1.3581584692001343 \n",
            "Regression loss: 0.05342397838830948\n",
            "----------------\n",
            "Loss:  0.6655803024768829 \n",
            "Classification loss: 1.2235333919525146 \n",
            "Regression loss: 0.05381360650062561\n",
            "----------------\n",
            "Loss:  0.7563839182257652 \n",
            "Classification loss: 1.3747423887252808 \n",
            "Regression loss: 0.06901272386312485\n",
            "----------------\n",
            "Loss:  0.7708607278764248 \n",
            "Classification loss: 1.4474091529846191 \n",
            "Regression loss: 0.04715615138411522\n",
            "----------------\n",
            "Loss:  0.6640284731984138 \n",
            "Classification loss: 1.2082688808441162 \n",
            "Regression loss: 0.05989403277635574\n",
            "----------------\n",
            "Loss:  0.7075807154178619 \n",
            "Classification loss: 1.32938551902771 \n",
            "Regression loss: 0.04288795590400696\n",
            "----------------\n",
            "Loss:  0.693646565079689 \n",
            "Classification loss: 1.312422752380371 \n",
            "Regression loss: 0.03743518888950348\n",
            "----------------\n",
            "Loss:  0.7276383489370346 \n",
            "Classification loss: 1.361332893371582 \n",
            "Regression loss: 0.04697190225124359\n",
            "----------------\n",
            "Loss:  0.6546264886856079 \n",
            "Classification loss: 1.2191052436828613 \n",
            "Regression loss: 0.045073866844177246\n",
            "----------------\n",
            "Loss:  0.7289089262485504 \n",
            "Classification loss: 1.3706703186035156 \n",
            "Regression loss: 0.0435737669467926\n",
            "----------------\n",
            "Loss:  0.6837805770337582 \n",
            "Classification loss: 1.2539728879928589 \n",
            "Regression loss: 0.05679413303732872\n",
            "----------------\n",
            "Loss:  0.6254350058734417 \n",
            "Classification loss: 1.1373428106307983 \n",
            "Regression loss: 0.056763600558042526\n",
            "----------------\n",
            "Loss:  0.6366787329316139 \n",
            "Classification loss: 1.1957014799118042 \n",
            "Regression loss: 0.03882799297571182\n",
            "----------------\n",
            "Loss:  0.757701363414526 \n",
            "Classification loss: 1.4107674360275269 \n",
            "Regression loss: 0.05231764540076256\n",
            "----------------\n",
            "Loss:  0.7552911415696144 \n",
            "Classification loss: 1.4061050415039062 \n",
            "Regression loss: 0.052238620817661285\n",
            "----------------\n",
            "Loss:  0.6427888199687004 \n",
            "Classification loss: 1.1905808448791504 \n",
            "Regression loss: 0.047498397529125214\n",
            "----------------\n",
            "Loss:  0.6319781430065632 \n",
            "Classification loss: 1.1732399463653564 \n",
            "Regression loss: 0.045358169823884964\n",
            "----------------\n",
            "Loss:  0.6807887256145477 \n",
            "Classification loss: 1.2927213907241821 \n",
            "Regression loss: 0.034428030252456665\n",
            "----------------\n",
            "Loss:  0.6938499957323074 \n",
            "Classification loss: 1.2320599555969238 \n",
            "Regression loss: 0.07782001793384552\n",
            "----------------\n",
            "Loss:  0.7062233537435532 \n",
            "Classification loss: 1.3015072345733643 \n",
            "Regression loss: 0.05546973645687103\n",
            "----------------\n",
            "Loss:  0.7512398958206177 \n",
            "Classification loss: 1.4286918640136719 \n",
            "Regression loss: 0.03689396381378174\n",
            "----------------\n",
            "Loss:  0.6597255170345306 \n",
            "Classification loss: 1.2403318881988525 \n",
            "Regression loss: 0.03955957293510437\n",
            "----------------\n",
            "Loss:  0.647881269454956 \n",
            "Classification loss: 1.2313876152038574 \n",
            "Regression loss: 0.032187461853027344\n",
            "----------------\n",
            "Loss:  0.7322464734315872 \n",
            "Classification loss: 1.3109875917434692 \n",
            "Regression loss: 0.0767526775598526\n",
            "----------------\n",
            "Loss:  0.724906675517559 \n",
            "Classification loss: 1.3545300960540771 \n",
            "Regression loss: 0.04764162749052048\n",
            "----------------\n",
            "Loss:  0.5537695251405239 \n",
            "Classification loss: 1.0064661502838135 \n",
            "Regression loss: 0.05053644999861717\n",
            "----------------\n",
            "Loss:  0.702503889799118 \n",
            "Classification loss: 1.3175337314605713 \n",
            "Regression loss: 0.0437370240688324\n",
            "----------------\n",
            "Loss:  0.7183146253228188 \n",
            "Classification loss: 1.3353650569915771 \n",
            "Regression loss: 0.05063209682703018\n",
            "----------------\n",
            "Loss:  0.6186988949775696 \n",
            "Classification loss: 1.0971602201461792 \n",
            "Regression loss: 0.07011878490447998\n",
            "----------------\n",
            "Loss:  0.6844557002186775 \n",
            "Classification loss: 1.2656984329223633 \n",
            "Regression loss: 0.05160648375749588\n",
            "----------------\n",
            "Loss:  0.6605129167437553 \n",
            "Classification loss: 1.2048534154891968 \n",
            "Regression loss: 0.05808620899915695\n",
            "----------------\n",
            "Loss:  0.682935543358326 \n",
            "Classification loss: 1.3000456094741821 \n",
            "Regression loss: 0.032912738621234894\n",
            "----------------\n",
            "Loss:  0.6457546800374985 \n",
            "Classification loss: 1.1771386861801147 \n",
            "Regression loss: 0.0571853369474411\n",
            "----------------\n",
            "Loss:  0.7498923987150192 \n",
            "Classification loss: 1.3601971864700317 \n",
            "Regression loss: 0.06979380548000336\n",
            "----------------\n",
            "Loss:  0.6849597655236721 \n",
            "Classification loss: 1.2703520059585571 \n",
            "Regression loss: 0.04978376254439354\n",
            "----------------\n",
            "Loss:  0.7240612059831619 \n",
            "Classification loss: 1.3679922819137573 \n",
            "Regression loss: 0.040065065026283264\n",
            "----------------\n",
            "Loss:  0.5892080925405025 \n",
            "Classification loss: 1.1228141784667969 \n",
            "Regression loss: 0.02780100330710411\n",
            "----------------\n",
            "Loss:  0.6865844950079918 \n",
            "Classification loss: 1.2894593477249146 \n",
            "Regression loss: 0.041854821145534515\n",
            "----------------\n",
            "Loss:  0.5739755388349295 \n",
            "Classification loss: 1.087785243988037 \n",
            "Regression loss: 0.03008291684091091\n",
            "----------------\n",
            "Loss:  0.5838068909943104 \n",
            "Classification loss: 1.0893936157226562 \n",
            "Regression loss: 0.039110083132982254\n",
            "----------------\n",
            "Loss:  0.6133734881877899 \n",
            "Classification loss: 1.1390470266342163 \n",
            "Regression loss: 0.04384997487068176\n",
            "----------------\n",
            "Loss:  0.6770455986261368 \n",
            "Classification loss: 1.2121968269348145 \n",
            "Regression loss: 0.07094718515872955\n",
            "----------------\n",
            "Loss:  0.7117342054843903 \n",
            "Classification loss: 1.287532091140747 \n",
            "Regression loss: 0.06796815991401672\n",
            "----------------\n",
            "Loss:  0.6846385821700096 \n",
            "Classification loss: 1.2502450942993164 \n",
            "Regression loss: 0.05951603502035141\n",
            "----------------\n",
            "Loss:  0.6385312303900719 \n",
            "Classification loss: 1.1780186891555786 \n",
            "Regression loss: 0.04952188581228256\n",
            "----------------\n",
            "Loss:  0.7264974266290665 \n",
            "Classification loss: 1.3517398834228516 \n",
            "Regression loss: 0.050627484917640686\n",
            "----------------\n",
            "Loss:  0.5873853918164968 \n",
            "Classification loss: 1.1196684837341309 \n",
            "Regression loss: 0.02755114994943142\n",
            "----------------\n",
            "Loss:  0.709478136152029 \n",
            "Classification loss: 1.3271560668945312 \n",
            "Regression loss: 0.04590010270476341\n",
            "----------------\n",
            "Loss:  0.6889450401067734 \n",
            "Classification loss: 1.3153613805770874 \n",
            "Regression loss: 0.031264349818229675\n",
            "----------------\n",
            "Loss:  0.6243375800549984 \n",
            "Classification loss: 1.155632495880127 \n",
            "Regression loss: 0.04652133211493492\n",
            "----------------\n",
            "Loss:  0.7544796168804169 \n",
            "Classification loss: 1.4208829402923584 \n",
            "Regression loss: 0.04403814673423767\n",
            "----------------\n",
            "Loss:  0.6930010169744492 \n",
            "Classification loss: 1.2726854085922241 \n",
            "Regression loss: 0.0566583126783371\n",
            "----------------\n",
            "Loss:  0.7067834697663784 \n",
            "Classification loss: 1.327457070350647 \n",
            "Regression loss: 0.043054934591054916\n",
            "----------------\n",
            "Loss:  0.7412409707903862 \n",
            "Classification loss: 1.40431547164917 \n",
            "Regression loss: 0.03908323496580124\n",
            "----------------\n",
            "Loss:  0.7103613838553429 \n",
            "Classification loss: 1.3502781391143799 \n",
            "Regression loss: 0.035222314298152924\n",
            "----------------\n",
            "Loss:  0.6383052282035351 \n",
            "Classification loss: 1.1967135667800903 \n",
            "Regression loss: 0.039948444813489914\n",
            "----------------\n",
            "Loss:  0.6738590151071548 \n",
            "Classification loss: 1.1985340118408203 \n",
            "Regression loss: 0.07459200918674469\n",
            "----------------\n",
            "Loss:  0.7272507958114147 \n",
            "Classification loss: 1.3649747371673584 \n",
            "Regression loss: 0.04476342722773552\n",
            "----------------\n",
            "Loss:  0.7077514827251434 \n",
            "Classification loss: 1.312840223312378 \n",
            "Regression loss: 0.05133137106895447\n",
            "----------------\n",
            "Loss:  0.6015778742730618 \n",
            "Classification loss: 1.1216404438018799 \n",
            "Regression loss: 0.04075765237212181\n",
            "----------------\n",
            "Loss:  0.6325496733188629 \n",
            "Classification loss: 1.188485860824585 \n",
            "Regression loss: 0.038306742906570435\n",
            "----------------\n",
            "Loss:  0.6667318716645241 \n",
            "Classification loss: 1.2290418148040771 \n",
            "Regression loss: 0.052210964262485504\n",
            "----------------\n",
            "Loss:  0.6362671460956335 \n",
            "Classification loss: 1.2175980806350708 \n",
            "Regression loss: 0.027468105778098106\n",
            "----------------\n",
            "Loss:  0.6492514796555042 \n",
            "Classification loss: 1.2134207487106323 \n",
            "Regression loss: 0.042541105300188065\n",
            "----------------\n",
            "Loss:  0.7139191403985023 \n",
            "Classification loss: 1.2813712358474731 \n",
            "Regression loss: 0.07323352247476578\n",
            "----------------\n",
            "Loss:  0.8188765197992325 \n",
            "Classification loss: 1.514348030090332 \n",
            "Regression loss: 0.06170250475406647\n",
            "----------------\n",
            "Loss:  0.7679691314697266 \n",
            "Classification loss: 1.4082037210464478 \n",
            "Regression loss: 0.06386727094650269\n",
            "----------------\n",
            "Loss:  0.6081472113728523 \n",
            "Classification loss: 1.1190446615219116 \n",
            "Regression loss: 0.048624880611896515\n",
            "----------------\n",
            "Loss:  0.7401553839445114 \n",
            "Classification loss: 1.3771796226501465 \n",
            "Regression loss: 0.05156557261943817\n",
            "----------------\n",
            "Loss:  0.6784326769411564 \n",
            "Classification loss: 1.3070075511932373 \n",
            "Regression loss: 0.024928901344537735\n",
            "----------------\n",
            "Loss:  0.5782346352934837 \n",
            "Classification loss: 1.055431604385376 \n",
            "Regression loss: 0.050518833100795746\n",
            "----------------\n",
            "Loss:  0.746507965028286 \n",
            "Classification loss: 1.4094278812408447 \n",
            "Regression loss: 0.04179402440786362\n",
            "----------------\n",
            "Loss:  0.6582840234041214 \n",
            "Classification loss: 1.2117775678634644 \n",
            "Regression loss: 0.05239523947238922\n",
            "----------------\n",
            "Loss:  0.7183493673801422 \n",
            "Classification loss: 1.3637807369232178 \n",
            "Regression loss: 0.036458998918533325\n",
            "----------------\n",
            "Loss:  0.6685101687908173 \n",
            "Classification loss: 1.2154067754745483 \n",
            "Regression loss: 0.06080678105354309\n",
            "----------------\n",
            "Loss:  0.6700687371194363 \n",
            "Classification loss: 1.2430099248886108 \n",
            "Regression loss: 0.048563774675130844\n",
            "----------------\n",
            "Loss:  0.6412651278078556 \n",
            "Classification loss: 1.2035294771194458 \n",
            "Regression loss: 0.039500389248132706\n",
            "----------------\n",
            "Loss:  0.7201406359672546 \n",
            "Classification loss: 1.277421236038208 \n",
            "Regression loss: 0.08143001794815063\n",
            "----------------\n",
            "Loss:  0.7063451111316681 \n",
            "Classification loss: 1.2682342529296875 \n",
            "Regression loss: 0.07222798466682434\n",
            "----------------\n",
            "Loss:  0.6361209079623222 \n",
            "Classification loss: 1.145129680633545 \n",
            "Regression loss: 0.06355606764554977\n",
            "----------------\n",
            "Loss:  0.6618685256689787 \n",
            "Classification loss: 1.2735095024108887 \n",
            "Regression loss: 0.025113774463534355\n",
            "----------------\n",
            "Loss:  0.7435983121395111 \n",
            "Classification loss: 1.3785864114761353 \n",
            "Regression loss: 0.05430510640144348\n",
            "----------------\n",
            "Loss:  0.7269739657640457 \n",
            "Classification loss: 1.35163152217865 \n",
            "Regression loss: 0.051158204674720764\n",
            "----------------\n",
            "Loss:  0.7487508617341518 \n",
            "Classification loss: 1.4226117134094238 \n",
            "Regression loss: 0.037445005029439926\n",
            "----------------\n",
            "Loss:  0.7719521746039391 \n",
            "Classification loss: 1.4444053173065186 \n",
            "Regression loss: 0.04974951595067978\n",
            "----------------\n",
            "Loss:  0.7603507079184055 \n",
            "Classification loss: 1.431221604347229 \n",
            "Regression loss: 0.04473990574479103\n",
            "----------------\n",
            "Loss:  0.7166563421487808 \n",
            "Classification loss: 1.3484796285629272 \n",
            "Regression loss: 0.0424165278673172\n",
            "----------------\n",
            "Loss:  0.6668804883956909 \n",
            "Classification loss: 1.2340168952941895 \n",
            "Regression loss: 0.04987204074859619\n",
            "----------------\n",
            "Loss:  0.6996586509048939 \n",
            "Classification loss: 1.2885370254516602 \n",
            "Regression loss: 0.0553901381790638\n",
            "----------------\n",
            "Loss:  0.6314652059227228 \n",
            "Classification loss: 1.2099149227142334 \n",
            "Regression loss: 0.026507744565606117\n",
            "----------------\n",
            "Loss:  0.6826753616333008 \n",
            "Classification loss: 1.254573106765747 \n",
            "Regression loss: 0.055388808250427246\n",
            "----------------\n",
            "Loss:  0.7175415009260178 \n",
            "Classification loss: 1.295190453529358 \n",
            "Regression loss: 0.0699462741613388\n",
            "----------------\n",
            "Loss:  0.6710919514298439 \n",
            "Classification loss: 1.2539875507354736 \n",
            "Regression loss: 0.044098176062107086\n",
            "----------------\n",
            "Loss:  0.7032527476549149 \n",
            "Classification loss: 1.321022868156433 \n",
            "Regression loss: 0.0427413135766983\n",
            "----------------\n",
            "Loss:  0.7673080265522003 \n",
            "Classification loss: 1.4446144104003906 \n",
            "Regression loss: 0.045000821352005005\n",
            "----------------\n",
            "Loss:  0.7125640138983727 \n",
            "Classification loss: 1.3067784309387207 \n",
            "Regression loss: 0.0591747984290123\n",
            "----------------\n",
            "Loss:  0.76407191157341 \n",
            "Classification loss: 1.3861703872680664 \n",
            "Regression loss: 0.07098671793937683\n",
            "----------------\n",
            "Loss:  0.6680773198604584 \n",
            "Classification loss: 1.2804217338562012 \n",
            "Regression loss: 0.027866452932357788\n",
            "----------------\n",
            "Loss:  0.7398476973176003 \n",
            "Classification loss: 1.3452749252319336 \n",
            "Regression loss: 0.06721023470163345\n",
            "----------------\n",
            "Loss:  0.7555111013352871 \n",
            "Classification loss: 1.4057292938232422 \n",
            "Regression loss: 0.052646454423666\n",
            "----------------\n",
            "Loss:  0.5655029565095901 \n",
            "Classification loss: 1.025787115097046 \n",
            "Regression loss: 0.0526093989610672\n",
            "----------------\n",
            "Loss:  0.7375762201845646 \n",
            "Classification loss: 1.3878540992736816 \n",
            "Regression loss: 0.04364917054772377\n",
            "----------------\n",
            "Loss:  0.7197501827031374 \n",
            "Classification loss: 1.3859443664550781 \n",
            "Regression loss: 0.026777999475598335\n",
            "----------------\n",
            "Loss:  0.7237153425812721 \n",
            "Classification loss: 1.3443810939788818 \n",
            "Regression loss: 0.05152479559183121\n",
            "----------------\n",
            "Loss:  0.6579026207327843 \n",
            "Classification loss: 1.2200281620025635 \n",
            "Regression loss: 0.04788853973150253\n",
            "----------------\n",
            "Loss:  0.6701748110353947 \n",
            "Classification loss: 1.263055682182312 \n",
            "Regression loss: 0.03864696994423866\n",
            "----------------\n",
            "Loss:  0.5379730332642794 \n",
            "Classification loss: 1.0349433422088623 \n",
            "Regression loss: 0.020501362159848213\n",
            "----------------\n",
            "Loss:  0.7028035297989845 \n",
            "Classification loss: 1.3378098011016846 \n",
            "Regression loss: 0.03389862924814224\n",
            "----------------\n",
            "Loss:  0.6702192947268486 \n",
            "Classification loss: 1.2718126773834229 \n",
            "Regression loss: 0.034312956035137177\n",
            "----------------\n",
            "Loss:  0.6762571781873703 \n",
            "Classification loss: 1.2950180768966675 \n",
            "Regression loss: 0.02874813973903656\n",
            "----------------\n",
            "Loss:  0.672105573117733 \n",
            "Classification loss: 1.2281174659729004 \n",
            "Regression loss: 0.058046840131282806\n",
            "----------------\n",
            "Loss:  0.5657827071845531 \n",
            "Classification loss: 1.0785843133926392 \n",
            "Regression loss: 0.026490550488233566\n",
            "----------------\n",
            "Loss:  0.5795077104121447 \n",
            "Classification loss: 1.1194692850112915 \n",
            "Regression loss: 0.01977306790649891\n",
            "----------------\n",
            "Loss:  0.674551572650671 \n",
            "Classification loss: 1.2733665704727173 \n",
            "Regression loss: 0.03786828741431236\n",
            "----------------\n",
            "Loss:  0.6501450696960092 \n",
            "Classification loss: 1.2692023515701294 \n",
            "Regression loss: 0.015543893910944462\n",
            "----------------\n",
            "Loss:  0.6807021982967854 \n",
            "Classification loss: 1.2864561080932617 \n",
            "Regression loss: 0.037474144250154495\n",
            "----------------\n",
            "Loss:  0.7155477926135063 \n",
            "Classification loss: 1.3412258625030518 \n",
            "Regression loss: 0.04493486136198044\n",
            "----------------\n",
            "Loss:  0.5394460558891296 \n",
            "Classification loss: 1.0077649354934692 \n",
            "Regression loss: 0.03556358814239502\n",
            "----------------\n",
            "Loss:  0.7885769940912724 \n",
            "Classification loss: 1.490675449371338 \n",
            "Regression loss: 0.04323926940560341\n",
            "----------------\n",
            "Loss:  0.6848323866724968 \n",
            "Classification loss: 1.2235338687896729 \n",
            "Regression loss: 0.07306545227766037\n",
            "----------------\n",
            "Loss:  0.5745008178055286 \n",
            "Classification loss: 1.0508095026016235 \n",
            "Regression loss: 0.04909606650471687\n",
            "----------------\n",
            "Loss:  0.7834531590342522 \n",
            "Classification loss: 1.415348768234253 \n",
            "Regression loss: 0.0757787749171257\n",
            "----------------\n",
            "Loss:  0.7477956302464008 \n",
            "Classification loss: 1.3814888000488281 \n",
            "Regression loss: 0.05705123022198677\n",
            "----------------\n",
            "Loss:  0.6228509694337845 \n",
            "Classification loss: 1.1526012420654297 \n",
            "Regression loss: 0.04655034840106964\n",
            "----------------\n",
            "Loss:  0.7168698012828827 \n",
            "Classification loss: 1.3561127185821533 \n",
            "Regression loss: 0.03881344199180603\n",
            "----------------\n",
            "Loss:  0.5766699388623238 \n",
            "Classification loss: 1.0428361892700195 \n",
            "Regression loss: 0.055251844227313995\n",
            "----------------\n",
            "Loss:  0.7735446766018867 \n",
            "Classification loss: 1.460888147354126 \n",
            "Regression loss: 0.04310060292482376\n",
            "----------------\n",
            "Loss:  0.7247155383229256 \n",
            "Classification loss: 1.3463464975357056 \n",
            "Regression loss: 0.051542289555072784\n",
            "----------------\n",
            "Loss:  0.763001061975956 \n",
            "Classification loss: 1.373119592666626 \n",
            "Regression loss: 0.07644126564264297\n",
            "----------------\n",
            "Loss:  0.7149762474000454 \n",
            "Classification loss: 1.3639216423034668 \n",
            "Regression loss: 0.033015426248311996\n",
            "----------------\n",
            "Loss:  0.6506461873650551 \n",
            "Classification loss: 1.1641998291015625 \n",
            "Regression loss: 0.06854627281427383\n",
            "----------------\n",
            "Loss:  0.6181746311485767 \n",
            "Classification loss: 1.1686856746673584 \n",
            "Regression loss: 0.03383179381489754\n",
            "----------------\n",
            "Loss:  0.7092796415090561 \n",
            "Classification loss: 1.3341110944747925 \n",
            "Regression loss: 0.04222409427165985\n",
            "----------------\n",
            "Loss:  0.7138804048299789 \n",
            "Classification loss: 1.329295039176941 \n",
            "Regression loss: 0.049232885241508484\n",
            "----------------\n",
            "Loss:  0.7198532521724701 \n",
            "Classification loss: 1.3194639682769775 \n",
            "Regression loss: 0.06012126803398132\n",
            "----------------\n",
            "Loss:  0.7154688723385334 \n",
            "Classification loss: 1.3325718641281128 \n",
            "Regression loss: 0.049182940274477005\n",
            "----------------\n",
            "Loss:  0.5664527602493763 \n",
            "Classification loss: 1.0271265506744385 \n",
            "Regression loss: 0.05288948491215706\n",
            "----------------\n",
            "Loss:  0.7231879681348801 \n",
            "Classification loss: 1.386899471282959 \n",
            "Regression loss: 0.029738232493400574\n",
            "----------------\n",
            "Loss:  0.6979405209422112 \n",
            "Classification loss: 1.2462615966796875 \n",
            "Regression loss: 0.0748097226023674\n",
            "----------------\n",
            "Loss:  0.7412233352661133 \n",
            "Classification loss: 1.3549902439117432 \n",
            "Regression loss: 0.0637282133102417\n",
            "----------------\n",
            "Loss:  0.7599813044071198 \n",
            "Classification loss: 1.3901009559631348 \n",
            "Regression loss: 0.06493082642555237\n",
            "----------------\n",
            "Loss:  0.5512358397245407 \n",
            "Classification loss: 1.028960943222046 \n",
            "Regression loss: 0.03675536811351776\n",
            "----------------\n",
            "Loss:  0.7055106535553932 \n",
            "Classification loss: 1.325406551361084 \n",
            "Regression loss: 0.04280737787485123\n",
            "----------------\n",
            "Loss:  0.5645652562379837 \n",
            "Classification loss: 1.0659247636795044 \n",
            "Regression loss: 0.031602874398231506\n",
            "----------------\n",
            "Loss:  0.6050493605434895 \n",
            "Classification loss: 1.1381105184555054 \n",
            "Regression loss: 0.03599410131573677\n",
            "----------------\n",
            "Loss:  0.6121050268411636 \n",
            "Classification loss: 1.1549935340881348 \n",
            "Regression loss: 0.03460825979709625\n",
            "----------------\n",
            "Loss:  0.765925470739603 \n",
            "Classification loss: 1.460379719734192 \n",
            "Regression loss: 0.035735610872507095\n",
            "----------------\n",
            "Loss:  0.7230538204312325 \n",
            "Classification loss: 1.3554332256317139 \n",
            "Regression loss: 0.04533720761537552\n",
            "----------------\n",
            "Loss:  0.5931929592043161 \n",
            "Classification loss: 1.1412171125411987 \n",
            "Regression loss: 0.022584402933716774\n",
            "----------------\n",
            "Loss:  0.7858244329690933 \n",
            "Classification loss: 1.459804892539978 \n",
            "Regression loss: 0.05592198669910431\n",
            "----------------\n",
            "Loss:  0.6515074446797371 \n",
            "Classification loss: 1.2189857959747314 \n",
            "Regression loss: 0.04201454669237137\n",
            "----------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndPe98RRAlmF",
        "colab_type": "code",
        "outputId": "366fa36d-ab12-43cb-9309-cd81cb817a89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# testing \n",
        "tot = 0\n",
        "tp,fp,fn = 0,0,0\n",
        "for t in test_ex:\n",
        "  N = rawX[t].shape[0]\n",
        "  print(\"Total frames in {}: {}\".format(t, N))\n",
        "  batchX = []\n",
        "  batchY = []\n",
        "  for i in range(num_frames, N+1,3):\n",
        "    batchX.append(rawX[t][i-num_frames:i])\n",
        "    batchY.append(rawY[t][i-num_frames:i])\n",
        "  batchX = np.array(batchX)\n",
        "  batchY = np.array(batchY)\n",
        "  batchY = batchY[:, -1, :]\n",
        "  batchX = np.transpose(batchX, (0, 3, 1, 2))\n",
        "  batchX = np.expand_dims(batchX, axis=4)\n",
        "  print(\"Batch X shape : \", batchX.shape, \", Batch Y shape : \", batchY.shape)\n",
        "  # cce = tf.keras.losses.CategoricalCrossentropy()\n",
        "  preds,_ = model2(batchX, training=False)\n",
        "  xx,yy,zz = f1(preds.numpy(),batchY)\n",
        "  tp+=xx\n",
        "  fp+=yy\n",
        "  fn+=zz\n",
        "  # print(np.shape(preds))\n",
        "  a = np.argmax(preds, axis=1)\n",
        "  b = np.argmax(batchY, axis=1)\n",
        "  # print(\"ground truth\", b)\n",
        "  # print(\"predictions\", a)\n",
        "\n",
        "  c = (a==b)\n",
        "  # print(np.sum(c.astype(int))/c.shape[0])\n",
        "  acc = np.sum(c.astype(int))/c.shape[0]\n",
        "  tot += acc\n",
        "  print(\"Accuracy of this example = \",acc*100)\n",
        "\n",
        "prec = tp/(tp+fp)\n",
        "rec = tp/(tp+fn)\n",
        "f1sc = (2*prec*rec)/(prec+rec)\n",
        "print(\"Test accuracy : \", (tot/len(test_ex)) * 100.)\n",
        "print(\"F1-score = \", f1sc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total frames in 854: 6455\n",
            "Batch X shape :  (2146, 3, 20, 25, 1) , Batch Y shape :  (2146, 52)\n",
            "Accuracy of this example =  61.83597390493942\n",
            "Total frames in 855: 6467\n",
            "Batch X shape :  (2150, 3, 20, 25, 1) , Batch Y shape :  (2150, 52)\n",
            "Accuracy of this example =  59.581395348837205\n",
            "Total frames in 856: 5381\n",
            "Batch X shape :  (1788, 3, 20, 25, 1) , Batch Y shape :  (1788, 52)\n",
            "Accuracy of this example =  56.09619686800895\n",
            "Total frames in 857: 6202\n",
            "Batch X shape :  (2061, 3, 20, 25, 1) , Batch Y shape :  (2061, 52)\n",
            "Accuracy of this example =  43.27996118389132\n",
            "Total frames in 858: 6228\n",
            "Batch X shape :  (2070, 3, 20, 25, 1) , Batch Y shape :  (2070, 52)\n",
            "Accuracy of this example =  42.608695652173914\n",
            "Total frames in 859: 5062\n",
            "Batch X shape :  (1681, 3, 20, 25, 1) , Batch Y shape :  (1681, 52)\n",
            "Accuracy of this example =  40.63057703747769\n",
            "Total frames in 860: 5690\n",
            "Batch X shape :  (1891, 3, 20, 25, 1) , Batch Y shape :  (1891, 52)\n",
            "Accuracy of this example =  57.00687466948704\n",
            "Total frames in 861: 5671\n",
            "Batch X shape :  (1884, 3, 20, 25, 1) , Batch Y shape :  (1884, 52)\n",
            "Accuracy of this example =  56.68789808917197\n",
            "Total frames in 862: 4645\n",
            "Batch X shape :  (1542, 3, 20, 25, 1) , Batch Y shape :  (1542, 52)\n",
            "Accuracy of this example =  51.81582360570688\n",
            "Total frames in 863: 4654\n",
            "Batch X shape :  (1545, 3, 20, 25, 1) , Batch Y shape :  (1545, 52)\n",
            "Accuracy of this example =  60.19417475728155\n",
            "Total frames in 864: 4642\n",
            "Batch X shape :  (1541, 3, 20, 25, 1) , Batch Y shape :  (1541, 52)\n",
            "Accuracy of this example =  60.350421804023355\n",
            "Total frames in 865: 3879\n",
            "Batch X shape :  (1287, 3, 20, 25, 1) , Batch Y shape :  (1287, 52)\n",
            "Accuracy of this example =  56.72105672105672\n",
            "Total frames in 866: 6365\n",
            "Batch X shape :  (2116, 3, 20, 25, 1) , Batch Y shape :  (2116, 52)\n",
            "Accuracy of this example =  59.87712665406427\n",
            "Total frames in 867: 6319\n",
            "Batch X shape :  (2100, 3, 20, 25, 1) , Batch Y shape :  (2100, 52)\n",
            "Accuracy of this example =  62.71428571428571\n",
            "Total frames in 868: 5205\n",
            "Batch X shape :  (1729, 3, 20, 25, 1) , Batch Y shape :  (1729, 52)\n",
            "Accuracy of this example =  57.43204164256795\n",
            "Total frames in 869: 6528\n",
            "Batch X shape :  (2170, 3, 20, 25, 1) , Batch Y shape :  (2170, 52)\n",
            "Accuracy of this example =  61.29032258064516\n",
            "Total frames in 870: 6519\n",
            "Batch X shape :  (2167, 3, 20, 25, 1) , Batch Y shape :  (2167, 52)\n",
            "Accuracy of this example =  63.9132441162898\n",
            "Total frames in 871: 5430\n",
            "Batch X shape :  (1804, 3, 20, 25, 1) , Batch Y shape :  (1804, 52)\n",
            "Accuracy of this example =  65.57649667405765\n",
            "Total frames in 872: 6756\n",
            "Batch X shape :  (2246, 3, 20, 25, 1) , Batch Y shape :  (2246, 52)\n",
            "Accuracy of this example =  70.30276046304542\n",
            "Total frames in 873: 6766\n",
            "Batch X shape :  (2249, 3, 20, 25, 1) , Batch Y shape :  (2249, 52)\n",
            "Accuracy of this example =  73.32147621164961\n",
            "Total frames in 874: 5595\n",
            "Batch X shape :  (1859, 3, 20, 25, 1) , Batch Y shape :  (1859, 52)\n",
            "Accuracy of this example =  69.8224852071006\n",
            "Total frames in 875: 5855\n",
            "Batch X shape :  (1946, 3, 20, 25, 1) , Batch Y shape :  (1946, 52)\n",
            "Accuracy of this example =  70.91469681397739\n",
            "Total frames in 876: 5912\n",
            "Batch X shape :  (1965, 3, 20, 25, 1) , Batch Y shape :  (1965, 52)\n",
            "Accuracy of this example =  69.46564885496184\n",
            "Total frames in 877: 5006\n",
            "Batch X shape :  (1663, 3, 20, 25, 1) , Batch Y shape :  (1663, 52)\n",
            "Accuracy of this example =  68.12988574864703\n",
            "Total frames in 878: 2366\n",
            "Batch X shape :  (783, 3, 20, 25, 1) , Batch Y shape :  (783, 52)\n",
            "Accuracy of this example =  62.57982120051086\n",
            "Total frames in 879: 2378\n",
            "Batch X shape :  (787, 3, 20, 25, 1) , Batch Y shape :  (787, 52)\n",
            "Accuracy of this example =  64.42185514612451\n",
            "Total frames in 880: 2060\n",
            "Batch X shape :  (681, 3, 20, 25, 1) , Batch Y shape :  (681, 52)\n",
            "Accuracy of this example =  64.75770925110133\n",
            "Total frames in 881: 2079\n",
            "Batch X shape :  (687, 3, 20, 25, 1) , Batch Y shape :  (687, 52)\n",
            "Accuracy of this example =  71.03347889374089\n",
            "Total frames in 882: 2085\n",
            "Batch X shape :  (689, 3, 20, 25, 1) , Batch Y shape :  (689, 52)\n",
            "Accuracy of this example =  70.24673439767778\n",
            "Total frames in 883: 1793\n",
            "Batch X shape :  (592, 3, 20, 25, 1) , Batch Y shape :  (592, 52)\n",
            "Accuracy of this example =  67.22972972972973\n",
            "Total frames in 884: 6889\n",
            "Batch X shape :  (2290, 3, 20, 25, 1) , Batch Y shape :  (2290, 52)\n",
            "Accuracy of this example =  65.37117903930131\n",
            "Total frames in 885: 6833\n",
            "Batch X shape :  (2272, 3, 20, 25, 1) , Batch Y shape :  (2272, 52)\n",
            "Accuracy of this example =  65.53697183098592\n",
            "Total frames in 886: 5691\n",
            "Batch X shape :  (1891, 3, 20, 25, 1) , Batch Y shape :  (1891, 52)\n",
            "Accuracy of this example =  60.86726599682708\n",
            "Total frames in 887: 5729\n",
            "Batch X shape :  (1904, 3, 20, 25, 1) , Batch Y shape :  (1904, 52)\n",
            "Accuracy of this example =  75.42016806722688\n",
            "Total frames in 888: 5834\n",
            "Batch X shape :  (1939, 3, 20, 25, 1) , Batch Y shape :  (1939, 52)\n",
            "Accuracy of this example =  73.23362558019598\n",
            "Total frames in 889: 4766\n",
            "Batch X shape :  (1583, 3, 20, 25, 1) , Batch Y shape :  (1583, 52)\n",
            "Accuracy of this example =  67.46683512318383\n",
            "Total frames in 890: 6378\n",
            "Batch X shape :  (2120, 3, 20, 25, 1) , Batch Y shape :  (2120, 52)\n",
            "Accuracy of this example =  68.4433962264151\n",
            "Total frames in 891: 6449\n",
            "Batch X shape :  (2144, 3, 20, 25, 1) , Batch Y shape :  (2144, 52)\n",
            "Accuracy of this example =  66.6044776119403\n",
            "Total frames in 892: 5405\n",
            "Batch X shape :  (1796, 3, 20, 25, 1) , Batch Y shape :  (1796, 52)\n",
            "Accuracy of this example =  66.48106904231625\n",
            "Total frames in 893: 5738\n",
            "Batch X shape :  (1907, 3, 20, 25, 1) , Batch Y shape :  (1907, 52)\n",
            "Accuracy of this example =  69.21866806502359\n",
            "Total frames in 894: 5777\n",
            "Batch X shape :  (1920, 3, 20, 25, 1) , Batch Y shape :  (1920, 52)\n",
            "Accuracy of this example =  72.08333333333333\n",
            "Total frames in 895: 4831\n",
            "Batch X shape :  (1604, 3, 20, 25, 1) , Batch Y shape :  (1604, 52)\n",
            "Accuracy of this example =  73.69077306733168\n",
            "Total frames in 896: 5857\n",
            "Batch X shape :  (1946, 3, 20, 25, 1) , Batch Y shape :  (1946, 52)\n",
            "Accuracy of this example =  63.10380267214799\n",
            "Total frames in 897: 5843\n",
            "Batch X shape :  (1942, 3, 20, 25, 1) , Batch Y shape :  (1942, 52)\n",
            "Accuracy of this example =  65.4994850669413\n",
            "Total frames in 898: 4903\n",
            "Batch X shape :  (1628, 3, 20, 25, 1) , Batch Y shape :  (1628, 52)\n",
            "Accuracy of this example =  65.9090909090909\n",
            "Total frames in 899: 4234\n",
            "Batch X shape :  (1405, 3, 20, 25, 1) , Batch Y shape :  (1405, 52)\n",
            "Accuracy of this example =  66.40569395017793\n",
            "Total frames in 900: 4388\n",
            "Batch X shape :  (1457, 3, 20, 25, 1) , Batch Y shape :  (1457, 52)\n",
            "Accuracy of this example =  66.0260809883322\n",
            "Total frames in 901: 3563\n",
            "Batch X shape :  (1182, 3, 20, 25, 1) , Batch Y shape :  (1182, 52)\n",
            "Accuracy of this example =  66.58206429780034\n",
            "Total frames in 902: 6732\n",
            "Batch X shape :  (2238, 3, 20, 25, 1) , Batch Y shape :  (2238, 52)\n",
            "Accuracy of this example =  68.67739052725648\n",
            "Total frames in 903: 6668\n",
            "Batch X shape :  (2217, 3, 20, 25, 1) , Batch Y shape :  (2217, 52)\n",
            "Accuracy of this example =  68.47090663058187\n",
            "Total frames in 904: 5547\n",
            "Batch X shape :  (1843, 3, 20, 25, 1) , Batch Y shape :  (1843, 52)\n",
            "Accuracy of this example =  65.59956592512208\n",
            "Total frames in 905: 3699\n",
            "Batch X shape :  (1227, 3, 20, 25, 1) , Batch Y shape :  (1227, 52)\n",
            "Accuracy of this example =  66.82966585167074\n",
            "Total frames in 906: 3784\n",
            "Batch X shape :  (1255, 3, 20, 25, 1) , Batch Y shape :  (1255, 52)\n",
            "Accuracy of this example =  66.85258964143426\n",
            "Total frames in 907: 3250\n",
            "Batch X shape :  (1077, 3, 20, 25, 1) , Batch Y shape :  (1077, 52)\n",
            "Accuracy of this example =  63.416898792943364\n",
            "Total frames in 908: 1571\n",
            "Batch X shape :  (518, 3, 20, 25, 1) , Batch Y shape :  (518, 52)\n",
            "Accuracy of this example =  67.37451737451737\n",
            "Total frames in 909: 1580\n",
            "Batch X shape :  (521, 3, 20, 25, 1) , Batch Y shape :  (521, 52)\n",
            "Accuracy of this example =  74.28023032629558\n",
            "Total frames in 910: 1282\n",
            "Batch X shape :  (421, 3, 20, 25, 1) , Batch Y shape :  (421, 52)\n",
            "Accuracy of this example =  53.681710213776725\n",
            "Total frames in 911: 1364\n",
            "Batch X shape :  (449, 3, 20, 25, 1) , Batch Y shape :  (449, 52)\n",
            "Accuracy of this example =  61.915367483296215\n",
            "Total frames in 912: 1388\n",
            "Batch X shape :  (457, 3, 20, 25, 1) , Batch Y shape :  (457, 52)\n",
            "Accuracy of this example =  59.51859956236324\n",
            "Total frames in 913: 1158\n",
            "Batch X shape :  (380, 3, 20, 25, 1) , Batch Y shape :  (380, 52)\n",
            "Accuracy of this example =  51.84210526315789\n",
            "Total frames in 914: 4865\n",
            "Batch X shape :  (1616, 3, 20, 25, 1) , Batch Y shape :  (1616, 52)\n",
            "Accuracy of this example =  60.334158415841586\n",
            "Total frames in 915: 4859\n",
            "Batch X shape :  (1614, 3, 20, 25, 1) , Batch Y shape :  (1614, 52)\n",
            "Accuracy of this example =  61.276332094175956\n",
            "Total frames in 916: 4092\n",
            "Batch X shape :  (1358, 3, 20, 25, 1) , Batch Y shape :  (1358, 52)\n",
            "Accuracy of this example =  58.983799705449194\n",
            "Total frames in 917: 3182\n",
            "Batch X shape :  (1055, 3, 20, 25, 1) , Batch Y shape :  (1055, 52)\n",
            "Accuracy of this example =  63.791469194312796\n",
            "Total frames in 918: 3187\n",
            "Batch X shape :  (1056, 3, 20, 25, 1) , Batch Y shape :  (1056, 52)\n",
            "Accuracy of this example =  65.625\n",
            "Total frames in 919: 2737\n",
            "Batch X shape :  (906, 3, 20, 25, 1) , Batch Y shape :  (906, 52)\n",
            "Accuracy of this example =  64.90066225165563\n",
            "Total frames in 920: 4754\n",
            "Batch X shape :  (1579, 3, 20, 25, 1) , Batch Y shape :  (1579, 52)\n",
            "Accuracy of this example =  65.42115262824572\n",
            "Total frames in 921: 4760\n",
            "Batch X shape :  (1581, 3, 20, 25, 1) , Batch Y shape :  (1581, 52)\n",
            "Accuracy of this example =  61.79633143580012\n",
            "Total frames in 922: 4068\n",
            "Batch X shape :  (1350, 3, 20, 25, 1) , Batch Y shape :  (1350, 52)\n",
            "Accuracy of this example =  62.22222222222222\n",
            "Total frames in 923: 4201\n",
            "Batch X shape :  (1394, 3, 20, 25, 1) , Batch Y shape :  (1394, 52)\n",
            "Accuracy of this example =  68.00573888091822\n",
            "Total frames in 924: 4227\n",
            "Batch X shape :  (1403, 3, 20, 25, 1) , Batch Y shape :  (1403, 52)\n",
            "Accuracy of this example =  67.07056307911617\n",
            "Total frames in 925: 3752\n",
            "Batch X shape :  (1245, 3, 20, 25, 1) , Batch Y shape :  (1245, 52)\n",
            "Accuracy of this example =  70.12048192771084\n",
            "Total frames in 926: 7002\n",
            "Batch X shape :  (2328, 3, 20, 25, 1) , Batch Y shape :  (2328, 52)\n",
            "Accuracy of this example =  68.08419243986255\n",
            "Total frames in 927: 6987\n",
            "Batch X shape :  (2323, 3, 20, 25, 1) , Batch Y shape :  (2323, 52)\n",
            "Accuracy of this example =  65.99225139905295\n",
            "Total frames in 928: 6180\n",
            "Batch X shape :  (2054, 3, 20, 25, 1) , Batch Y shape :  (2054, 52)\n",
            "Accuracy of this example =  65.43330087633885\n",
            "Total frames in 929: 4417\n",
            "Batch X shape :  (1466, 3, 20, 25, 1) , Batch Y shape :  (1466, 52)\n",
            "Accuracy of this example =  59.34515688949522\n",
            "Total frames in 930: 4448\n",
            "Batch X shape :  (1477, 3, 20, 25, 1) , Batch Y shape :  (1477, 52)\n",
            "Accuracy of this example =  59.918754231550444\n",
            "Total frames in 931: 3907\n",
            "Batch X shape :  (1296, 3, 20, 25, 1) , Batch Y shape :  (1296, 52)\n",
            "Accuracy of this example =  57.02160493827161\n",
            "Total frames in 932: 6340\n",
            "Batch X shape :  (2107, 3, 20, 25, 1) , Batch Y shape :  (2107, 52)\n",
            "Accuracy of this example =  62.45847176079734\n",
            "Total frames in 933: 6397\n",
            "Batch X shape :  (2126, 3, 20, 25, 1) , Batch Y shape :  (2126, 52)\n",
            "Accuracy of this example =  60.018814675446855\n",
            "Total frames in 934: 5633\n",
            "Batch X shape :  (1872, 3, 20, 25, 1) , Batch Y shape :  (1872, 52)\n",
            "Accuracy of this example =  56.623931623931625\n",
            "Total frames in 935: 5914\n",
            "Batch X shape :  (1965, 3, 20, 25, 1) , Batch Y shape :  (1965, 52)\n",
            "Accuracy of this example =  70.68702290076337\n",
            "Total frames in 936: 5911\n",
            "Batch X shape :  (1964, 3, 20, 25, 1) , Batch Y shape :  (1964, 52)\n",
            "Accuracy of this example =  71.18126272912424\n",
            "Total frames in 937: 5222\n",
            "Batch X shape :  (1735, 3, 20, 25, 1) , Batch Y shape :  (1735, 52)\n",
            "Accuracy of this example =  70.02881844380403\n",
            "Total frames in 938: 1766\n",
            "Batch X shape :  (583, 3, 20, 25, 1) , Batch Y shape :  (583, 52)\n",
            "Accuracy of this example =  64.15094339622641\n",
            "Total frames in 939: 1783\n",
            "Batch X shape :  (588, 3, 20, 25, 1) , Batch Y shape :  (588, 52)\n",
            "Accuracy of this example =  71.08843537414967\n",
            "Total frames in 940: 1543\n",
            "Batch X shape :  (508, 3, 20, 25, 1) , Batch Y shape :  (508, 52)\n",
            "Accuracy of this example =  60.23622047244095\n",
            "Total frames in 941: 1726\n",
            "Batch X shape :  (569, 3, 20, 25, 1) , Batch Y shape :  (569, 52)\n",
            "Accuracy of this example =  68.0140597539543\n",
            "Total frames in 942: 1744\n",
            "Batch X shape :  (575, 3, 20, 25, 1) , Batch Y shape :  (575, 52)\n",
            "Accuracy of this example =  66.26086956521739\n",
            "Total frames in 943: 1503\n",
            "Batch X shape :  (495, 3, 20, 25, 1) , Batch Y shape :  (495, 52)\n",
            "Accuracy of this example =  62.82828282828283\n",
            "Total frames in 944: 7456\n",
            "Batch X shape :  (2479, 3, 20, 25, 1) , Batch Y shape :  (2479, 52)\n",
            "Accuracy of this example =  62.68656716417911\n",
            "Total frames in 945: 7417\n",
            "Batch X shape :  (2466, 3, 20, 25, 1) , Batch Y shape :  (2466, 52)\n",
            "Accuracy of this example =  65.77453365774534\n",
            "Total frames in 946: 6674\n",
            "Batch X shape :  (2219, 3, 20, 25, 1) , Batch Y shape :  (2219, 52)\n",
            "Accuracy of this example =  63.587201442091036\n",
            "Total frames in 947: 4718\n",
            "Batch X shape :  (1567, 3, 20, 25, 1) , Batch Y shape :  (1567, 52)\n",
            "Accuracy of this example =  66.94320357370772\n",
            "Total frames in 948: 4625\n",
            "Batch X shape :  (1536, 3, 20, 25, 1) , Batch Y shape :  (1536, 52)\n",
            "Accuracy of this example =  66.40625\n",
            "Total frames in 949: 4294\n",
            "Batch X shape :  (1425, 3, 20, 25, 1) , Batch Y shape :  (1425, 52)\n",
            "Accuracy of this example =  72.0\n",
            "Total frames in 950: 5829\n",
            "Batch X shape :  (1937, 3, 20, 25, 1) , Batch Y shape :  (1937, 52)\n",
            "Accuracy of this example =  59.73154362416108\n",
            "Total frames in 951: 5887\n",
            "Batch X shape :  (1956, 3, 20, 25, 1) , Batch Y shape :  (1956, 52)\n",
            "Accuracy of this example =  61.707566462167684\n",
            "Total frames in 952: 5316\n",
            "Batch X shape :  (1766, 3, 20, 25, 1) , Batch Y shape :  (1766, 52)\n",
            "Accuracy of this example =  59.22989807474519\n",
            "Total frames in 953: 4546\n",
            "Batch X shape :  (1509, 3, 20, 25, 1) , Batch Y shape :  (1509, 52)\n",
            "Accuracy of this example =  63.4857521537442\n",
            "Total frames in 954: 4562\n",
            "Batch X shape :  (1515, 3, 20, 25, 1) , Batch Y shape :  (1515, 52)\n",
            "Accuracy of this example =  67.39273927392739\n",
            "Total frames in 955: 4122\n",
            "Batch X shape :  (1368, 3, 20, 25, 1) , Batch Y shape :  (1368, 52)\n",
            "Accuracy of this example =  67.39766081871345\n",
            "Total frames in 956: 6975\n",
            "Batch X shape :  (2319, 3, 20, 25, 1) , Batch Y shape :  (2319, 52)\n",
            "Accuracy of this example =  56.79172056921087\n",
            "Total frames in 957: 7024\n",
            "Batch X shape :  (2335, 3, 20, 25, 1) , Batch Y shape :  (2335, 52)\n",
            "Accuracy of this example =  63.16916488222698\n",
            "Total frames in 958: 6270\n",
            "Batch X shape :  (2084, 3, 20, 25, 1) , Batch Y shape :  (2084, 52)\n",
            "Accuracy of this example =  57.48560460652591\n",
            "Total frames in 959: 5624\n",
            "Batch X shape :  (1869, 3, 20, 25, 1) , Batch Y shape :  (1869, 52)\n",
            "Accuracy of this example =  70.73301230604602\n",
            "Total frames in 960: 5614\n",
            "Batch X shape :  (1865, 3, 20, 25, 1) , Batch Y shape :  (1865, 52)\n",
            "Accuracy of this example =  78.92761394101878\n",
            "Total frames in 961: 4914\n",
            "Batch X shape :  (1632, 3, 20, 25, 1) , Batch Y shape :  (1632, 52)\n",
            "Accuracy of this example =  79.35049019607843\n",
            "Total frames in 962: 6475\n",
            "Batch X shape :  (2152, 3, 20, 25, 1) , Batch Y shape :  (2152, 52)\n",
            "Accuracy of this example =  77.04460966542752\n",
            "Total frames in 963: 6560\n",
            "Batch X shape :  (2181, 3, 20, 25, 1) , Batch Y shape :  (2181, 52)\n",
            "Accuracy of this example =  77.67079321412196\n",
            "Total frames in 964: 5830\n",
            "Batch X shape :  (1937, 3, 20, 25, 1) , Batch Y shape :  (1937, 52)\n",
            "Accuracy of this example =  80.33040784718636\n",
            "Total frames in 965: 5980\n",
            "Batch X shape :  (1987, 3, 20, 25, 1) , Batch Y shape :  (1987, 52)\n",
            "Accuracy of this example =  76.14494212380473\n",
            "Total frames in 966: 6115\n",
            "Batch X shape :  (2032, 3, 20, 25, 1) , Batch Y shape :  (2032, 52)\n",
            "Accuracy of this example =  78.69094488188976\n",
            "Total frames in 967: 5367\n",
            "Batch X shape :  (1783, 3, 20, 25, 1) , Batch Y shape :  (1783, 52)\n",
            "Accuracy of this example =  76.94896242288279\n",
            "Total frames in 968: 1962\n",
            "Batch X shape :  (648, 3, 20, 25, 1) , Batch Y shape :  (648, 52)\n",
            "Accuracy of this example =  34.72222222222222\n",
            "Total frames in 969: 1972\n",
            "Batch X shape :  (651, 3, 20, 25, 1) , Batch Y shape :  (651, 52)\n",
            "Accuracy of this example =  71.88940092165899\n",
            "Total frames in 970: 1682\n",
            "Batch X shape :  (555, 3, 20, 25, 1) , Batch Y shape :  (555, 52)\n",
            "Accuracy of this example =  55.4954954954955\n",
            "Total frames in 971: 1917\n",
            "Batch X shape :  (633, 3, 20, 25, 1) , Batch Y shape :  (633, 52)\n",
            "Accuracy of this example =  52.44865718799369\n",
            "Total frames in 972: 1902\n",
            "Batch X shape :  (628, 3, 20, 25, 1) , Batch Y shape :  (628, 52)\n",
            "Accuracy of this example =  74.68152866242038\n",
            "Total frames in 973: 1658\n",
            "Batch X shape :  (547, 3, 20, 25, 1) , Batch Y shape :  (547, 52)\n",
            "Accuracy of this example =  72.94332723948813\n",
            "Total frames in 974: 6261\n",
            "Batch X shape :  (2081, 3, 20, 25, 1) , Batch Y shape :  (2081, 52)\n",
            "Accuracy of this example =  69.1975012013455\n",
            "Total frames in 975: 6267\n",
            "Batch X shape :  (2083, 3, 20, 25, 1) , Batch Y shape :  (2083, 52)\n",
            "Accuracy of this example =  70.18722995679309\n",
            "Total frames in 976: 5615\n",
            "Batch X shape :  (1866, 3, 20, 25, 1) , Batch Y shape :  (1866, 52)\n",
            "Accuracy of this example =  65.4876741693462\n",
            "Total frames in 977: 5806\n",
            "Batch X shape :  (1929, 3, 20, 25, 1) , Batch Y shape :  (1929, 52)\n",
            "Accuracy of this example =  66.35562467599793\n",
            "Total frames in 978: 5872\n",
            "Batch X shape :  (1951, 3, 20, 25, 1) , Batch Y shape :  (1951, 52)\n",
            "Accuracy of this example =  69.60533059969246\n",
            "Total frames in 979: 5188\n",
            "Batch X shape :  (1723, 3, 20, 25, 1) , Batch Y shape :  (1723, 52)\n",
            "Accuracy of this example =  72.78003482298317\n",
            "Total frames in 980: 6670\n",
            "Batch X shape :  (2217, 3, 20, 25, 1) , Batch Y shape :  (2217, 52)\n",
            "Accuracy of this example =  62.562020748759586\n",
            "Total frames in 981: 6561\n",
            "Batch X shape :  (2181, 3, 20, 25, 1) , Batch Y shape :  (2181, 52)\n",
            "Accuracy of this example =  63.04447501146263\n",
            "Total frames in 982: 5989\n",
            "Batch X shape :  (1990, 3, 20, 25, 1) , Batch Y shape :  (1990, 52)\n",
            "Accuracy of this example =  65.92964824120602\n",
            "Total frames in 983: 6231\n",
            "Batch X shape :  (2071, 3, 20, 25, 1) , Batch Y shape :  (2071, 52)\n",
            "Accuracy of this example =  63.44760985031386\n",
            "Total frames in 984: 6278\n",
            "Batch X shape :  (2087, 3, 20, 25, 1) , Batch Y shape :  (2087, 52)\n",
            "Accuracy of this example =  70.38811691423096\n",
            "Total frames in 985: 5463\n",
            "Batch X shape :  (1815, 3, 20, 25, 1) , Batch Y shape :  (1815, 52)\n",
            "Accuracy of this example =  71.51515151515152\n",
            "Test accuracy :  64.98087057481474\n",
            "F1-score =  0.4575014261266401\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpMPj_nqyPEN",
        "colab_type": "code",
        "outputId": "0986984a-e3a5-41a8-c2d0-67089490663f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "c = 0\n",
        "w = 0\n",
        "\n",
        "tt = preds.numpy()\n",
        "x = np.argmax(tt, axis = 1)\n",
        "for j in x:\n",
        "  print(j,end = '\\t')\n",
        "y = np.argmax(batchY,axis = 1)\n",
        "print()\n",
        "for j in y:\n",
        "  print(j,end = '\\t')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tFalse\tFalse\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tTrue\tFalse\tFalse\tFalse\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tTrue\tFalse\tFalse\tFalse\tFalse\tTrue\tTrue\tTrue\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tTrue\tTrue\tFalse\tFalse\tFalse\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\t\n",
            "False\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tTrue\tTrue\tTrue\tTrue\tTrue\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\t[False False False ... False False False]\n",
            "[False False False ... False False False]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aC5RAoqxr4TC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def give_intervals(y):\n",
        "  y = np.argmax(y,axis = 1)\n",
        "  ints = []\n",
        "  i = 0\n",
        "  while(i < len(y)):\n",
        "    t = y[i]\n",
        "    if(t != 0):\n",
        "      st = i\n",
        "      while(i < len(y) and y[i] == t):\n",
        "        i+=1\n",
        "        if i >= len(y):\n",
        "          break\n",
        "      end = i-1\n",
        "      ints.append(np.array([t,st,end]))\n",
        "    i+=1\n",
        "  return np.array(ints)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHElpwRpMbrt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def iou(x,y):\n",
        "  x1,x2 = x[1],x[2]\n",
        "  y1,y2 = y[1],y[2]\n",
        "  if x2 < y1 or y2 < x1:\n",
        "    return 0\n",
        "  num = min(x2,y2) - max(x1,y1) + 1\n",
        "  den = max(x2,y2) - min(x1,y1) + 1\n",
        "  return num/den"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XDvHlDGKFmW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def f1(preds, truth, threshold = 0.75):\n",
        "  pr = give_intervals(preds)\n",
        "  tr = give_intervals(truth)\n",
        "  tp, fn, fp = 0,0,0\n",
        "  for x in tr:\n",
        "    ff = False\n",
        "    for y in pr:\n",
        "      if(iou(x,y) >= threshold):\n",
        "        if(x[0] == y[0]):\n",
        "          tp+=1\n",
        "        else:\n",
        "          fp+=1\n",
        "        ff = True\n",
        "        break\n",
        "    if ff == False:\n",
        "      fn+=1\n",
        "  return tp, fp, fn\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cx722UJcMayA",
        "colab_type": "code",
        "outputId": "3ca7224e-1fba-4d3a-89a5-6b25e1031c68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# pr = give_intervals(preds.numpy())\n",
        "# tr = give_intervals(batchY)\n",
        "f1(preds.numpy(),batchY)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6, 2, 14)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    }
  ]
}